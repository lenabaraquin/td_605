<TXT=w-Communisme1>

I. Étymologie
Le terme communisme vient du terme latin communis, ce qui est commun à un groupe, auquel s'adjoint le suffixe « -isme » désignant une doctrine. La commune serait également un des termes définissant le cadre du groupe (de ce qui lui serait commun) dans lequel se définit le communisme. 


II. Les usages du terme
Divers usages existent autour du terme « communisme ». Ainsi, il est nécessaire de reprendre l'historique du mot et de séparer les différents courants ainsi que les différents concepts utilisés.

II.1. Qu'est ce que le communisme ? 
En 1845, dans L'idéologie allemande, pour Marx et Engels, « le communisme n'est pas un état de choses qu'il convient d'établir, un idéal auquel la réalité devra se conformer. » Ils appellent « communisme le mouvement réel qui abolit l'état actuel des choses. Les conditions de ce mouvement résultent des données préalables telles qu'elles existent actuellement. »3 
En 1847, Friedrich Engels définit ce mouvement réel dans les « Principes du communisme ». Ainsi, « le communisme est l'enseignement des conditions de la libération du prolétariat. »4. 


II.2. Communisme, marxisme, collectivisme


II.3. Sentiments parmi la population
On désigne parmi les premiers « marxistes » en France les Guesdistes comme Jules Guesde et Paul Lafargue qui ne se sont jamais d'ailleurs revendiqués comme marxistes ; contrairement et probablement aux Broussistes dans les années 1870 et à certains Hégéliens dans les années 1890 5 et « autres amis de ce genre » dont Marx a dit : « je ne suis pas marxiste. ». 
Marx critiqua ainsi les visions encore utopique de ces mouvements dont leur sentiment vont historiquement en France au socialiste Joseph Proudhon et au révolutionnaire Auguste Blanqui même parmi les Guesdistes ("marxistes français"). Par ailleurs, vers la fin de sa vie, n'a-t-il pas dit de ces gendres que Longuet était le dernier des proudhonniens et de Lafargue le dernier des boukaninistes ? De plus, les guesdistes, trop enthousiastes de cette nouvelle pensée, prévoient contrairement à Karl Marx une victoire rapide et facile contre les méfaits du capitalisme. Ainsi, pour les autres socialistes, les guesdistes font preuve d'un messianisme parce qu'ils citent constamment Marx soit d'une manière trop simpliste, soit comme un prêtre citant la Bible. 
Cependant, dans son Introduction de la Guerre Civil, Engels écrit que la Commune de Paris a sonné le glas des écoles prudhonniennes et du blanquisme puisque ces élus, majoritaires, ont fait le contraire de leur pensée, c'est-à-dire essayer de construire selon Marx le Communisme ; et qu'à partir de 1891, « c'est maintenant la théorie de Marx qui y règne chez les « possibilistes » non moins que chez les « marxistes ». Ce n'est que chez les bourgeois « radical » qu'on trouve encore des prouddhonniens. ». 
Ainsi, tandis que le proudhommisme eût perdu de l'influence, est-ce cependant la pensée marxiste qui prédomine dans les milieux ouvriers français ? Même avec la diffusion du marxisme par Guesde et Lafargue dans le prolétariat, c'est le sentiment anarchiste par "l'action directe" et la Propagande par le fait qui prédominent. Les événements de Fourmies en 1891 exalté par l'anarchiste Renard, les lois scélérates en 1893, la formation de la CGT en 1895 (dominé par des syndicalistes révolutionnaires libertaires) l'attestent. 
Et, comme le dira, en 1896, Wilhelm Liebknecht dans ces Souvenirs sur Marx: « Il n'y avait alors au sein de la classe ouvrière elle-même qu'une infime minorité qui se fût haussée jusqu'au socialisme ; et parmi les socialistes eux-mêmes, les socialistes dans l'esprit scientifique de Marx - dans l'esprit du manifeste communiste - n'étaient qu'une minorité. Le gros des ouvriers, dans la mesure où ils s'étaient éveillés à la vie politique en général, étaient encore plongés dans la brume des aspirations et des formules démocratiques sentimentales qui caractérisaient le mouvement de 1848 aussi que ses levers et baissers de rideau. ». 


II.4.  Moyens politiques et collectivisme
Le communisme est souvent assimilé au mouvement marxiste. 
Cependant, le marxisme n'est qu'une composante du communisme tout comme l'arnarchisme. Selon Jules Guesde, les marxistes se posent en « double qualité de communiste - comme but - et de collectiviste - comme moyen - »6. 
Ces derniers se font appeler les collectivistes. En effet, dans son sens ancien, le communisme prescrivait la mise en commun de tous les biens. C'est par « collectivisme » qu'on désignait les doctrines de mise en commun des seuls moyens de production. 
Cependant, « Le collectivisme ne se distingue pas du communisme scientifique, tel qui est sorti de la critique maîtresse de Karl Marx. Si cette appellation a prévalu en France, c'est que, pour les besoins de notre propagande, il y avait lieu de nous distinguer des divers systèmes communistes qui, forgés de toutes pièces par des hommes de plus ou moins de bonne volonté ou de génie, versaient tous dans l'utopie. »7 
Ainsi, peu après la commune de Paris en 1871, et jusqu'en 1917, les marxistes se sont désignés comme « collectivistes » et la majorité des anarchistes comme « communistes », du fait que le premier courant voulait utiliser un État dirigé par les travailleurs et la collectivisation comme outil de transition, au contraire du second qui voulait utiliser directement des moyens libres et autonomes pour atteindre le but recherché. 
Cependant, selon le français Paul Lafargue le terme de « Collectivisme est un mauvais synonyme belge pour communisme. Le communisme a un passé historique et a eu pour représentants au début du siècle Owen et Fourier. En 1847, Marx et Engels ont publié le Manifeste communiste, document incomparable au XIXème siècle... Enfin, le mot ne fait rien l'affaire si nous propageons les idées du communisme. »8 




II.5. Communisme et régimes de type soviétique
Les régimes politiques qui étaient en place dans l'Union soviétique et dans le « bloc communiste » sont communément désignés aujourd'hui sous le nom de « régimes communistes ». En URSS, on désigne du nom de « communiste » les membres du parti et seulement eux. La qualification « communiste » de ces régimes est contestée principalement par des communistes ou sympathisants. Des marxistes et des anarchistes, ont analysé dès leur apparition ces pays comme des régimes « capitalistes d'État ». Les trotskystes estiment qu'il s'agit d'une monstrueuse dégénérescence d'un « État ouvrier » issu de la Révolution russe. La majorité des communistes (staliniens) reconnurent longtemps ces régimes comme découlant de leurs idées. Les adversaires du communisme considèrent que la nature autoritaire ou totalitaire de ces régimes découle logiquement de l'idéologie communiste. 
Les dirigeants de ces pays les déclaraient « socialistes ». Ce terme fut le plus communément utilisé dans ces pays, où était pratiquée une planification économique. Le « socialisme » fut défini par des théoriciens léninistes comme l'étape préliminaire et nécessaire vers le communisme, la société idéale sans classes ni propriété. Ce régime fut appelé aussi au départ la dictature du prolétariat, terme abandonné vers 1936 pour le « socialisme triomphant » et ensuite, vers les années 1970, le « socialisme développé ». 
Une société communiste se définit au plan théorique comme une société sans classes, sans salariat et sans État. L'URSS, ses républiques composites et satellites et tous les pays socialistes n'ont jamais réalisé l'idéal communiste, même selon les théories staliniennes : ils se disaient « en route vers » le communisme. 


II.6. Les bases du communisme
Les bases du communisme sont beaucoup plus complexes que l'on pourrait le penser. Ainsi Marx lui-même n'est pas le créateur du socialisme ou communisme. Bien d'autres auteurs y avaient pensé avant lui, Rousseau avance déjà l'idéologie comme dans son Discours sur l'origine et les fondements de l'inégalité, où il explique que l'invention de la propriété est nuisible "Gardez-vous d'écouter cette imposteur; vous êtes perdus, si vous oubliez que les fruits sont à tous, et que la terre n'est à personne". On peut aussi citer Gracchus Babeuf. Le communisme s'appuie aussi sur le philosophe allemand Kant. 



III. Les idées du communisme

III.1. La théorie économique et sociale
Sur le plan de l'organisation sociale et économique, le mot communisme conjecture l'organisation d'une société : 

où, selon le principe « de chacun selon ses capacités, à chacun selon ses besoins », les rapports économiques ne sont pas réglés par un principe d'échange intéressé de marchandises, mais collectivement organisés en vue de satisfaire les besoins. Le flux de biens n'a pas besoin d'être réciproque, il est donc basé sur la gratuité. Il n'y a ni commerce, ni argent, ni tout autre rapport marchand (chacun ayant tous ses besoins couvert par le mode de production communiste, il n'y a plus d'« échange » à proprement parler). La propriété privée est, sinon abolie, du moins très limitée, de sorte que la notion de « don » perd également de sa pertinence. 
Où les moyens de production (usines, terres agricoles, fermes d'élevage, moyens de transport, distribution, etc.) sont mis en commun, avec comme but de répondre aux besoins de chacun, et gérés par la population travailleuse elle-même. Ainsi, les « gaspillages » dus à la concurrence et au « contrôle d'une minorité sur la production » disparaîtraient.
Sans division en classes sociales et sans État (sans force de coercition au service d'une classe). Le communisme admet que tous ne sont pas aussi productifs pour tout, et qu'il doit attendre une production « de chacun selon ses capacités », mais il considère que chacun peut généralement à son gré se livrer aux occupations de son choix, qu'on peut abolir la division sociale du travail sans mettre en péril la production et donc la disponibilité des biens (vision qu'on retrouve dans la théorie, plus récente, de la société post-industrielle évoquée par Herman Kahn), et supprimer toute force coercitive, comme la police ou l'armée, et plus généralement fonctionner sans État. Il admet que certaines tâches et fonctions sont plus désagréables ou agréables que d'autres, et compte sur la démocratie directe pour juguler les conflits, notamment ceux relatifs à la production.

Des difficultés théoriques et pratiques se posent pour l'instauration du communisme : 

côté théorique, il faut définir les « besoins » vers lesquels il faut orienter l'appareil de production. Tout le monde n'a pas les mêmes priorités. La réalisation du principe « de chacun selon ses besoins » implique que chaque individu définisse ses propres besoins, et non un système ou un appareil qui le fasse à sa place ; une solution serait d'organiser le recensement des besoins des individus au niveau communal ou des conseils ouvriers, pour conséquemment produire le nécessaire afin de répondre à ces besoins recensés, mais cela suppose une décentralisation du pouvoir décisionnel. On rétablit ainsi une sorte de loi de l'offre et de la demande puisque quoi qu'il advienne, si la société ne produit pas « naturellement » les besoins recensés, on lui demandera un effort de production. Mais si on les recense c'est que les besoins ne sont pas évidents, et sont donc nouveaux, l'État devient alors une sorte d'intermédiaire.
Également au niveau théorique, le recours à la planification part du présupposé que l'organisation de la production selon des bases rationnelles par des autorités planificatrices serait plus efficace que le recours au marché. Ce point est remis en cause à un double niveau par plusieurs auteurs : Pour Friedrich Hayek dans La Route de la servitude (1944), en supposant la possibilité de la planification de l'économie, celle-ci requiert le contrôle total des ressources et débouche forcément sur le totalitarisme. Pour Michael Polanyi dans La Logique de la liberté, la planification est « impossible » car les économies actuelles sont trop complexes pour être gérées centralement. À l'ordre monocentrique des économies planifiées, il oppose la polycentricité des économies de marché, dans lesquelles les décisions sont décentralisées et où beaucoup plus d'information peut être utilisée pour prendre les bonnes décisions. Les résultats comparés de la planification et des économies de marché semblent confirmer l'avantage de ces dernières. 
Côté pratique, comment éliminer le risque de voir un groupe s'auto-définir comme « l'avant-garde du prolétariat » et se constituer en classe privilégiée centralisant tous les pouvoirs ?

En URSS, les individus parvenant à maîtriser le système politique chargé de gérer l'appareil de production ont formé spontanément ce que Léon Trotski a appelé la "bureaucratie", et Mikhaïl Voslenski la "nomenklatura". Poursuivant des fins propres (intérêt personnel et/ou intérêt de classe) et non l'intérêt général, ce groupe a utilisé le régime à son profit. Pour prévenir de tels dangers (que les léninistes estiment présents surtout peu de temps après l'instauration de « l'État ouvrier ») il faudrait un contrôle de l'ensemble de la population sur les différents responsables, à tous les niveaux, et leur révocabilité (c'était la revendication des ouvriers et soldats des années 1920 en Russie soviétique : « Tout le pouvoir aux Soviets ! »).


III.2. Les moyens politiques
Le passage d'une société non communiste à une société communiste constitue un moment délicat : les moyens divergent selon les tendances existantes au sein du mouvement communiste, néanmoins la période révolutionnaire (avec toutes les conséquences d'une révolution) est une base commune, même si les moyens pour la réaliser ne le sont pas. 
Dans la théorie marxiste, le communisme est l'aboutissement ultime de l'évolution des sociétés humaines de la commune primitive à l'esclavagisme, de l'esclavagisme au féodalisme, du féodalisme au capitalisme, du capitalisme au socialisme par la révolution, et du socialisme (qui n'a pas fini de s'affranchir de toutes les traces du capitalisme ni la propriété privée de biens meubles ou immeubles familiaux) au communisme (où il n'y a plus ni propriété privée, ni classes). Celui-ci ne pourrait alors en fin de processus qu'embrasser l'humanité entière. 
Des expériences, notamment en Espagne en 1936 dans une période révolutionnaire, dans des collectivités (il existera aux alentours de 3000 collectivités dans toute l'Espagne) aragonaises ou catalanes (et dans d'autres régions), ont effectué des réalisations partiellement communistes (en laissant, et de manières diverses selon la situation de chaque collectivité, aux petits propriétaires, lorsque cela ne gênait en rien les moyens de réaliser la subsistance des collectivités, la liberté de garder leurs biens ou de s'associer ou non aux collectivités). 
Par ailleurs, dans les pays à régime dit « marxiste-léniniste » d'inspiration soviétique, l'étape dite « révolutionnaire » a donné naissance à des républiques dites « populaires » ou « démocraties populaires », l'évolution ultérieure, après « liquidation des classes exploiteuses » devant aboutir à des « républiques socialistes » (certains de ces états estimèrent y être parvenus), et finalement à une seule « république communiste » mondiale. Les intitulés officiels de ces États étaient censés refléter l'état d'avancement de leurs sociétés vers l'objectif communiste. Les appellations de courtoisie officielles reflétaient elles aussi la situation de chaque interlocuteur par rapport à cet objectif : quiconque était censé le poursuivre était un « camarade » ; quiconque y consentait passivement était un « citoyen » ; quiconque ne le poursuivait pas était un « monsieur » ou une « madame » (interlocuteurs étrangers issus des pays non-communistes) et quiconque s'y opposait était un « ennemi de classe » ou « du peuple ». 
C'est sur la façon d'organiser ce passage progressif à une société communiste que les courants se séparent en plusieurs approches : 

Le léninisme (incluant le trotskisme), partant de l'idée de Karl Marx que l'État est une machine au service de la classe dominante, conçoit le passage de la société capitaliste à la société communiste comme la destruction de l'appareil d'État de la bourgeoisie dans une révolution mobilisant de larges couches de la population et emmenée par la classe ouvrière, et de la construction d'un État ouvrier, au service de la majorité de la population. Cet État doit s'approprier les grands moyens de production et organiser l'activité économique en vue de la satisfaction des besoins de la population. Cette période historique, appelée dictature du prolétariat, doit être, selon le mot de Lénine, « un million de fois plus démocratique que n'importe quelle démocratie bourgeoise10 ». Cette période de transition doit conduire à la constitution d'une société sans classes, dans laquelle l'État sera amené à disparaître, à « s'éteindre » (Engels), afin de conduire au communisme.
Le réformisme (Eduard Bernstein) s'est développé au sein du mouvement ouvrier avec la croissance et les succès des premiers grands partis ouvriers, à la fin du xixe siècle. Les réformistes suggéraient la possibilité d'un passage à une société communiste sans recourir à une confrontation violente, par l'acquisition de positions institutionnelles et les nationalisations de pans vitaux de l'économie (transports, énergie...). Ce réformisme n'a rien à voir avec l'acception actuelle du mot, qui qualifie aujourd'hui des courants visant à « améliorer » le capitalisme, et non à le supprimer.
L'approche anti-étatique (anarchistes et minorité des marxistes), utilisant les moyens autogestionnaires et révolutionnaires d'appropriation des moyens de production refusant immédiatement la division en classe sociale de gestionnaires/gérés, et réalisant le communisme par les moyens utilisés, sans attendre une transition.
D'autres courants, comme les maoïstes, envisagent la prise du pouvoir comme résultant d'une guerre populaire prolongée.

Selon Marx, la Commune de Paris, malgré toutes les imperfections qu'elle comportait, a été l'expérience historique la plus proche du communisme, par la mise en place d'un début de démocratie véritable. 
Des anarchistes appelleront les associations de travailleurs à se fédérer de manière autonome. Errico Malatesta développe dans le gradualisme révolutionnaire la nécessité de l'autonomie du mouvement ouvrier, afin d'éviter toute avant-garde éclairée, ou de futurs gouvernements « ouvriers ». L'entraide (pour réaliser les moyens à l'émancipation sociale) et la lutte révolutionnaire directe (pour se libérer de l'exploiteur et de ses soutiens) afin de mener à des conditions favorables à la réalisation du communisme. Le communisme est considéré par les anarchistes, selon la situation, comme une économie réalisable aussitôt la révolution entreprise ; des auteurs comme Pierre Kropotkine exposeront des possibilités d'économies communistes, dont la prise au tas, organisée de manière communale. 


III.3. Les idées connexes du communisme 
On doit également citer des valeurs ou idées inventées ou reprises à son compte par le communisme :

La conception matérialiste de l'histoire,
L'entraide et sa variante contemporaine, la solidarité,
Le gradualisme révolutionnaire tendant à réaliser révolutionnairement des conditions favorables à la réalisation du communisme (ainsi que de l'anarchisme), 
La prise au tas, comme moyen direct d'organisation du communisme.




IV. Histoire : le communisme dans l'histoire
Le mot communisme désigne une idée mais aussi des mouvements politiques, divers et contradictoires, qui militent pour l'avènement d'une société sans classe sociale, sans salariat, sans propriété privée de moyens de production, sans État et sans capitalisme. L'objectif proclamé est la liberté et l'égalité de l'homme. 
On trouve parmi les mouvements politiques qui se revendiquent du communisme aussi bien des anarchistes que des marxistes ou des mouvements de lutte ouvrière. 
Les régimes s'étant déclarés « socialistes » ou « en route vers le communisme » : l'Union soviétique, les pays autodésignés comme "démocraties populaires" d'Europe centrale et orientale, la République démocratique allemande (RDA), la Chine, Cuba, le Cambodge, le Vietnam, l'Afghanistan, l'Angola, la Corée du Nord se revendiquaient du « marxisme-léninisme », courant qualifié par les autres communistes de Stalinisme, y compris après la « déstalinisation » de 1956 (qui n'a concerné ni la Chine de Mao, ni la Corée du Nord, ni l'Albanie). Ces États se sont livrés au bâillonnement de l'opposition, pouvant aller jusqu'à l'élimination physique, à un contrôle pratiquement absolu de la société et à la destruction de toute liberté d'expression. Toutefois, à plusieurs reprises, il y a eu dans ces régimes des dirigeants qui tentèrent de réaliser un "socialisme à visage humain" dans le but de s'approcher de l'idéal communiste par la démocratie et en répondant aux aspirations des peuples : ce fut par exemple le cas de d'Imre Nagy en Hongrie, d'Alexandre Dubcek en Tchécoslovaquie ou de Mikhaïl Gorbatchev en URSS. Tous échouèrent, « pris entre l'enclume stalinienne et le marteau capitaliste » selon l'expression de Dubcek en 1989. 

IV.1. Les différentes formes de communisme
Le mouvement politique anticapitaliste naît dans les années 1840. Porté par une classe ouvrière démographiquement croissante et pauvre, il se développe plus tard au sein de l'AIT, dont il est l'un des principaux courants. Au lendemain de la défaite de la Commune de Paris en 1871, c'est la scission entre marxistes et anarchistes autour de la question de la méthode pour éliminer la propriété individuelle caractéristique du capitalisme : les marxistes estiment nécessaire une période de transition avec collectivisation des propriétés, sous le contrôle d'État « socialiste » devant dépérir progressivement ; alors que les anarchistes prônent une abolition directe de la propriété, tout en organisant la fédération économique des moyens de production et de consommation. Le mouvement communiste est donc composé à l'origine de deux branches politiques principales : anarchistes communistes et marxistes. 
Sur les différents mouvements historiques et/ou politiques ayant appliqué le communisme ou s'étant référencés au communisme (en tant que théorie économique et sociale), et proposant différents moyens (République, étatisme, fédéralisme, préceptes de la Bible, conseillisme, communalisme, syndicalisme, révolutions, spontanéisme, etc.), plus ou moins complémentaires, pour réaliser ce communisme : 

communisme primitif, sur le mode d'organisation économique de certaines sociétés primitives ou traditionnelles ;
communisme de Platon (dans son dialogue La République),
communisme chrétien (Bêcheux, la théologie de la libération, etc.),
communisme marxiste (Karl Marx, dictature du prolétariat, lutte des classes, Rosa Luxemburg, Antonio Gramsci, Anton Pannekoek, Karl Korsch, Lénine, Trotsky), 
communisme libertaire (Pierre Kropotkine, Errico Malatesta, Prise au tas, Entr'aide, gradualisme révolutionnaire, communalisme libertaire). 



IV.2. Les sociétés présentant des caractéristiques communistes

Le communisme primitif est une société primitive qui aurait existé dans la période préhistorique (toutefois, cette idée est mise en doute par des anthropologues). 
Certaines sociétés ayant un environnement hostile qui impose de fait une société d'entraide (Inuits, tribus amérindiennes). 
Dans la Russie tsariste, la terre appartenait généralement collectivement à la commune (le village) et elle était redistribuée périodiquement aux familles en fonction de leur taille12. C'est exactement le système ayant eu cours pendant la période soviétique pour la redistribution d'appartements (appartenant collectivement à la nation et affectés aux familles selon leurs tailles). 
Les premières communautés chrétiennes auraient fonctionné sur un mode communiste (ce dernier point étant contesté par d'autres chrétiens). Des chrétiens interprètent des passages bibliques comme une indication d'idéal communiste (le partage des biens). Au Moyen Âge, des communautés chrétiennes liées par un idéal religieux mettront leurs biens en commun. Les kibboutzim sont également proches du communisme (collectivisme lié au réseau marchand du reste d'Israël mais avec mise en commun des moyens de production d'ailleurs souvent basés à l'identique sur la structure des sovkhozes soviétiques). 
Des collectivités (en Catalogne, en Aragon, etc.) lors de la révolution espagnole réaliseront, selon des anarchistes et des marxistes notamment conseillistes, pour certaines d'entre elles un début de communisme libertaire. 

La famille est, selon certains communistes chrétiens, un groupement ayant des aspects communistes (bien qu'une majorité des communistes refusent la famille comme réalité communiste, du fait entre autres de l'héritage).



V. Critiques du communisme
Les critiques du communisme visent à la fois les fondements théoriques du communisme et les politiques menées dans les faits par les régimes communistes. 
Lors d'un séjours à la Prison Sainte-Pélagie le 10 août 1883, Jules Guesde en réponse à la vision caricaturale de l'économiste libéral Paul Leroy-Beaulieu envers le socialisme sort trois brochures, le Collectivisme au Collège de France dans lequel il réédite des articles de son journal L'Égalité datant de 1881 et 1882. Ces articles sont regroupés sous le titre ironique de "Leçon à un professeur". Dans son avertissement, il explique pourquoi il ressort ces articles : c'est en effet « à seul fin d'établir que, contre nos conclusions collectivistes ou communistes, il est plus facile de trouver des juges et des géôliers que des argumentations ». 
V.1. Critique sur la révolution
Sur les fondements théoriques, il est reproché au communisme de prôner la « révolution ouverte » ou le « renversement violent » de la société par la révolution et de mettre en place une société fondée sur la contrainte, bien que pour les hommes et femmes de la société, ils existent déjà des contraintes.                 

V.2. Critique sur la propriété
Ainsi, Yves Guyot qualifie-t-il le collectivisme de « tyrannique » en 1893 tandis que le communisme en rejetant la propriété privée s'oppose à la déclaration des droits de l'homme et du citoyen de 1789 : « Le but de toute association politique est la conservation des droits naturels et imprescriptibles de l'homme. Ces droits sont la liberté, la propriété, la sûreté, et la résistance à l'oppression. » Max Stirner, un des fondateurs de l'anarcho-individualisme, écrivait ainsi qu'« en abolissant la propriété personnelle, le communisme ne fait que me rejeter plus profondément sous la dépendance d'autrui, autrui s'appelant désormais la généralité ou la communauté ». 
En 1876, dans une brochure du Parti ouvrier, Jules Guesde leur répond : « Pas de spoilation, mais au contraire, maintien de la propriété réellement personnelle existante, ou création pour les sans-propriété d'aujourd'hui, de la copropriété de demain. Nous sommes aujourd'hui le seul parti plus que défenseurs, créateur de la propriété pour tous. »17. 


V.3.  Critique libérale
Les économistes libéraux Ludwig von Mises et Friedrich Hayek ont également développé une critique du communisme en tant qu'économie planifiée, estimant qu'une « économie communiste » ne pouvait pas exister, en particulier à cause du rejet du mécanisme des prix et de la loi de l'offre et de la demande. Mises écrivit ainsi dans Socialisme en 1922 : « Du fait de la destruction du système des prix, le paradoxe de la « planification » tient à ce qu'il est impossible d'y faire un plan, faute de calcul économique. Ce que l'on dénomme économie planifiée n'est pas une économie du tout. C'est tout juste un système de tâtonnements dans le noir. » L'économiste hongrois János Kornai poursuivit la critique économique du communisme et de l'économie planifiée, condamnée à être une « économie de la pénurie » selon lui, non à cause de défauts temporaires mais à cause des problèmes fondamentaux de la théorie. 

V.4. Critiques des régimes dit communistes

V.4.1. Critique de faits extraordinaires
Le communisme est critiqué pour les conséquences humaines, économiques, politiques ou environnementales entrainées par les régimes communistes. Un collectif d'historiens s'est livré à un recensement des victimes des régimes marxistes-léninistes dans Le Livre noir du communisme et Stéphane Courtois qui en dirigea les travaux écrit dans la préface que « [l]e total approche la barre des cent millions de morts. »18 (chiffre contesté par plusieurs historiens dont une partie des co-auteurs de l'ouvrage). Sont également critiquées les conséquences économiques avec la moindre progression de la prospérité des peuples sous un régime communiste, en particulier à partir de la comparaison entre RDA et RFA ou Corée du Nord et Corée du Sud. Par ailleurs, la remise en cause des libertés fondamentales et le culte de la personnalité des dirigeants ont été soulignés. 
Enfin, les conséquences environnementales comme l'assèchement de la mer d'Aral à la suite des décisions prises par le gouvernement soviétique ont pu être soulignées par certains auteurs. 


V.4.2. Critiques des faits aux quotidiens : les phénomènes communalistes
cf les oeuvres d'Alexandre Zinoviev.
L'originalité de Alexandre Zinoviev est « d'avoir observé la réalité soviétique, d'avoir perçu comment le communiste idéaliste était vaincu par le communisme réel et d'en avoir conclu que la société soviétique excluait tout possibilité de créer le communisme idéal » (in Les confessions d'un homme en trop). 
Bien que les phénomènes communalistes selon Alexandre Zinoviev soient visibles dans toutes les sociétés aussi bien communistes et qu'occidentalistes, c'est en Union Soviétiques et probablement dans tous les pays avec un système social communiste (communiste réel) que ces phénomènes suivant les lois sociales et de la nature humaine jaillissent aux quotidiens dans la vie de millions de gens d'une façon extrême et poussée à outrance. 

« Le stalinisme historique (ou simplement stalinisme) est la forme sous laquelle la société communiste s'est créé en Union Soviétique sous l'impulsion de Staline, de ces lieutenants et de tous ceux qui exécutaient leurs volontés et agissaient conformément à leurs idées et directives (ces derniers peuvent être qualifiés de « staliniens historiques ». La société communiste n'est pas le produit de la volonté d'un homme. Elle surgi en obéissant à des lois sociales objectives, qui se sont révélées à travers l'activité de certains individus, de sorte qu'elles ont prise porte la marque de Staline et des staliniens »19 
Et, « Contrairement à une opinion répandue, la société communiste ne s'est pas formée sur la base d'un projet marxiste, mais en fonction de lois régissant l'organisation des masses humaines en une communauté unique. »20 





VI. Adage communiste

VI.1. Son origine
« De chacun selon ses moyens, à chacun selon ses besoins » est l'adage de groupes politiques ou encore certains syndicats comme la CGT qui l'ont inclut dans la Charte d'Amiens depuis 1912. Il provient de La critique du programme socialiste allemand de Gotha de 1875, écrite par Karl Marx. 
La formule exacte est « De chacun selon ses capacités, à chacun selon ses besoins ! »21 entre guillemets. D'après cette critique, elle doit-être portée ou pourrait-être portée « dans une phase supérieure de la société communiste. » une fois le communisme achevé. Et, Selon Lénine, dans L'État et la Révolution de 1917, « L'État pourra s'éteindre complètement quand la société aura réalisé le principe ». 
Cependant, Lénine pose le problème de « Par quelles étapes, par quelles mesures pratiques l'humanité s'acheminera-t-elle vers ce but suprême, nous ne le savons ni ne pouvons le savoir. »25 
En, 1936, Léon Trotsky va s'en servir comme levier contre le constitution soviétique et plus particulièrement contre le premier titre, « dit De la structure sociale en URSS, qui se termine par ces mots : "Le principe du socialisme : De chacun selon ses capacités, à chacun selon son travail, est appliqué en URSS" ». 26. Staline aurait, donc, trouvé une solution à la réalisation concrète de l'adage par « De chacun selon ses capacités, à chacun selon son travail ». Pour Trotsky, ce système mise en place est La Révolution trahie et « À tous ces égards, l'État soviétique est bien plus prés du capitalisme arriéré que du communisme. »26. 


VI.2. Critique communiste

VI.2.1. Jules Guesde
En 1882, pour Jules Guesde, « de chacun selon ses forces, à chacun selon ses besoins » est un « vieux cliché - prétendu communiste ». 
Dans un article de son journal L'Égalité, il y écrit que cet adage a été détourné « en vain » par « un de ces pères » (de l'adage) qu'est Louis Blanc. Or, Louis Blanc est le créateur des Ateliers nationaux, bien qu'il prônait pour la créations des ateliers sociaux. Ces idées associatives sous l'égide de l'Église ont été dépassées par les idées de Karl Marx et de Proudhon. C'est de Louis Blanc selon l'article, que l'adage a été repris à leur compte par certains socialistes du Parti ouvrier français. Cependant, aujourd'hui, on pense que ces partisans l'aurait reprise de la Gloses marginales au programme du Parti Ouvrier allemand datant de 1875, mais d'une façon changée. 
La formule collectiviste qu'il faut employer pour Jules Guesde est : « De chacun selon les nécessités de la production, à chacun selon son temps de travail. »31. Certains du parti ouvrier de son époque l'opposent donc à leur formule, selon Guesde, "associative" : « de chacun selon ses forces, à chacun selon ses besoins ». 
Pour Jules Guesde : « ce n'est donc pas les intentions qu'il incrimine. Il ne l 'en prends comme toujours -qu'à la conclusion, qui n'est pas seulement fausse, mais pleine de péril. »32. Et « Quant à la société communiste, qui ne deviendra une réalité vivante ... et qui sortira de l'ordre collectiviste avec des producteurs ou des hommes transformés par les conditions nouvelles du travail, elle n'aura pas d'autre devise que celle inscrite par Rabelais à la porte de son abbaye de Thélème : fais ce que vouldras. »32 


VI.2.2. Alexandre Zinoviev
D'après ce que rapporte Alexandre Zinoviev dans Les Confessions d'un homme en trop, cet adage ou une parti de celle-ci, « à chacun selon ces besoins » est souvent discuté à son école de Moscou des années 1930. En effet, puisque le communisme (dit le communisme réel par Zinoviev) a été réalisé pleinement dans la vision politique russe, l'étape suivante est d'aller vers « De chacun selon ses capacités, à chacun selon ses besoins ! » ou plutôt vers sa forme soviétique. 
Dans les années 1930, des questions d'Alexandre Zinoviev, à 12 ou 13 ans telle que « Fallait-il entendre par là n'importe quel besoin ou bien seulement les besoins minimaux? Comment les besoins seraient-ils définis, qui les fixerait et qui contrôlerait leur satisfaction? »34 étaient en effet trop déroutantes et gênantes aux adultes dans cette ambiance que le système a construit. 
Ainsi, Zinoviev ayant vécu dans l'atmosphère en cette Russie de l'époque, rapporte que certaines questions et problèmes ne doivent pas être mis en évidence puisque le communisme réel en URSS est supposé parfait et donc non sujette à la critique ou aux soulèvements. Et, mêmes si les élèves apprennent les histoires des révoltes et révolutions de l'histoire de l'humanité, il faut être esclave pour avoir l'honneur de se soulever. Cependant, dans un monde martelé de perfection, on ne le doit pas, sinon sans être pris, dans ce cas, pour un contre-révolutionnaire ou un renégat. De ce fait et entre autres, ce communisme réel va à l'encontre du communisme idéal.. Ainsi, Zinoviev adulte, remarque que « dans le collectivisme soviétique réel, le principe "à chacun selon son travail" était violé plus souvent qu'il n'était observé » 



VI.3. Conclusion à l'adage communiste
A travers l'histoire de Jules Guesde en 1885 et dans les années 1930 par le témoignage de Alexandre Zinoviev, l'expression qui au départ a été définie selon des valeurs du communisme par Karl Marx peut-être détournée dans sa forme, sa signification et son application. 
En effet, « Marx usait, pour définir la société communiste, de la formule célèbre: "De chacun selon ses forces, à chacun selon ses besoins." Les deux propositions sont indissolublement liées. "De chacun selon ses forces", cela signifie, dans l'interprétation communiste et non capitaliste, que le travail a cessé d'être une corvée, pour devenir un besoin de l'individu ; que la société n'a plus à recourir à la contrainte; que les malades et les anormaux peuvent seuls se dérober au travail. Travaillant selon leurs forces, c'est-à-dire selon leurs moyens physiques et psychiques, sans se faire violence, les membres de la communauté, bénéficiant d'une haute technique, rempliront suffisamment les magasins de la société pour que chacun puisse y puiser largement "selon ses besoins" sans contrôle humiliant. La formule du communisme, bipartite mais indivisible, suppose donc l'abondance, l'égalité, l'épanouissement de la personnalité et une discipline très élevée. »26 



<TXT=w-Formicidae2>

I. Description 
Les premières fourmis connues seraient apparues à la fin du Crétacé et seraient une évolution des guêpes du jurassique. Morphologiquement, elles se distinguent des autres insectes principalement par des antennes avec un coude marqué et par un pédoncule en forme de perle formé des premiers segments abdominaux (qui sont joints au thorax chez les guêpes). Ce pétiole intercalé donne à l'abdomen une plus grande mobilité par rapport au reste du corps (c'est la forme du pétiole qui permet de déterminer l'espèce de la fourmi à coup sûr). À l'exception des individus reproducteurs, la plupart des fourmis sont aptères (sans ailes). Elles se sont adaptées à presque tous les milieux terrestres et souterrains (on en a trouvé jusqu'au fond d'une grotte de 22 km de long en Asie du Sud-est), sans toutefois avoir colonisé les milieux aquatiques et les zones polaires et glaciaires permanentes. 
Les oeufs sont pondus par une ou parfois plusieurs reines (les espèces de fourmis possédant une seule reine sont appelées monogynes et celles possédant plusieurs reines sont dites polygynes). Certaines espèces peuvent tolérer, lorsque la colonie est conséquente, deux reines tellement éloignées qu'elles ne se rencontrent jamais (on parle alors d'espèce olygynes). La plupart des individus grandissent pour devenir des femelles aptères et stériles appelées ouvrières. Périodiquement, des essaims de nouvelles reines et de mâles, généralement pourvus d'ailes, quittent la colonie pour se reproduire. Les mâles meurent ensuite rapidement, tandis que les reines survivantes, fécondées, fondent de nouvelles colonies ou, parfois, retournent dans leur fourmilière natale. 

I.1. Densité de nids
Elle varie fortement selon l'espèce et l'environnement, étant notamment liée à la disponibilité en nourriture. 
La Formica yessensis, une espèce de fourmi des bois, a construit une colonie de 45 000 nids sur 1250 ha à Hokkaido (Japon), abritant plus d'un million de reines et 306 millions d'ouvrières. 


I.2. Développement
Les fourmis se développent par métamorphose complète, en passant par trois stades successifs : oeuf, larve, nymphe (parfois pupe ou cocon, principalement chez les Formicinae) puis adulte (sans croissance à l'état adulte). La larve, privée de pattes, est particulièrement dépendante des adultes. Les larves et les pupes doivent être maintenues à température constante pour assurer leur développement et sont souvent déplacées parmi les diverses chambres de couvée de la fourmilière. Les différences morphologiques majeures entre les reines et les ouvrières, et entre les différentes castes d'ouvrières quand elles existent, sont induites par le régime alimentaire au stade larvaire. Quant au sexe des individus, il est génétiquement déterminé : si l'oeuf est fécondé, l'individu est alors diploïde et l'oeuf donnera une femelle (ouvrière ou reine); s'il ne l'est pas, l'individu est haploïde et forme un mâle 
Une nouvelle ouvrière passe les premiers jours de sa vie adulte à s'occuper de la reine et des jeunes. Ensuite, elle participe à la construction et au maintien du nid, puis à son approvisionnement et à sa défense. Ces changements sont assez brusques et définissent des castes temporelles[réf. nécessaire]. C'est-à-dire que les ouvrières se regroupent selon l'activité commune qu'elles auront à un stade de leur vie. 
Chez certaines fourmis, il existe également des castes physiques. Selon leur taille, les ouvrières sont mineures, moyennes ou majeures, ces dernières participant plutôt à l'approvisionnement. Souvent les fourmis les plus grandes sont disproportionnées : tête plus grande et mandibules plus fortes. Chez quelques espèces, les ouvrières moyennes ont disparu, et il existe une grande différence physique entre les petites et les géantes, appelées parfois soldats bien que leur rôle défensif ne soit pas nécessairement prépondérant. 


I.3. Type de morphologie
Parmi les 11 800 espèces connues environ (on estime à plus de 20 000 le nombre total d'espèces), la plus grande (30 mm de long) est Dinoponera quadriceps chez laquelle la reproduction d'une ouvrière aboutit, invariablement, à la mort en pleine action de son soupirant : encore accouplée, elle lui sectionne l'abdomen. Puis retourne au nid, toujours munie des pièces génitales de sa brève rencontre, ce qui la rend non réceptive aux avances des autres mâles. 
Toutes sortes de comportements sont observés chez les fourmis, le nomadisme en est l'un des plus remarquable. Les fourmis légionnaires d'Amérique du Sud et d'Afrique notamment ne forment pas de nid permanent, mais alternent plutôt entre des étapes de vie nomade et des étapes où les ouvrières forment un nid provisoire (le bivouac) à partir de leurs propres corps. La plupart des fourmis forment des colonies stationnaires, creusant d'habitude dans le sol ou une cavité. Les colonies se reproduisent par des vols nuptiaux comme décrit plus haut, ou par la fission (un groupe d'ouvrières creuse simplement un nouveau trou et élève de nouvelles reines). Les membres de différentes colonies sont identifiés par l'odeur et habituellement les intrus sont attaqués, avec des exceptions notables. D'autres méthodes de développement de nouvelles colonies ont été observées : 

Quelques fourmis sont esclavagistes, comme les Formica sanguinea, et pillent le couvain des autres espèces en faisant de véritables raids dans les colonies d'autres fourmis, s'emparent de pupes, cocons et nymphes qui sont traitées comme le couvain génétiquement parent, nourries, choyées, protégées. Une fois nées, les ouvrières esclaves ne se rendent compte de rien, et pensent être dans leur fourmilière d'origine. Elles se mettent donc tout naturellement au travail. Il arrive parfois qu'une reine d'une autre espèce soit prise en esclavage, la fourmilière disposera donc pendant une vingtaine d'années d'esclaves à profusion. Quelques espèces, comme les fourmis amazones (Polyergus rufescens), sont devenues complètement dépendantes de telles esclaves, au point d'être incapables de s'alimenter sans leur aide.
Les fourmis pot-de-miel, ont des ouvrières spécialisées appelées replètes qui stockent simplement l'alimentation pour le reste de la colonie ; elles sont généralement immobilisées par leurs abdomens considérablement gonflés. En Afrique, Amérique (Myrmecocystus) et Australie où elles vivent, on les considère comme un mets délicieux. 
Les fourmis tisserandes (Oecophylla) construisent leur nid dans des arbres en attachant des feuilles ensemble, d'abord en les joignant par un pont d'ouvrières puis en les collant ensemble avec de la soie produite par des larves. 
Les coupeuses de feuilles (Atta) se nourrissent, pour une part importante, d'un champignon symbiotique qui se développe uniquement dans leurs colonies. Elles récoltent continuellement des feuilles dans lesquelles elles découpent de petits morceaux qui servent à cultiver le champignon. Les castes de ces fourmis sont organisées autour de la découpe des feuilles et en fonction de la taille des morceaux dont elles sont chargées. 
Les fourmis charpentières (certaines espèces du genre Camponotus) font leurs nids en creusant le bois. Elles varient en taille (polymorphisme), elles mesurent, en général, plus d'un centimètre, elles comptent parmi les plus grandes espèces d'Europe. 
Les fourmis moissonneuses (Messor sp.) du Bassin méditerranéen amassent des graines de graminées sauvages et cultivées, parfois par tonnes, dans des "greniers" souterrains. Les fourmis adultes (ouvrières et guerrières) décortiquent et mâchent chaque grain pendant plusieurs heures, de façon à en obtenir une pâte comestible. 
Les fourmis "pestes", envahisseuses ou encore nuisibles, sont des espèces venues de pays lointains qui envahissent une nouvelle région et s'installent de telle manière qu'on ne puisse les chasser. Les plus connues en France sont les fourmis d'Argentine. Cette espèce particulièrement remarquable par sa petite taille (1-3 mm)et très agressive, a formé une super-colonie de Barcelone à Milan. Les différentes fourmilières, contrairement aux autres espèces, sont alliées entre elles et par conséquent inarrétables quand elles forment de très grandes colonies. Cette espèce introduite en France par des pots de Lauriers roses venus d'Argentine à déjà chassé plusieurs espèces d'autres insectes du sud du pays (dorandillula en particulier). 
À noter qu'une espèce était classée espèce protégée en France auparavant (elle l'est toujours dans plusieurs pays européens), car elle est utile dans son environnement : Formica rufa. Sa présence au sein d'une forêt, protège les arbres du développement d'insectes ravageurs. Une colonie mature capture, en été, pas moins de 1kg d'insectes par jour et autant de miellat. La fourmilière de ces dernières constitue un dôme de brindilles pouvant atteindre plus d'un mètre de haut, souvent en lisière de forêt ou de clairière. Le dôme permet une régulation de la température interne et une exposition optimisée aux rayonnements solaires, favorisant ainsi une croissance rapide du couvain. Fait, notable, certaines espèces de Fourmis rousses peuvent s'associer en de supercolonie. L'utilisation de feuilles de résineux ou de particules de résines contribue à la désinfection du nid. 

Concernant la reproduction, la Wasmannia auropunctata a la possibilité assez exceptionnelle d'avoir deux modes de multiplication : la reproduction ou la multiplication asexuée par clonage.

I.3.1. Fourmi sans reine
Un pour cent des espèces de fourmis recensées dans le monde sont des fourmis sans reine. Elles vivent dans des colonies très réduites où des ouvrières se reproduisent de temps à autre. On peut citer Streblognathus peetersi, une fourmi vivant en Afrique.
Citons :

Dinoponera quadriceps 
Diacamma ceylonense 
Gnamptogenys striatula 
Streblognathus peetersi 
Streblognathus aethiopicus

Le privilège de la reproduction est le fruit d'une organisation hiérarchique, où la gamergate, individu dominant de la colonie, occupe cette place centrale. Son privilège reproductif pourra être remis en cause par des rivales au cours de joutes phéromonales et d'agressions ritualisées.


I.3.2. Sous-familles

I.3.2.a. Sous-famille des Ponérinés
Chez les Ponérinés, les reines ne se distinguent généralement que difficilement des ouvrières ; le passage d'une caste à l'autre se fait plutôt par des formes de transition. Elles diffèrent des autres fourmis par la base de l'abdomen : le pétiole se compose d'un segment avec un noeud, et l'anneau abdominal qui suit est séparé du gastre par une encoche très nette. Reines et ouvrières possèdent un aiguillon. Les nymphes sont toujours enveloppées par un cocon. Cette sous-famille habite surtout les pays chauds. En France, elle est représentée par 7 espèces. 
Espèces particulièrement connues en France : Ponera coarctata (fait partie des "Fourmis sans reine" citées plus haut).


I.3.2.b. Sous-famille des Myrmicinés
Les Myrmicinés se distinguent facilement des autres fourmis par leur pétiole abdominal. Il se compose toujours de deux segments en forme de noeuds qui correspondent aux 1er et 2nd segments abdominaux. Reines et ouvrières possèdent un aiguillon, et certaines espèces peuvent infliger des piqûres très douloureuses. Les nymphes ne sont pas enveloppées d'un cocon comme chez la plupart des fourmis à écaille (myrmicinés, dolichodérinés, formicinés). En France, on trouve 106 espèces de Myrmicinés. 
Espèces particulièrement connues en France : Myrmica scabrinodis, Myrmica Rubra, Tetramorium caespitum, Leptothorax Tuberum, Diplorhoptrum fugax ( ou Solenopsis fugax), Crematogaster scutellaris, Pheidole pallidula, Messor sp. 


I.3.2.c. Sous-famille des Dolichodérinés
Les représentants de cette sous-famille peu nombreuse (9 espèces en France) possèdent un pétiole à écaille, mais celle-ci est basse et inclinée vers l'avant, contrairement à celui des Formicinés, que nous verrons par la suite. Le gastre, ou abdomen, n'est composé que de 4 segments chez les reines et ouvrières. Aiguillon atrophié, nymphes nues. 
Espèces particulièrement connues en France : Tapinoma erraticum.


I.3.2.d. Sous-famille des Formicinés
Chez les Formicinés, le pétiole entre thorax et abdomen forme une écaille plate et dressée. Le gastre, derrière le pétiole, se compose de 5 segments chez les ouvrières et les reines, contraiment aux dolichodérinés. L'aiguillon est atrophié mais les glandes à venin sont totalement développées ; l'acide formique est rejeté l'abdomen relevé, après que les mandibules aient infligées une blessure. Chez presque toutes les espèces, les nymphes sont enveloppées d'un cocon. Ce cocon ne fait défaut que chez les Camponotus truncatus, une espèce rare. 55 espèce des Formicinées sont présentes en France. 


I.3.2.e. Genre Lasius, appartenant aux fourmis Formica
Très important, il mérite un paragraphe. Ce genre comprend de petites espèces dont les ouvrières ne possèdent en général que des ocelles à peine développées. Les articles des antennes, du 2nd au 6e, sont toujours nettement plus courts que l'avant-dernier d'entre eux. La plupart des espèces se nourrissent principalement de miellat (de pucerons ou de conchenilles). Ces fourmis sont les traditionnelles fourmis noires des jardins, qui apprécient les fruits et les liquides sucrés. 
Espèces particulièrement connues en France : Camponotus ligniperda, Lasius sp., Formica rufa, Formica sanguinea, Polyergus rufescens. 

Source : Cette partie de l'article, à partir de "Sous-familles", vient de "Identification de Sous-familles", par Grey, forum akolab et forum myrmecofourmis. Travail de documentation à partir de documents de Bert Hölldobler, Luc Passera.




II. Comportements
Les fourmis possèdent un comportement que l'on retrouve chez les poussins consistant à rassembler un grand nombre d'individus afin de créer une colonie fonctionnelle et rapide. 

II.1. Communication
La communication entre les fourmis se fait surtout au moyen de produits chimiques volatiles appelés phéromones, émises par diverses glandes, parfois dans une substance lipophile qui recouvre naturellement tout le corps de la fourmi. Comme d'autres insectes, les fourmis sentent avec leurs antennes. Celles-ci sont assez mobiles, ayant - comme mentionné plus haut - une articulation coudée après un premier segment allongé (le scape), leur permettant d'identifier aussi bien la direction que l'intensité des odeurs. Ce système d'orientation olfactif est combiné avec des composantes visuelles (points de repère, position du soleil), capacité à mesurer la distance parcourue. 
L'utilisation principale des phéromones réside dans la définition et le repérage de « pistes » olfactives destinées à guider les fourmis vers des sources de nourriture (voir ci-dessous). Les phéromones sont aussi mélangées avec la nourriture échangée par trophallaxie, informant chacune sur la santé et la nutrition de ses congénères. Les fourmis peuvent aussi détecter à quel groupe de travail (par exemple le fourragement ou la maintenance de nid) l'une ou l'autre appartient. De même, une fourmi écrasée ou attaquée produira une phéromone d'alerte dont la concentration élevée provoque une frénésie agressive chez les fourmis à proximité ou dont une concentration plus faible suffit à les attirer. Dans certains cas, les phéromones peuvent être utilisées pour tromper les ennemis, ou même pour influencer le développement des individus. Ainsi, la reine produit une phéromone spéciale en l'absence de laquelle les ouvrières commenceront à élever de nouvelles reines. 
Certaines fourmis émettent des sons, on parle alors de stridulations (friction de la râpe, formée d'un alignement de côtes, de stries, de dents, d'épines, et du grattoir, qui consiste en une saillie ou un bord vif, qui produit la stridulation, un peu comme le ferait un clou grattant sur une lime ou l'ongle passant sur les dents d'un peigne). Ces sons permettent alors d'attirer d'autres ouvrières pour, par exemple, porter une proie trop lourde pour un individu isolé. Cette méthode est toutefois moins efficace que la piste de phéromones, comme l'a montré G.D dans sa fameuse expérience du même nom. 
D'autres utilisent aussi la communication visuelle, de moins en moins répandue. Chez les Tetraponeras par exemple, lorsque les larves ont un besoin en nourriture, elles remuent simplement la tête pour que, rapidement, une ouvrière intervienne pour lui ingurgiter de la nourriture liquide de bouche à bouche. Chez les Tisserandes, lorsqu'une ouvrière se lance dans la construction d'un nouveau nid, elle commence par agripper une feuille pour la courber. Elle sera immédiatement rejointe par son entourage qui aura aperçu la scène et qui l'aidera dans sa tâche. C'est ainsi qu'elles pourront rejoindre les bords de deux feuilles pour les tisser entre elles. 


II.2. La trophallaxie
La majorité des fourmis pratiquent la trophallaxie, le processus alimentaire au cours duquel une fourmi régurgite une partie de la nourriture qu'elle a ingérée dans son jabot social pour la restituer à une autre fourmi. Le genre Messor a la particularité de n'avoir pas de jabot social et de ne pas faire de trophallaxies.


II.3. Comportement collectif
Les fourmis attaquent et se défendent en mordant et, pour certaines espèces, en projetant de l'acide formique (formicinae) qui fait fondre la chitine des insectes, ou d'autres substances pouvant engluer un adversaire, ou encore en piquant à l'aide d'un aiguillon (qui chez quelques espèces reste piqué avec la glande à venin dans la peau de la victime). 
Chez la plupart des espèces, la colonie a une organisation sociale complexe et est capable d'accomplir des tâches difficiles (exploiter au mieux une source de nourriture, par exemple). Cette organisation apparaît grâce aux nombreuses interactions entre fourmis, et n'est pas dirigée - contrairement à une idée répandue - par la reine. On parle alors d'intelligence collective, pour décrire la manière dont ce comportement collectif complexe apparaît, grâce à des règles individuelles relativement simples. 
Dans les colonies de fourmis, le « comportement global » n'est donc pas programmé chez les individus, on dit qu'il émerge de l'enchaînement d'un grand nombre d'interactions locales entre les individus et leur environnement. 
Un exemple classique de comportement collectif auto-organisé est l'exploitation des pistes de phéromones. Une fourmi seule n'a pas l'intelligence nécessaire pour choisir le plus court chemin dans un environnement complexe. De fait, c'est la colonie dans son ensemble (du moins, les individus impliqués dans le fourragement) qui va choisir ce chemin. 
En 1980, Jean-Louis Deneubourg a pu vérifier expérimentalement qu'une colonie de fourmis (de l'espèce Lasius niger) disposant de deux chemins de longueurs différentes pour rallier une source de nourriture, choisissait plus souvent le chemin le plus court. Il décrit ainsi ce phénomène : 

« (...) un « éclaireur », qui découvre par hasard une source de nourriture, rentre au nid en traçant une piste chimique. Cette piste stimule les ouvrières à sortir du nid et les guide jusqu'à la source de nourriture. Après s'y être alimentées, les fourmis ainsi recrutées rentrent au nid en renforçant à leur tour la piste chimique. Cette communication attire vers la source de nourriture une population de plus en plus nombreuse. Un individu qui découvre une source de nourriture y « attire » en quelques minutes n congénères (par exemple 5) ; chacun de ceux-ci y attirent à leur tour n congénères (25), et ainsi de suite. »

Si l'on considère plusieurs chemins pour se rendre sur le lieu d'approvisionnement, on comprend que les individus empruntant le plus court reviendront plus vite à la fourmilière que ceux qui auront pris le plus long. C'est ainsi que ce chemin comportera une trace olfactive de plus en plus forte par rapport aux autres et sera donc préféré par les fourmis. 
On connaît depuis d'autres exemples de ce type, comme la construction du nid, la répartition du couvain dans celui-ci, l'entassement des cadavres de la colonie, l'organisation en « supercolonies », etc. 


III. Chronologie

III.1. Répartition
Une estimation du nombre de fourmis vivant aujourd'hui sur terre à un instant donné est environ 10 millions de milliards d'individus. Les fourmis constitueraient 1 à 2 % du nombre d'espèces d'insectes, mais près de 20% de leur biomasse. Chaque individu ne pèse que de 1 à 10 milligrammes, mais leur masse cumulée est environ quatre fois supérieure à celle de l'ensemble des vertébrés terrestres.. Environ 12 000 espèces de fourmis sont répertoriées en 2005, mais on en découvre régulièrement, essentiellement en zone tropicale et dans la canopée (qui n'est explorée que depuis quelques dizaines d'années). Seules 400 espèces sont connues en Europe, alors qu'on peut compter jusqu'à 40 espèces différentes sur un seul mètre carré de forêt tropicale en Malaisie (668 espèces comptées sur 4 hectares à Bornéo, et 43 espèces sur un seul arbre de la forêt péruvienne amazonienne, soit presque autant que pour toute la Finlande ou les îles Britanniques). Environ huit millions d'individus ont été comptés sur un hectare d'Amazonie brésilienne[réf. nécessaire], soit trois à quatre fois la masse cumulée des mammifères, oiseaux, reptiles, et amphibiens vivant sur cette surface. Elles jouent un rôle majeur dans le recyclage des espèces et dans la formation et la structuration des sols. Plusieurs espèces vivent en symbiose avec des bactéries, des champignons, des animaux (papillons ou pucerons par exemple) ou avec des arbres ou des fleurs.


III.2. Relations de coopération et de prédation

Des pucerons sécrètent un liquide sucré appelé le miellat. Normalement il tombe au sol, mais certaines fourmis s'en nourrissent. Les fourmis tiennent à distance les prédateurs des pucerons et les transportent aux meilleurs emplacements pour se nourrir. Certaines les accueillent au sein même de la fourmilière, pour les espèces se nourrissant sur les racines des plantes. Les fourmis sont donc les seuls animaux connus à posséder, tout comme l'homme, des animaux domestiques. 
Des chenilles myrmécophiles ou aimant la fourmi (généralement bleues, cuivrées, ou aux poils rayés) sont mises en pâture comme du bétail par les fourmis le jour, et sont ramenées à l'intérieur du nid des fourmis la nuit. Ces chenilles ont une glande qui sécrète le miellat quand les fourmis les massent . 
Quelques chenilles myrmécophages (se nourrissant de fourmis) sécrètent une phéromone qui fait que les fourmis prennent la larve pour une des leurs. Les chenilles sont alors emportées dans le nid où elles peuvent se nourrir de larves de fourmi . 
D'autres espèces de chenilles sécrètent une phéromone les faisant passer pour des larves de fourmis. Elles peuvent ainsi se développer en étant protégées et nourries par la colonie. C'est une forme de parasitisme . 



III.3. Résistance
Les fourmis produisent naturellement, notamment pour protéger leurs oeufs et leurs cultures des champignons, des insecticides, des fongicides, des bactéricides, des virucides et une batterie de molécules complexes dont les fonctions ne sont pas toutes connues. Elles font partie des premières espèces pionnières et montrent des capacités étonnantes de terrassement, de colonisation et de résilience écologique, et même de résistance à la radioactivité.


III.4. Rôle environnemental

III.4.1. Terrassement
Les ouvrières de l'espèce Atta d'un seul nid peuvent mobiliser et répartir sur 100 mètres carrés jusqu'à 40 tonnes de terre. Certaines espèces jouent un rôle au moins aussi important que celui des lombrics pour les couches superficielles du sol ; ce sont de 400 à 800 kg de sol qui sont creusés, mobilisés, transportés, maçonnés pour construire un nid climatisé dans le désert, et 2,1 tonnes en Argentine par Camponotus punctulatus. De nombreuses espèces décolmatent et acidifient le sol rendant mobilisables des nutriments autrement moins biodisponibles. Elles enfouissent de la matière organique et remontent en surface un sol fragmenté en petites particules propices à la croissance des graines. Les fourmis contribuent à la fois à homogénéiser et aérer le sol, à l'enrichir en surface et en profondeur, tout en diversifiant les habitats en fonction de la proximité de la fourmilière. 


III.4.2. Fonctions écologiques
Les fourmis jouent un rôle pédologique majeur, elles protègent certains arbres de parasites. La fourmi rousse des bois Formica polyctena est ainsi protégée par la loi dans plusieurs pays, à juste titre puisqu'elle consommerait 14 500 t d'insectes par an, rien que dans les forêts alpines d'Italie, conservant des « îlots verts » autour de leurs nids lors des épisodes de défoliation). 
D'autres espèces cultivent des parasites des plantes (pucerons ou cochenilles dont elles exploitent le miellat) Elles protègent aussi certaines espèces qui leur fournissent abri ou nourriture. Elles contribuent à disperser et à faire germer de nombreuses graines, près de 100 % des graines d'une euphorbe méditerranéenne sont transportées par 3 ou 4 espèces de fourmis qui consomment l'élaiösome charnu et gras de la graine en rejetant le reste, sans affecter sa capacité germinative[réf. nécessaire]. Dans un même environnement, une prairie avec fourmilières est plus productive que celle qui en est dépourvue. De nombreuses épiphytes dépendent des fourmis ou sont favorisées par leur présence. Pour les attirer, ces épiphytes leur offrent du nectar et/ou un abri en échange d'une protection contre divers prédateurs et parfois d'une aide à la dispersion des graines (certaines fourmis (Crematogaster ou Camponotus) végétalisent leurs nids et fabriquent des jardins suspendus en incorporant des graines d'épiphytes dans les parois de leurs nids faits de fibres ou pulpe de bois mâchées) Elles défendent activement leurs jardins et en tirent un nectar extrafloral, un abri supplémentaire et peut-être une protection microclimatique. 
Certaines espèces causent cependant des dégâts à certaines plantes cultivées par l'élevage des pucerons et cochenilles. Des espèces introduites et très invasives ne sont pas combattues par les fourmis locales du pays d'arrivée (elles ne les reconnaissent pas comme dangereuses). C'est une cause de régression de la biodiversité, par régression ou disparition d'espèces de fourmis concurrentes ou d'espèces d'autres règnes. 


III.4.3. Fonctions agronomiques ou pour l'agrosylviculture
Certaines espèces de fourmis tisserandes sont depuis longtemps introduites dans les cultures fruitières pour défendre les fruits d'attaques d'insectes, des fourmis du genre Ectatomma à petits effectifs mais à nids nombreux (11 000 nids/ha comptabilisés dans les plantations de café ou cacao au Chiapas au Mexique patrouillent en permanence et mangeraient annuellement 16 millions de proies pour Ectatomma tuberculatum et 15 fois plus (260 millions) pour Ectatomma ruidum.les Solenopsis invicta défendent la canne à sucre de certains parasites majeurs, comme la Wasmannia auropunctata protège les cocotiers des punaises, mais ces espèces sont souvent invasives et provoquent des piqûres très douloureuses.


III.4.4. Fonction sanitaire
Les fourmis jouent un rôle majeur de nécrophage, même en pleine ville et en zone tempérée pour des oiseaux, rats, souris et autres petits animaux morts par exemple. En nettoyant rapidement les cadavres dont elles ne laissent souvent que les os, cuticules dures ou arêtes elles empêchent la libération dans l'environnement de nombreux propagules de microbes pathogènes. 
On estime que 90 % au moins des cadavres d'insectes, dans la nature finissent dans des fourmilières, avant d'être recyclés dans le sol. 
Les fourmis se nettoient sans cesse et s'enduisent, elles, leurs reines ainsi que leurs oeufs de molécules bactéricides, virucides et antifongiques. Les fourmis chargées d'éliminer les cadavres du nid, les excréments et autres déchets sont souvent des ouvrières en fin de vie ou des individus qui restent dans les endroits consacrés aux déchets et n'ont plus de contacts directs avec les autres fourmis. Certaines espèces s'enduisent de bactéries filamenteuses "amies" qui repoussent d'autres bactéries, pathogènes. Cependant, leurs élevages de pucerons peuvent induire l'infestation des plantes par des champignons, via le miellat ou les piqûres faites dans les feuilles. 


III.4.5. Autres fonctions
L'industrie, pharmaceutique notamment, s'intéresse aux nombreuses substances synthétisées par les fourmis. Des fourmilières reconstituées et circulant dans des salles et couloirs de plastique sont utilisés comme moyen pédagogique. La fourmi en tant qu'individu ou société intéresse également les cybernéticiens ou les scientifiques qui travaillent sur l'auto-organisation.




IV. Menaces
Certaines pollutions, dont celles par les pesticides affectent de nombreuses espèces, mais c'est surtout l'introduction d'autres espèces de fourmis, invasives, et la destruction de leurs habitats (forêts, prairies, savanes et brousses tempérées, savanes, bocage) qui sont les premières menaces. Leurs prédateurs naturels sont nombreux, des mouches parasites, aux mammifères tels que le pangolin ou le tamanoir qui sont des consommateurs spécialisés, de nombreux animaux les consomment épisodiquement, le faisan ou l'ours brun en Europe, ou encore les chimpanzés, qui savent utiliser des brindilles pour aller les chercher dans leur nid, sans jamais mettre en péril les espèces, semble-t-il. 
Les fourmis arboricoles se déplaçant le long des branches ou sur les feuilles dans la canopée de la forêt courent le risque d'être balayées par le vent, la pluie, ou même par un singe qui passe. On a observé en 2005 que les fourmis arboricoles survivent en se comportant en "parachutistes". Lorsqu'elles tombent, elles se mettent en position pattes écartées, comme les parachutistes qui contrôlent leur chute en inclinant leurs membres et leur corps. Ces fourmis glissent avec les pattes antérieures et l'abdomen orientés vers le tronc d'arbre, effectuant souvent des virages à 180° en direction de la cible dans les airs. 


V. Histoire de l'espèce
Les Formicidae sont nés il y a 120 millions d'années depuis des insectes apparentés aux guêpes. Dès lors, de nombreuses espèces sont apparues en se spécialisant aussi bien pour la vie souterraine que arboricole, voire les deux. La sous-famille Martialinae, dont la seule espèce membre en 2008 est Martialis heureka, pourrait être à l'origine de toutes les autres sous-familles.


VI. Liste des espèces de fourmis françaises les plus fréquentes

Aphaenogaster senilis 
Crematogaster scutellaris
Camponotus ligniperdus
Formica fusca
Lasius niger
Messor barbarus
Messor structor
Myrmica rubra
Pheidole pallidula
Tetramorium caespitum



VII. La fourmi et l'homme
Les rapports entre humains et fourmis sont très variables. D'une part, les fourmis ont souvent été utilisées dans des fables et des histoires enfantines pour représenter l'acharnement au travail et l'effort coopératif. Elles peuvent aussi être perçues comme utiles pour nettoyer des insectes parasites et aérer le sol. D'autre part, elles peuvent devenir sources de nuisances mineures ou parasites elles-mêmes quand elles envahissent les maisons, les cours, les jardins et les champs. La fourmi Tetraponera colonise un arbre creux le Barteria surnommé au Gabon l'arbre de l'adultère. On y attachait les femmes adultères dans le temps. La morsure d'une fourmi étant aussi douloureuse que celle d'une guêpe mais moins durable. 
Avec la mondialisation des échanges commerciaux et des transports, plusieurs espèces sont devenues invasives. Une certaine espèce, appelée fourmi tueuse, a tendance à attaquer des animaux beaucoup plus grands qu'elle dans sa quête de nourriture ou dans la défense de ses nids. Les attaques sur l'homme sont rares, mais les piqûres et les morsures peuvent être très douloureuses et incapacitantes si elles sont répétées, avec un choc anaphylactique possible pour quelques espèces dangereuses. Les fourmis peuvent aussi être source de problème lorsqu'elles sont introduites dans des zones géographiques où elles ne sont pas indigènes (comme Linepithema humile, la fourmi d'Argentine, formant la supercolonie qui va des côtes italiennes aux côtes espagnoles en passant par la France, soit plus de 6 000 km12, et exterminant les espèces indigènes). Les fourmis de feu peuvent par exemple attaquer et tuer de jeunes alligators du Mississippi au sortir de l'oeuf. 


VIII. L'invasion de la fourmi d'Argentine
La fourmi d'Argentine ou Linepithema humile, décrite pour la première fois en 1868, a profité des échanges commerciaux pour s'expatrier et coloniser le Sud des États-Unis dès 1891, l'Europe en 1904, l'Afrique du Sud en 1908 et l'Australie en 1939. Il est probable qu'elle atteignit les côtes méditerranéennes en 1920 par le biais de plantes à fleur. 
En 2002, des entomologistes européens ont constaté que la fourmi d'Argentine avait envahi l'Europe du Sud sur 6 000 km du nord de l'Italie jusqu'à la Galice et le Portugal, en passant par le sud de la France. Cette colonie est la plus grande jamais observée dans le monde. La deuxième se situe en Catalogne. 
Le changement d'environnement de ces fourmis serait à l'origine de leur très grande cohésion. En effet, lorqu'elles sont en Argentine, les colonies de Linepithema Humile ne comptent qu'un seul nid. C'est l'absence de prédateur en Europe qui a permis à ces fourmis d'augmenter la densité des nids et donc les échanges entre les ouvrières de ceux-ci, entraînant un appauvrissement de la diversité génétique des gènes de reconnaissance des individus au sein de leur nid. Les fourmis d'Argentine apprirent la diplomatie, et les différents nids ne s'entretuèrent plus. Au fil du temps, la densité des nids permit la création d'une supercolonie, et deux individus d'un bout à l'autre de cette mégalopole de fourmis peuvent se reconnaître au premier coup de phéromones, comme étant de la même fratrie. 
Les fourmis d'Argentine ne sont pas dangereuses pour l'homme mais elles nuisent à l'écosystème originel de l'Europe du Sud: elles détruisent les bourgeons des arbres et prennent la place des fourmis européennes. La seule façon d'empêcher l'expansion de cette supercolonie serait de détruire l'esprit de l'unicolonialité qui unit les nids de fourmis. Cet esprit d'équipe est condamné à disparaître une fois l'objectif de la super colonie atteint: coloniser un maximum de territoire. La deuxième supercolonie en Catalogne serait plus belliqueuse que la première et pourrait bien chercher à l'éliminer. 
En 2004, des scientifiques américains ont remis en cause l'idée d'appauvrissement génétique. L'étude de Deborah Gordon sur une supercolonie présente en Californie, publiée dans la revue Ecology, a révélé que la coopération des fourmis aurait donc pour origine un régime alimentaire commun. 



<TXT=w-Guerre3>

I. Définitions et explications de la guerre

I.1. En anthropologie
Pour l'anthropologue René Girard la guerre est une forme de rivalité mimétique entre groupes, dans laquelle la violence est orientée vers l'extérieur de la communauté. L'anthropologue Marvin Harris 2 de la Columbia University a proposé une théorie sur les origines de la guerre dans les sociétés non-étatiques, tribales et villageoises. L'idéologie dominante dans notre société tend à blâmer l'individu pour la guerre sur la base supposément biologique d'une « violence innée » de la « nature humaine » (le péché originel) ou de l'« instinct de mort ». C'est un point de vue simple et simpliste qui nous lave de toute responsabilité dans notre conduite envers autrui. Si la guerre était naturelle, il n'y aurait pas besoin de tant d'efforts de propagande pour dresser les uns et les autres à s'entretuer. Le dressage ici se rapporte à ce que l'Anglais nomme par « basic training » dès l'enfance dans la famille, la parenté, l'école, le milieu social et à travers les jeux et les divertissements apparemment les plus inoffensifs, le rejet et le déni de l'autre, la compétition et la coopération. 
Harris répertorie quatre théories, selon lui les plus communes sur l'origine de la guerre :

la guerre comme solidarité,
la guerre comme jeu,
la guerre comme nature humaine
la guerre comme continuation de la politique (Cf. Clausewitz entre autres).

Dans cette perspective et en couvrant à la fois les sociétés non-étatiques et les sociétés étatiques, la guerre apparaît comme la forme et le moment (à la fois comme instant et comme rapport de forces) de violence extrême d'un vol organisé dont l'objet peut être physique, imaginaire ou symbolique.

I.1.1. La guerre comme facteur de cohésion sociale
Tant du point de vue de l'attaquant que de l'attaqué, la guerre semble pouvoir contribuer à maintenir ou restaurer la cohésion sociale d'un groupe ou pays. Car le fait est que dans l'Histoire, nombre de guerres furent déclenchées sous un prétexte dans le but unique (et souvent réussi) de resserrer les rangs derrière le destin supérieur de la patrie en cherchant l'« union sacrée », et soutenir celui qui apparait alors comme son meilleur défenseur : le chef. Ainsi Otto von Bismarck qui rêvait de l'unité allemande incita t-il la guerre franco-allemande de 1870. Rappelons que l'Allemagne n'était à l'époque qu'une confédération d'États indépendants, et que la guerre permit d'unifier l'Empire allemand sous la couronne prussienne, ce qui fut l'avènement de l'Allemagne bismarckienne qui domina seule l'Europe continentale pendant près de trente ans. 


I.1.2. La guerre comme jeu
Harris veut démontrer que les gens, les hommes surtout, sont élevés dans le culte et la croyance de la guerre comme une activité anoblissante, flamboyante et glorieuse, avec un substitut qui est la compétition sportive collective. L'histoire montre qu'on peut être élevé à prendre plaisir à pourchasser d'autres personnes et à les tuer, à les détester et les haïr ou bien à se révolter contre les résultats de tels actes. Si on croit que les valeurs belliqueuses sont sources des guerres, alors le problème crucial et critique devient celui de spécifier les conditions dans lesquelles des personnes sont amenées à valoriser et à révérer la guerre. La théorie de la guerre comme jeu trouve là sa limite. Comme activité ludique, le jeu est une représentation du type « théâtral » et prépare à la guerre en la glorifiant et en la valorisant.


I.1.3. La guerre comme nature humaine
Du point de vue de la nature humaine décrite par ses "pulsions" d'origines génétiques, biologiques et/ou culturelles acquises, la pulsion de meurtre pourrait ou voudrait expliquer, au-delà de la pulsion de mort que l'humain est programmé pour tuer. « Instinct », comme ailleurs « Dieu » seraient alors des principes explicatifs passe-partout pour justifier absolument et définitivement ce que nous ne comprenons pas. 
La théorie de l'instinct de mort ou pulsion de mort néglige (aussi bien dans la signification française de « ne pas savoir » que dans la signification anglaise de « ne pas vouloir savoir ») l'environnement bio-physico-chimique et le contexte culturel, historique et social dans lesquels les tueries et les guerres prennent place. L'argument de la « nature humaine », réincarnation du déterminisme génétique de la sociobiologie qui va aussi loin que proclamer le viol comme un acte logique dans l'intérêt du « succès reproductif » du violeur, se contredit lui-même car guerre et tueries ne sont pas universellement et de tous les temps admirées et pratiquées par les humains. 
De plus, il y a d'énormes distinctions entre les « lois de la guerre » (à différentes époques et dans différentes sociétés) et, par ailleurs la quantité de violences distribuée. La théorie d'un universel « instinct de meurtre » est insoutenable même dans une société en guerre. 
L'être humain est bien entendu capable de devenir dangereusement agressif en apprenant à jouir et à se réjouir de la guerre et de l'exercice de la cruauté. Mais, « comment et quand nous devenons agressif sont plutôt sous le contrôle de nos cultures que de nos gènes » écrit Harris (p. 54), dans le vieux débat scientifique entre l'inné et l'acquis (ou du déterminisme génétique contre le déterminisme culturel). 



I.2. Conflit et métaphores dans le monde animal
De nombreux animaux grégaires ont des comportements d'agression qui, lorsqu'ils s'expriment collectivement, peuvent évoquer la guerre. Il s'agit généralement d'animaux territoriaux qui disposent aussi parfois de comportements évoquant la négociation. Ainsi certains insectes sociaux (fourmis, termites, etc) vivant en colonies forment de véritables armées, disposant d'individus que nous nommons "soldats" chargés de défendre la colonie, puis attaquent leurs ennemis dans des combats violents. Les rapports entre fourmilières voisines ne sont pas toujours très harmonieux. Des luttes territoriales terribles opposent des combattants avec pitié. Un chercheur néerlandais, Mabelis, s'est passionné pour les guerres que se livrent des colonies de fourmis rousses. Au printemps, quand le nid sort de sa torpeur hivernale, les ouvrières fourrageuses vont explorer les environs. Quand elles rencontrent un nid voisin apparenté, de type super-colonie, des échanges de nourriture ou des transports de matériaux peuvent s'effectuer entre les nids. Mais s'il s'agit d'une colonie étrangère, des combats éclatent entre les ouvrières. L'intensité du combat va crescendo car chaque colonie recrute au fur et à mesure de nouvelles combattantes. Les combats durent toute la journée et se soldent par la mort de milliers de fourmis. C'est au cours de telles guerres que les territoires des colonies évoluent. D'après certains biologistes, ces guerres entre fourmis permettraient aux colonies de se procurer des protéines à une époque où les proies sont encore très rares. Les arts martiaux se sont en Asie beaucoup inspiré des attitudes et moyens de défense des animaux. Cependant, le sentiment durable de vengeance, qui a chez l'Homme entretenu des guerres durant des siècles ou décennies leur semble inconnu.


I.3. Définition classique
Selon le théoricien prussien Carl von Clausewitz (1780-1831) :

« la guerre est le prolongement de la politique par d'autres moyens. »

Cette théorie pose qu'un conflit armé est « la suite logique d'une tentative d'un groupe pour protéger ou augmenter sa prospérité économique, politique et sociale au dépens d'un autre ou d'autres groupe(s) » (Harris, p. 54). C'est la définition de la guerre impériale ou étatique, où l'attaquant se battrait pour élever son niveau de vie au détriment des autres (les intérêts économiques sous-jacents peuvent être enfouis et cachés derrière et par des alibis politiques, raciaux et religieux). Dans cette approche l'État n'existerait que par son organisation politique - impérialiste à l'usage interne ou externe - capable de réaliser des guerres de conquête territoriale, d'agencement économique et de colonisation. Par cette continuité politique, la guerre est aussi un élément incontournable des relations humaines, et donc une chose à laquelle il faut être prêt, ce que traduit le proverbe romain : Si vis pacem, para bellum (Si tu veux la paix, prépare la guerre), ou bien l'aphorisme de Nicolas Machiavel : « une guerre prévisible ne se peut éviter, mais seulement repousser ». Cette définition rejoint les antiques idées de la civilisation chinoise : la guerre n'est qu'un des moyens pour imposer sa volonté à un groupe ou à l'inverse y résister. Comme ce moyen est le plus risqué et le plus coûteux, la victoire la plus intéressante est celle qui ne se voit pas, l'adversaire n'ayant pas perdu la face, ce qui pourrait être une des définitions de la diplomatie. La guerre est souvent une façon de ressouder une communauté contre un ennemi commun, de justifier une forte discipline, voire d'acquérir ou conserver une gloire politiquement nécessaire à un pouvoir se voulant charismatique . Ces raisons rendent la guerre fréquente dans les dictatures et les États où les hommes voient leurs certitudes troublés par une brutale évolution politique (ethnique), économique ou technique. Il arrive cependant, bien que ce soit plus rare, que des démocraties se fassent la guerre entre elles aussi (voir théorie de la paix démocratique). 



II. Les formes de la guerre

II.1. Les guerres intraétatiques


II.2. Les guerres civiles
Les guerres internes à un pays en cause mettant aux prises une partie de la population contre l'autre sont qualifiées de guerres civiles. Chacun voit dans son ennemi, et même en celui qui voudrait rester neutre, un traître avec lequel il n'est plus possible de cohabiter et avec lequel aucun compromis territorial n'est possible (comme cela serait possible avec un ennemi étranger). C'est pourquoi l'unique issue envisagée est bien souvent l'anéantissement de l'autre et de ses alliés réels ou potentiels (y compris femmes et enfants), avec emploi de la terreur, ce qui rend ces guerres meurtrières et sans merci. Le statut juridique d'une guerre civile ne permet pas aux puissances étrangères et aux institutions internationales, comme l'ONU d'intervenir, comme la France pour les « intelligence » ou Guerre d'Algérie pour régler les différends. Inversement, une guerre étrangère est déguisée en guerre civile pour masquer l'agression étrangère, comme la France a déguisé sa reconquête coloniale en guerre civile avec la création d'Un État vietnamien indépendant en 1948, durant la Première Guerre d'Indochine d'indépendance et comme les États-Unis qui ont « aidé » la République du Viêt Nam en lutte contre la République Démocratique du Viêt Nam pendant la Deuxième Guerre d'Indochine ou Guerre du Vietnam de réunification. 
Il faut et il suffit simplement de fabriquer un gouvernement à sa solde qui demande l'aide pour intervenir en toute légalité dans les affaires intérieures d'un État souverain.


II.3. Autres formes de conflit 



III. La structure d'un conflit
La guerre n'est pas une simple manifestation de la violence humaine, elle est un fait social, qui demande une organisation des hommes, une convergence de leur force, vers un objectif unique. Comme toute organisation de la société, elle est hiérarchisée, et se décompose donc en une structure à deux niveaux principaux : le niveau politique qui initie la guerre lorsqu'on la juge souhaitable ou du moins nécessaire, et le niveau tactique, celui de la bataille et du rapport de force qui, par la fortune des armes, donnera raison politiquement à celui des deux belligérants qui aura su faire fléchir l'autre. Par sa violence, son inhumanité, son coût qui peut condamner à la misère des générations, la guerre imposa le niveau intermédiaire qu'est la stratégie, pour rendre plus efficace et plus soutenable le terrible effort de guerre, pour le rationaliser et le rendre plus « rentable ». Un conflit se décompose donc ainsi :

Le niveau politique. Lorsqu'elle ne peut tolérer une situation, et ne peut obtenir satisfaction, la politique décide de l'entrée en guerre, de manière à obtenir la décision (l'acceptation par l'adversaire des termes qu'on veut lui imposer). 
Le niveau stratégique. Parce que la guerre est un phénomène extrêmement grave et ne peut être laissé au hasard, elle se doit d'être pensée, ce que fait la stratégie en planifiant, organisant et rationalisant l'utilisation des forces. 
Le niveau tactique. Alors que le rôle de la stratégie est d'apporter à la politique le moyen le moins coûteux d'accéder à la décision (en indiquant la façon dont on use de la diplomatie, de l'économie, de la politique, de la force), celui de la tactique est réaliser ce que lui ordonne la stratégie pour obtenir cette décision. Pour simplifier, la tactique est l'art de gagner les batailles, alors que la stratégie est l'art de savoir gagner la guerre, en décidant quand, où et comment faire les batailles. 


III.1. Le niveau politique
Si l'on soustrait de la guerre l'aspect psychologique de la haine et de la violence (ce que l'on fait, comme vu plus haut, dans la théorie classique), la volonté politique précède nécessairement la volonté de violence elle-même, et plus encore la réalisation de cette violence. La guerre commence alors bien avant qu'on l'imagine pouvoir éclater, puisqu'elle nait en amont sur le papier, d'ambitions et de nécessités rationnelles.

« La guerre est la continuation de la politique par d'autres moyens » - Général Carl von Clausewitz

La violence est donc un outil dont dispose la politique pour arriver à ses fins. Généralement elle n'est qu'un dernier recours dans le cadre d'une stratégie plus globale (regroupant tous les moyens d'action), car l'objectif du stratège est d'imposer sa volonté, et non pas de gagner des batailles. C'est ce que défend Sun Zi :

« L'idéal est que votre adversaire se plie à votre volonté sans que vous ayez à utiliser la force » - L'Art de la guerre

et à Clausewitz de surenchérir :

« Le conquérant aime toujours la paix ; il entre volontiers tranquillement dans notre pays [même s'il n'a pas à faire la guerre] »

Dans les pays développés et démocratiques, où l'État se caractérise de plus en plus par sa fonction abstraite "d'administrateur de la société", on peut effectivement penser que cette appréciation de la guerre comme calcul froid de la nécessité politique correspond à la réalité. Cependant, lorsque la pouvoir se confond avec la personnalité de celui qui l'exerce, la rationalité de la volonté politique à tendance à laisser place à l'irrationalité de l'autocratie, car un despote sera beaucoup plus enclin à considérer la guerre comme une affaire personnelle. Mais l'on remarque que même dans ce dernier cas, on ne peut affirmer que la politique n'est pas à l'initiative du conflit, puisqu'en dictature la volonté personnelle devient la substance même de la politique.

« La guerre éclate lorsque les États n'ont plus une conscience claire de leurs devoirs, une intelligence nette de leurs droits, une notion exacte de leurs intérêts respectifs. Ils ne peuvent plus arriver à une entente commune, ils ne peuvent plus accepter les lois que leur traçait le droit des gens en temps de paix : ils s'y soustraient. La guerre est l'acte politique par lequel des États, ne pouvant concilier ce qu'ils croient être leurs devoirs, leurs droits et leurs intérêts, recourent à la lutte armée, et demandent à cette lutte de décider lequel d'entre eux étant le plus fort pourra en raison de sa force imposer sa volonté aux autres » - Funck-Brentano et Sorel, Précis du droit des gens, Paris, Plon, 1900, p. 74

Par sa nature destructrice, la guerre implique un coût très important, et il faut des motivations conséquentes et suffisantes pour initier un conflit. Comme tous les phénomènes sociaux extrêmes, ces motivations sont en bonne part de nature psychologique, mais la volonté de destruction s'est peu à peu rationalisée au cours du temps pour devenir un outil au service de la volonté politique. Celles-ci peuvent-être :

l'agrandissement de la nation ou le renforcement de sa puissance : par la conquête territoriale, la guerre coloniale 
l'affaiblissement voire l'anéantissement d'une nation ou d'un groupe humain jugé fondamentalement hostile ou nuisible : la guerre d'extermination, le génocide 
l'accès ou la défense d'intérêts vitaux : ressources précieuses ou indispensables telles que l'eau, le pétrole ; des villes ou régions productives ou peuplées ; des voies de communications comme les détroits, les canaux, des débouchés sur la mer 
la défense d'une morale, d'une justice ou d'un droit international : obtenir par la force le respect d'accords internationaux, ou du droit international (depuis la création de la SDN et de l'ONU), porter secours à un État "injustement" attaqué (en honorant un pacte d'alliance par exemple), porter secours à des populations victimes de leur État ou que leur État ne peut protéger (principe du droit d'ingérence, guerre humanitaire) 
l'affirmation ou la défense d'une idéologie, d'une religion ou d'un mode de vie (culturel) : ériger un système politique, social ou culturel présenté comme le meilleur.

À ces motivations matérielles et réfléchies qui peuvent encourager le recours à la force, s'ajoute d'autres motivations d'origines psychologiques et plus instinctives, d'autant plus courantes que le pouvoir est personnalisé :

renforcer le prestige de la nation
obtenir réparation d'un affront, porté à l'encontre du dirigeant, de l'État ou la nation
comme vu plus haut, renforcer la cohésion du groupe ou de la nation grâce à la création d'un ennemi commun. En éliminant les divisions, le consensus national peut légitimer un pouvoir charismatique (cas de la Junte argentine qui déclenche opportunément la Guerre des Malouines), ou faire oublier une impasse politique sans la régler (éventuellement inconsciemment et ce qui serait pour Jean Jaurès : « naturel au capitalisme »). 


III.1.1. Analyse logique de l'origine de la violence en politique
On peut décrire l'origine de la violence en politique de la manière logique grâce au schéma suivant : quand la politique souhaite obtenir quelque chose d'un groupe social sur lequel elle n'a pas autorité (ce peut être un autre État), il lui faut son acceptation, ce que l'on nomme la décision en stratégie. La seule et unique manière d'y parvenir est de convaincre (voir la section niveau stratégique) l'autre qu'il est de son intérêt de répondre favorablement aux demandent qui lui sont faites :

soit par des propositions attrayantes (influence positive)
soit par la menace (influence négative) qui, elle, vise à faire comprendre qu'un refus serait désavantageux

Dans les deux cas le but est de rendre rentable l'acceptation des conditions imposées, et si la réponse est négative, c'est que le refus semble préférable. Si l'argument de la menace lui-même ne fait pas son effet, deux solutions se présentent alors :

soit le statu quo parce que mettre les menaces à exécution imposerait un coût plus important que le gain espéré ("le jeu n'en vaudrait pas la chandelle")
soit le conflit car il semble rentable malgré les coûts, il y a alors confrontation

Il faut donc en conclure, et c'est essentiel, que lors d'un litige entre des entités politique indépendantes, toute décision et toute situation qui résultera de la confrontation de leurs volontés respectives, y comprit l'usage de la violence, aura été considérée par tous les protagonistes comme étant le choix le meilleur.



III.2. Le niveau stratégique
La stratégie est selon le Général André Beaufre :

« l'art de la dialectique des volontés employant la force pour résoudre leur conflit. » - Introduction à la Stratégie, Armand Colin, 1963, p. 34

Alors que le niveau politique formule une volonté, le rôle de la stratégie est de réfléchir au moyens d'amener l'adversaire à y répondre favorablement, ce que l'on nommera la décision. Pour le Général Beaufre, dans la dialectique des volontés, la décision est un événement d'ordre psychologique que l'on veut produire chez l'adversaire : le convaincre qu'engager ou poursuivre la lutte est inutile. Le but de la stratégie est donc:

« d'atteindre la décision en créant et en exploitant une situation entrainant une désintégration morale de l'adversaire suffisante pour lui faire accepter les conditions qu'on veut lui imposer. » - Introduction à la Stratégie, Armand Colin, 1963, p. 36

On admet généralement que le but de la stratégie est ni plus ni moins que de "gagner la guerre", d'où la formule prêtée à Clausewitz de "la décision par la bataille victorieuse". La réalité est plus subtile : n'oublions pas que la décision est psychologique, et qu'il faut "convaincre qu'engager ou poursuivre la lutte est inutile", d'où cette réflexion de Lénine analysant Clausewitz : "retarder les opérations jusqu'à ce que la désintégration morale de l'ennemi rende à la fois possible et facile de porter le coup décisif". Ainsi, il ne suffit pas d'être le plus fort pour gagner la guerre, mais de démoraliser le pouvoir adverse et c'est ce qu'apprirent à leurs dépens les États-Unis lors des guerres du Viêt Nam, de Somalie, d'Afghanistan ou encore d'Irak. D'ailleurs, la stratégie dans les guerre insurrectionnelles devient de plus en plus un cas d'école, et elle sera présentée ci-après.

III.2.1. Les moyens de la stratégie
L'art de la stratégie réside précisément dans la subtile confrontation entre d'une part, les capacités d'influence sur l'adversaire, positives ou négatives, et d'autre part, l'évaluation des coût inhérents aux moyens à disposition pour effectuer cette influence. L'influence peut être négative pour l'adversaire : destruction de ses forces et de ses biens, ou peut être positive : proposition de traité de commerce, négociation avantageuse ; la conjonction de ces moyens d'influence doit permettre une meilleure efficacité au rapport des coût qu'impliquent chaque combinaison possible, entre techniques d'influence négative et positive, c'est donc "jouer de la carotte et du bâton", en fonction des prix et de l'efficacité de la carotte et du bâton. Pour l'exemple, on peut imaginer qu'une phase destructrice qui apparaisse catastrophique à l'ennemi, soit suivie d'une proposition de paix dotée d'avantages inattendus, proférant alors un caractère providentiel à ce qui ne sont que des exigences. Nous voyons qu'ici, la stratégie tient à un choix subtil, émanant d'une réflexion qui vise à faire converger vers un objectif des moyens parfois contradictoires, ce choix constitue l'art de la stratégie. 
Là est donc l'intelligence de la stratégie, ses moyens sont de complexes combinaisons de techniques d'influence, mais pour les élaborer, il faut analyser l'effet moral décisif et savoir qui on veut convaincre. Dans le cas d'un gouvernement central, on peut choisir d'agir directement sur ses dirigeants et sur ce qui fait leur capacité d'influence (attaquer ou arrêter les personnes dirigeantes, leur administration, ou plus souvent leur propre capacité d'action : l'armée ennemie), ou bien sur un tiers qui a une influence sur eux (une organisation internationale comme l'ONU, des alliés influents, ou la population : solution particulièrement efficace dans une démocratie ou une société très divisée politiquement ou ethniquement). S'il faut convaincre non pas un gouvernement unique et centralisé, mais une constellation de personnalités ou un groupe (population, ethnie, groupe religieux, mouvance idéologique...), la stratégie comporte d'autant plus de variables et de complexité que le pouvoir adverse est décentralisé voir totalement explosé, car dans ce cas la décision doit être obtenue d'un ensemble d'individus, avec tout l'arc en ciel de sensibilité et d'intelligence stratégique qui le compose. 


III.2.2. Les modèles stratégiques
Il y a victoire de la stratégie lorsque l'adversaire décide de stopper ou de ne pas engager le combat, c'est-à-dire, dès lors qu'il y a démoralisation de son pouvoir décisionnaire. Or, suivant qu'un conflit est interétatique ou insurrectionnel, qu'il est de l'ère pré-nucléaire ou post-nucléaire, les moyens susceptibles d'arriver à cette fin sont très différents. 
Le paradigme de la guerre industrielle entre États monopolise grandement la réflexion stratégique encore aujourd'hui. Car il est tentant de penser que disposer d'une armée "puissante" selon les critères traditionnels (une armée de masse) autorise à se sentir prémuni de tous les types de guerre. L'armée américaine, de très loin la plus puissante au monde selon la définition classique, ne peut pourtant se permettre d'obtenir la décision que très rarement et difficilement dans les conflits insurrectionnels, autrement dit, elle ne peut pas gagner une guerre non conventionnelle avec des stratégies conventionnelles. Et c'est ce qu'a démontré le Général Petraeus à travers ses réflexions sur la guerre contre-insurrectionnelle. 
Les raisons de ce paradoxe encore mal compris sont les suivantes : la capacité des armées et plus généralement des politiciens à obtenir la décision a subi des mutations considérables au cours de l'histoire, en particulier en raison de l'évolution des possibilités opérationnelles qui résultaient de l'armement, de l'équipement (évolutions des technologies) et des méthodes de guerre et de ravitaillement, mais surtout à cause des stratégies préférées par l'adversaire en fonction de ses propres caractéristiques politiques et sociales. Or cette évolution inéluctable fut rarement comprise, au contraire, l'évolution à généralement surpris les deux adversaires, qui durent en tâtonnant rechercher les solutions nouvelles menant à la décision. De là vient l'idée que "les stratèges se préparent toujours pour la guerre précédente". L'exemple le plus surprenant pourrait être celui de la ligne Maginot, gigantesque structure à objectif défensif héritée des dogmes de la Première Guerre mondiale (guerre défensive de position), totalement incapable de protéger la France de la stratégie de guerre éclaire offensive des armées nazies (guerre offensive et décisive très dynamique, mécanisée et aéroportée). 
L'évolution la plus importante de ces dernières décennies parait être l'apparition de la force nucléaire, dont les caractéristiques foudroyantes ont totalement bouleversées les lois de la guerre, d'où la naissance d'une nouvelle stratégie ; mais l'atome n'est pas la seule révolution, et stratégie de guérilla, utilisée par le terrorisme, est elle aussi capable de mettre en échec des armées industrielles conventionnelles. 
Sont analysées ci-dessous les trois stratégies principales, s'adaptant chacune aux moyens dont on dispose et à l'ennemi à combattre. Aucune n'est meilleure que l'autre dans l'absolu, et aucune ne peut s'adapter à toutes les situations. 

III.2.2.a. Stratégie militaire classique
Dans la stratégie militaire classique, la guerre fut toujours comprise comme un rapport de force interétatique. Ainsi Hegel, contemporain des guerres napoléoniennes qui devaient redessiner la carte de l'Europe, comprend la dynamique des rapports de forces entre nations comme la matière même de l'histoire. L'histoire est alors le théâtre de la lutte des États pour l'hégémonie, où chacun se doit d'être le plus fort sous peine de disparaitre. 
Nation contre nation, front contre front, stratèges contre stratèges, la guerre interétatique a cette caractéristique jusqu'à la Seconde Guerre mondiale de voir se répondre en miroir des logiques de guerre pratiquement symétriques. La décision à l'état pur est celle qui résulte de la bataille victorieuse, et toute la stratégie classique a pour objectif de gagner la guerre en terrassant l'ennemi sur le champ de bataille. Dans cette logique d'équilibre des forces, une faiblesse, un calcul qui s'avère faux, ou une manoeuvre inventive et décisive, peut décider du sort de la guerre, tout l'art de la stratégie classique est un jeu d'équilibriste où chacun s'efforce de pallier ses points faibles et de gagner en supériorité. 
Le général Beaufre examine les solutions principales employées dans le jeu de la stratégie, il en dénombre trois :

Lorsqu'existent des moyens supérieurs et une capacité offensive suffisamment assurée, la campagne visera offensivement la bataille décisive. C'est la stratégie offensive d'approche directe où doit se réaliser la concentration du maximum de moyens visant la masse principale ennemie. Cette solution est largement préférée dans les guerres interétatiques, c'est celle qui fit la grandeur de Napoléon. 

Lorsque la supériorité est moins évidente, deux solutions apparaissent :

soit d'user l'adversaire par une défensive exploitée par une contre-offensive. C'est la stratégie directe défensive offensive
soit de dérouter l'adversaire par une action offensive excentrique avant de chercher à le battre. 

C'est la stratégie directe d'approche indirecte. Cette dernière possibilité rejoint la stratégie d'approche indirecte de Liddell Hart. 

Lorsque les moyens militaires sont insuffisants pour atteindre le résultat escompté, l'action militaire ne joue plus qu'un rôle auxiliaire dans le cadre d'une manoeuvre de stratégie totale sur le mode indirect où la décision résultera d'actions politiques, économiques ou diplomatiques convenablement combinées. Dans ce cas, on parle généralement pas de guerre, mais peut-être de confrontation diplomatique ; celles-ci sont si nombreuses depuis la guerre froide qu'elles sont presque devenues l'état normal des relations internationales. 

Le paradigme de la guerre interétatique conçoit la puissance comme synonyme de masse. Tout doit être massifié, densifié, les armes comme les hommes. Dès l'Antiquité, les armées se battaient en rangs serrés pour plus d'efficacité, à la fois tactique, mais aussi logistique (resserré autour de son chef, le groupe entendait ses ordres et agissait comme un seul homme avec une grande cohérence). Et ce fut donc naturellement que la guerre devint industrielle au xixe siècle, lors de la révolution industrielle. Les armes sont fabriquées en très grand nombre, et leur mécanique est grandement améliorée (mitrailleuse, canon rayé), parallèlement à leur efficacité sur le champ de bataille. De même, les moyens de transports tels que le train et les navires à vapeur permirent la massification des troupes en des temps records sur des théâtres d'opérations très éloignés. En 1904, la Russie transporta sur rail une armée de plusieurs centaines de millier d'hommes sur 6500 kilomètres par delà les espaces perdus et démesurés de Sibérie, afin de rencontrer les armées japonaises de Mandchourie. 

« La planète entière, au début du XXe siècle était devenue une seule entité maillée par les réseaux de transports et de transmissions, des chemins de fer, des navires à vapeur et des télégraphes. Et à l'intérieur de cette entité, les structures civiles et militaires de chaque nation sont devenues étroitement interconnectées. En temps de guerre, les chemins de fer seraient réquisitionnés et les hommes mobilisés. Les nations étaient mûres pour les guerres mondiales » - Général Sir Rupert Smith,L'utilité de la force, Economica, p. 69, ISBN : 978-2-7178-5366-7



III.2.2.b. Stratégie indirecte et stratégie insurrectionnelle
Nous avons vu ci-dessus que l'approche indirecte était parfois préconisée par des théoriciens classiques, puisque le Général Beaufre la citait parmi ses solutions préférées, car elle a l'avantage de dérouter l'adversaire avec peu de moyens : « l'idée centrale de cette conception est de renverser le rapport des forces opposées par une manoeuvre et non par le combat. Au lieu d'un affrontement direct, on fait appel à un jeu plus subtil destiné à compenser l'infériorité où l'on se trouve » . L'approche indirecte était alors un outil subtil mis à disposition de la stratégie classique, mais « a trouvé son application en stratégie totale sous une forme différente dans tous les conflits où l'un des adversaire [...] avait des moyens inférieurs à ceux qui pouvaient lui être opposés. » Autrement dit, la stratégie indirecte est l'arme du pauvre, et celle préférée par l'insurgé. 
La stratégie classique fut théorisée à l'ère napoléonienne, puisque que c'est l'empereur qui lui donna ses lettres de noblesse. Mais la stratégie de ce que le jargon militaire nomme les conflits de basse intensité, c'est-à-dire les conflits où ne s'opposent pas deux armées centralisées et symétriques, mais où au moins un belligérant (voire les deux) se constitue d'individus émanant directement de la société civile menant plus une guérilla qu'une véritable guerre, vit ses premières manifestations sérieuses en Espagne en luttant justement contre les troupes de l'Empire, entre 1808 et 1814. Le peuple appelait ça la « petite guerre », de guerra (nom pour guerre) et illa (suffixe diminutif). On voit donc la guérilla, seule solution des peuples face à la force classique, se développer et remporter des succès précisément au moment où cette dernière montre tout son prestige. 
Le général Sir Rupert Smith dit à ce sujet :

« [...] des groupes de combat, petits, mobiles et souples, issus de la population, cachés et soutenus par celle-ci, s'ingéniaient à harceler les armées ennemies supérieures en force, tout en évitant toute confrontation sur une grande échelle. Par la poursuite de cette guerre, l'objectif politique était de conserver l'identité politique de la population, même sous occupation, en soutenant sa volonté de continuer à combattre et à résister.[...]Privées de la force du nombre, et des armes pour s'opposer à une armée en campagne, les guérillas préfèrent éviter les batailles rangées. L'embuscade et le raid représentent leur modes de combats favoris. » - L'utilité de la force, p. 153

Cette stratégie est une réponse à la faiblesse tactique des guérilléros (peu d'hommes, peu d'entrainement, peu d'armes), et la règle d'or jamais démentie de tout combattant insurgé vise à palier cette faiblesse : toujours éviter de se trouver dans une position dans laquelle l'ennemi pourrait nous forcer à combattre. Il convient donc d'éviter d'occuper des infrastructures, des positions visibles, ou même simplement du terrain (choses que convoite généralement une armée classique) de façon à ne pas se trouver encerclé et de ne pas avoir à lutter sur un front fixe à homme contre homme.


III.2.2.c. Stratégie atomique et dissuasion nucléaire 
L'imaginaire collectif considère encore aujourd'hui l'arme atomique comme une menace pour la paix, mais si le risque de prolifération nucléaire vers des pays irresponsables et à craindre, il est absolument nécessaire de comprendre que dans toute l'histoire de la guerre (dont les conflits furent de plus en plus terribles et meurtriers au fur et à mesure de l'évolution des techniques de combats), aucune stratégie n'a autant oeuvré pour la diplomatie et le statu quo, et contre la violence, que la dissuasion nucléaire. La bien nommée Guerre Froide, qui avait tous les ingrédients (situation géopolitique, détestation mutuelle, rapport de force idéologique...) pour dégénérer en conflit mondial, fut un exemple de guerre impossible. 
Pour comprendre pourquoi l'apparition de l'arme nucléaire a nécessité l'invention d'une nouvelle approche stratégique, il faut rappeler en quoi elle détruit le paradigme de la guerre classique : 

Le première caractéristique est la puissance de feu en elle-même : une petite bombe thermonucléaire de 1 MT correspondrait à une salve de 200.000.000 de canons de 7510, soit plus que la totalité des capacités d'une armée. Il fallut pendant la Seconde Guerre mondiale près 3000 avions pour détruire la ville d'Hambourg (voir Opération Gomorrhe), alors qu'aujourd'hui une seule action individuelle aurait des conséquences bien plus destructrices (voir la Tsar Bomba, d'une puissance potentielle de 100 MT).
la deuxième caractéristique est la mobilité et la fluidité d'utilisation presque totale de ces armes grâce à leurs vecteurs : il fallait auparavant envoyer des milliers d'hommes sur le lieu de l'action, avec tous les délais et difficultés logistique que cela comportait, la guerre était uniquement frontale entre des murailles humaines qui se faisaient face ; à présent, une seule fusée atteint en quelques dizaines de minutes n'importe quel point du globe, une bombe larguée depuis un avion (comme à Hiroshima et Nagasaki), pouvait dès son invention en 1945 frapper un territoire ennemi en son centre. 

Du fait de cette double caractéristique, l'arme atomique produit deux phénomènes entièrement nouveaux :

il n'y a plus de rapport direct entre l'effort de guerre et la capacité destructrice (détruire un pays ne coûte en soi que le prix de la technologie, et non une mobilisation totale du pays attaquant)
aucun effort de guerre autre que technologique ne peut sauver une nation de la destruction en cas d'attaque. Posséder une armée conventionnelle n'est d'aucune protection contre un conflit de type nucléaire. 

On dénombre habituellement quatre types de protection possibles contre ce danger sans précédant :

la destruction préventive des armes adverses pour prévenir un risque d'offensive (moyen offensif direct), qui suppose des moyens de destruction très puissants et perfectionnés, donc nucléaires 
l'interception des armes atomiques au moment de l'offensive (moyen défensif)
la protection physique contre les effets des explosions (moyen défensif)
la menace de représailles (moyen offensif indirect), qui suppose de disposer soi-même de moyens nucléaires

Ces quatre directions furent exploitées concurremment avec des fortunes diverses et ont fini par se combiner dans des formules stratégiques très compliquées, mais on peut dire que celle qui semble, et de loin, la plus dissuasive est sans conteste la quatrième (menace de riposte nucléaire en cas d'offensive), à tel point qu'on la nomme précisément la dissuasion nucléaire. 
Le principe de la dissuasion nucléaire est fort simple : toute agression trop directe envers une puissance nucléaire expose l'agresseur à une riposte cataclysmique et absolument insoutenable, un pays non-nucléaire ne peut donc en aucun cas s'attaquer frontalement à une telle puissance. On nomme également équilibre de la terreur ou MAD en anglais (Mutually Assured Destruction ou « Destruction mutuelle assurée ») une situation plus précise : lorsque deux puissances sont nucléaires, toute agression atomique de l'une expose l'agresseur à une riposte destructrice d'une ampleur au moins équivalente, ainsi le déclenchement des hostilités est a priori impossible. Car le fait d'être agresseur ne profère aucun avantage particulier comme ce pouvait être le cas dans la stratégie classique, être attaquant ou attaqué signifie de toute façon la destruction, d'où l'exigence constante de garantir la paix. 
Et la solution la plus efficace trouvée pendant la Guerre Froide pour garantir la sécurité du monde, fut justement, par le Traité ABM (ABM pour anti-missiles balistiques) signé en 1972, de limiter non pas les capacité offensives de chacun des deux camps, mais de limiter les capacités défensives. En effet, le meilleur gage de paix était de préserver à tout prix la destruction mutuelle assurée, et ainsi d'interdire l'utilisation généralisée de ces nouvelles technologies des années soixante-dix des missiles intercepteurs. Ceci montre à quel point la très contre-intuitive dissuasion nucléaire est l'une des meilleures assurances imaginable pour sécurité du monde, y compris lorsque celui-ci était divisé par la confrontation est-ouest. 




III.3. Le niveau tactique



IV. Les représentations de la guerre

IV.1. Représentation de la guerre dans l'art

IV.1.1. Dans la peinture et les arts graphiques


IV.1.2. Dans la littérature



IV.2. Réflexion sur la guerre en philosophie
Pour paraphraser Paul Valery- la guerre est un conflit social opposant des hommes qui ne se connaissent pas aux dépens d'homme qui ne se connaissent que trop bien sans jamais s'être vraiment battu. 
Les responsables des nations ont considéré depuis longtemps que l'éventualité des guerres étant fréquente, il convenait de s'y préparer. La préparation de ces guerres se fait le plus souvent par l'entraînement d'une ou plusieurs armées. Après la deuxième guerre mondiale, les états ont créé l'ONU qui développe par la coopération et la diplomatie des stratégies de préparation et de maintien de la paix (avec, l'aide de contingents de casques bleus lorsque le stade du conflit armé est atteint). 
Depuis l'histoire de la libération de l'Inde, qui s'est terminée au milieu du XX siècle, le Mahatma Gandhi a fait école auprès de certains courants minoritaires qui réfléchissent à des moyens «non-violents» pour régler les conflits entre nations. Ils cherchent à réformer les réflexes ancestraux des nations et des peuples vis-à-vis des guerres. 
John Foster Dulles, alors Ministre des Affaires étrangères du Président Eisenhower, a déclaré qu'il y eût deux moyens pour soumettre un pays, par la force des armes et par le contrôle de son économie. 
Jacob Bronowski - mathématicien, philosophe et poète réfugiés en Angleterre et aux États-Unis durant les années 1930-40 a estimé que la guerre était le résultat de la conjonction d'une technologie appropriée et de la logique du pillage. L'agriculture avec la domestication des animaux et des plantes a fait sortir l'humanité de l'errance perpétuelle. La domestication du cheval s'insèrerait alors dans cette logique du pillage, le cheval permettant à des nomades de faire des razzias chez des cultivateurs fixés à leurs terres, et aux temps des semailles et des récoltes et voler le fruit de leur travail. La frayeur suscité par ces cavaliers serait la source de la légende du « centaure ». Une tradition de pillage et d'ailleurs de guerre par des cavaliers a persisté en de nombreux lieux et époques, en Afrique, Amérique et Asie. 
On peut même considérer que l'état de guerre est naturel, et que c'est la paix qui résulte d'une construction, motivée par les plus grands gains d'un mauvais compromis que de la plus grande victoire. La régulation et le traitement de la guerre sont l'un des sujets majeurs pour les acteurs politiques et religieux et depuis quelques années par l'Organisation des Nations unies et d'autres institutions internationales et des organisations non gouvernementales. 
En 1933, à l'initiative de la Société des nations, Albert Einstein a questionné Sigmund Freud :"Pourquoi la guerre ?"11 et, au terme d'une longue réponse, Freud conclut son courrier "Tout ce qui travaille au développement de la culture travaille aussi contre la guerre". 
Des théoriciens ont émis l'hypothèse que la guerre était aussi une nécessité naturelle pour réguler la population humaine. C'est une idée qui semble assez communément partagée, qui voudrait que malgré son « intelligence » l'humanité ne saurait se réguler autrement, mais cette théorie est infirmée par au moins deux faits ;

les guerres même les plus sanglantes ont eu un impact provisoire et limité sur la démographie humaine (la dernière guerre mondiale ayant même suscité un baby boom).
Quand le niveau de vie augmente, la population se stabilise, et en Europe depuis la période de paix (interne au moins), la démographie s'est rapidement stabilisée.


IV.2.1. Opinion d'André Maurois
Dans Les silences du colonel Bramble André Maurois voit la guerre comme aussi inévitable que les mouvements d'un dormeur dans son sommeil : suite à un immobilisme prolongé, des parties du corps éprouvent des soucis qui se cumulent avec le temps d'approvisionnement en ressources, et la souffrance engendrée déclenche une tentative de retournement brutale. Après s'ensuit un nouveau calme, temporaire, à l'issue duquel le cycle recommence.


IV.2.2. Opinion de Rosa Luxemburg

« Les guerres sont un phénomène barbare, profondément immoral, réactionnaire et contraire aux intérêts du peuple » - Déclaration devant le tribunal de Francfort, février 1914.




IV.3. La guerre dans les religions et la mythologie



V. Les politiques de la guerre et de la paix dans l'histoire et aujourd'hui

V.1. Les organisations militaires et les politiques de coopération                        internationales


V.2. Promotion de la paix au niveau et son Conseil de sécurité des Nations                        unies 
Ils ont cherché des moyens nouveaux moyens de résolution non-violente des conflits, s'appuyant notamment sur la polémologie et la signature et ratification de traités de paix et de conventions internationales. Mais ces outils restent fragiles. 
Les principes du développement durable visent par un partage des ressources, en amont et plus solidairement, à atténuer les tensions entre groupes. Ils dépendent cependant du bon vouloir et des possibilités qu'ont les pouvoirs et les habitants à les appliquer. 

V.2.1. La convention de La Haye
La convention de La Haye interdit l'incorporation dans les armées de la population d'un territoire occupé. Le pouvoir nazi a donc fait usage d'une ruse : à partir du 25 août 1942, il a conféré la citoyenneté allemande à un nombre croissant de Français d'Alsace et de Moselle à commencer par les hommes (les Malgré-nous).


V.2.2. Les conventions de Genève du 12 août 1949 dont la troisième relative                            au traitement des prisonniers de guerre



V.3. Droit de la guerre (jus in bello) et dommages de guerre
Le philosophe Hugo Grotius s'est penché sur le sujet de la guerre juste et a publié le livre De jure belli ac pacis (Le droit de la guerre et de la paix) qui explique que pour qu'une guerre soit juste, elle doit être en accord avec les conditions suivantes:


Jus ad bellum (Droit à la guerre):

déclarée
menée pour une juste cause
dans une intention juste
en utilisant des moyens qui soient proportionnés aux fins (à l'agression)
avec un espoir raisonnable de succès
constitue le dernier recours



Jus in bello (Droit dans la guerre)

proportionnalité
discrimination


Jus post bellum(Droit à la sortie de la guerre) 




<TXT=w-Hippie4>

I. Étymologie
Le terme hippie trouverait son origine dans un vocable africain, hip, que certains pensent être un terme wolof (hipi signifiant « ouvrir ses yeux »), repris dans le mot hipster désignant les amateurs de bebop des années 1940. 
Une autre origine du terme parfois donnée est une dérivation de l'acronyme H.I.P., désignant un quartier de San Francisco, le Haight-Ashbury Independant Property, occupé par les hippies. 
L'acronyme serait également un jeu de mot avec hype signifiant « décontracté, branché, dans le coup ». Comme le hipster, le hippie devait en effet être « cool ». 
Cependant, les hippies n'utilisaient pas ce terme pour se désigner eux-mêmes, ils se disaient plutôt freaks ou heads voire acid heads (les monstres ou les têtes). 
La première occurrence du mot dans les médias semble être trouvée dans un numéro du Time de novembre 1964 évoquant l'usage de drogue d'un jeune homme de 20 ans qui avait fait scandale. 


II. Histoire

II.1. Les précurseurs
Même si le phénomène hippie naît véritablement aux États-Unis au début des années 1960, on en trouve les prémices au moins dès le xixe siècle. 
Le mouvement hippie est considéré par l'historien de l'anarchisme Ronald Creagh comme la dernière résurgence spectaculaire du socialisme utopique, qui se caractérise par une volonté de transformation de la société non pas par une révolution politique, ni sur une action réformiste impulsée par l'État, mais sur la création d'une contre-société socialiste au sein même du système, en mettant en place des communautés idéales plus ou moins libertaires. Cette filiation est revendiquée par certains d'entre eux, comme par exemple les Diggers de San-Francisco dont le nom est une référence à un collectif de squatteurs du xviie siècle. 
Selon Patrick Rambaud, l'un des piliers d'Actuel, acteur et observateur du mouvement soixante-huitard français « Les communautés ne sont pas nées dans les années 1960 aux États-Unis en France et en 70 en France. Ça existait au xixe siècle avec Fourier, Cabet qui part en Floride fonder l'Icarie, et même les pirates du xvie siècle ! » 7. Les liens sont parfois même structurels entre communautés socialistes utopiques et hippies à l'exemple de Joan Baez, qui aurait été élevée dans la Ferrer Colony de Stelton (New Jersey). 
En Allemagne également dès 1896, la Lebensreform inspirée du paganisme ancien, avec les wandervogel et les naturmensch, précédait les hippies de plusieurs décennies. Adolf Just ouvrit son premier centre en 1896 dans les montagnes du Harz et publia son livre best-seller «Retour à la nature », qui devint le modèle des « enfants de la nature » la même année. Les images de l'époque, si elles n'étaient pas en noir et blanc, pourraient donner l'impression d'avoir été prises dans une communauté hippies des années 1960 aux États-Unis. Un immigrant allemand, Bill Pester, s'installa en 1906 à Palm Canyon en Californie dans une hutte pour vivre un mode de vie en tout point identique à celui qui allait surgir au sein de la société américaine soixante ans plus tard. Un autre allemand, Maximillian Sikinger, s'installa à Santa Monica Mountains à partir de 1935 pour inspirer les américains à devenir des « Nature Boys » (naturmensch) et fut très actif au sein du mouvement hippie des années 1960. 
Les précurseurs directs dans les années 1950 sont les beatniks, dont les figures emblématiques telles que William Burroughs, Allen Ginsberg et Jack Kerouac furent des références pour le mouvement hippie. Ils veulent vivre une évasion hors de l'Amérique post-nucléaire et consommatrice bien-pensante, notamment sous l'influence du jazz et du mouvement surréaliste dont les membres ont trouvé refuge à New York pendant la guerre. Ils mènent une vie libérée, faite de sexe, de musique et de déplacements constants : Sur la route était un livre emblématique de cette quête, il le restera pour les hippies, bien que Kerouac se désintéressa du mouvement. Allen Ginsberg par contre en resta proche, et inspira entre autres Bob Dylan. 


II.2. Les débuts aux États-Unis
Aux États-Unis, les débuts du mouvement se situent autour des années 60 dans un contexte de contestation et de refus de l'ordre établi (les surf bohemians, sans constituer un mouvement véritable, ont déjà toutes les caractéristiques des hippies dès le début des années 1950) ; les manifestations contre la guerre du Viêt Nam et les émeutes des Noirs dans les grandes villes américaines fédèrent une partie de la jeunesse. Mais cette génération, née juste après la Seconde Guerre mondiale, refuse aussi le conformisme, la soumission au pouvoir et aux canons de l'art. Elle cherche à fuir la société de consommation en mettant en avant des valeurs écologistes et égalitaires inspirées des philosophies orientales et primitives. 
Beaucoup des aspirations hippies sont héritées des écrivains de la Beat generation, également considérés comme précurseurs du mouvement car eux aussi exprimaient une rupture avec la société de masse. A l'idéal d'une vie centré sur la liberté, le sexe et la musique, les hippies ajoutent le psychédélisme et sa recherche de nouvelles perceptions par l'usage de drogues. 
Timothy Leary prône la révolution psychédélique par le LSD - à cette époque encore légal - et en 1964 Ken Kesey fonde les Merry Pranksters avec qui il sillonne les États-Unis dans un bus décoré par leurs soins afin d'organiser des acid tests. 
La médiatisation des Merry Pranksters entraîne la naissance de communautés psychédéliques comme Haight-Ashbury à San Francisco ou l'East Village à New York. 
À partir de 1965, de nombreux hippies commencent à s'installer à San Francisco, dans le quartier de Haight-Ashbury. Les diggers assurent l'intendance, en pratiquant entre autres la récupération des surplus de la ville, et distribuant gratuitement nourriture, soins et LSD. L'essor des communautés hippies inquiète les autorités. La Californie interdit l'usage du LSD le 6 octobre 1966, rapidement suivie par le reste du pays. L'image populaire du LSD change et devient celle d'un produit dangereux. 


II.3. Summer of Love
En 1967, de grands rassemblements ont lieu à Haight-Ashbury. D'abord, en janvier, le happening géant du Human Be-In rassembla des centaines de personnes venues lire de la poésie, être ensemble et écouter de la musique. Au coucher du soleil, la foule se dirige vers la plage pour y passer la soirée. Ce soir là, la police profite de l'absence des habitants de Haight-Ashbury, pour arrêter cinquante personnes, ce qui occasionna une période de traque aux dealers de drogues douces. 
Des étudiants des colleges et high schools commencèrent à arriver à Haight-Ashbury durant leurs vacances de printemps 1967. Les dirigeants de la municipalité étaient déterminés à arrêter l'afflux de jeunes gens que leurs écoles avaient laissé libres pour l'été, et, malgré eux, attirèrent davantage l'attention sur l'événement. Une série d'articles d'actualité dans les journaux locaux alerta les médias nationaux sur le mouvement hippie grandissant. Certains membres de la communauté de Haight y répondirent en formant le Council of the Summer of Love, donnant à un mouvement créé par le bouche-à-oreille un nom officiel. Durant l'été, pas moins de 100 000 jeunes originaires du monde entier ont convergé dans le quartier d'Haight-Ashbury, à San Francisco, à Berkeley, et dans d'autres villes de la région de San Francisco, pour se joindre à une version populaire de l'expérience hippie. L'évènement de l'été est le festival international de musique pop de Monterey qui rassemble 200 000 personnes et où Jimmy Hendrix et The Who jouent pour la première fois. 
Haight-Ashbury est alors victime de son succès : tandis que des hippies, de plus en plus jeunes, continuent d'affluer, les drogues dures y font leur apparition et les descentes de police se multiplient. Les hippies estimaient alors leur nombre à 300 000 dans tout le pays. 


II.4. Les révolutions de 1968
En 1968, alors qu'aux États-Unis sous l'influence d'activistes comme Jerry Rubin et Abbie Hoffman une partie du mouvement hippie se radicalise et parle de révolution, c'est en Europe qu'elle se concrétise en actions. 
À cette époque, le mouvement hippie est encore peu présent en Europe continentale, où il arrive par l'influence de sa musique. En France, les relais du courant hippie au début des années 1960 étaient le magazine Rock & Folk ainsi que le Pop Club sur France Inter avec Patrice Blanc-Francard. Le magazine Actuel, la référence du mouvement en France, ne sera créé qu'en 1970. 
Cependant en France, aux Pays-Bas, comme dans de nombreux pays, les années 60 virent également fleurir la contestation de l'ordre établi. Dès 1965, aux Pays-Bas, les provos d'Amsterdam prônaient la circulation en vélo. Ce mouvement de gauche invitait chacun à peindre son vélo en blanc et à le laisser à la libre disposition des habitants, ou organise des manifestations lors du mariage de la reine Beatrix avec Claus von Amsberg, ancien membre des Jeunesses hitlériennes. Souvent jugés plus provocateurs, plus politisés et militants que les hippies, ils sont parfois crédités des changements survenus à cette époque en Europe. 
Pour Dany Cohn-Bendit, « sans les provos et l'exemple qu'ils ont donné aux jeunes des autres pays, l'Europe d'aujourd'hui ne serait pas ce qu'elle est devenue. »22. 
Les paroles d'un jeune hippie français de ces années-là ne sont pas différentes de celles d'outre-atlantique : 

« Ainsi vont les choses dans nos sociétés dites de consommation : passée l'adolescence, âge irrécupérable mais dont on sait qu'il n'a qu'un temps, une certaine image de vous-même vous attend, tirée d'ailleurs à plusieurs millions d'exemplaires ; elle vous guette d'autant plus tôt que votre famille ne dispose pas des ressources financières qui, quelques années encore, vous garantiraient le droit à l'irresponsabilité. Gare à vous si vous ne marchez pas ensuite. On vous culpabilisera d'abord ; quelques bonnes lois feront le reste. » - Pourquoi n'êtes-vous pas hippie?, de Bernard Plossu, p. 8

Le mouvement est plus politisé qu'aux États-Unis, mais, même si des hippies furent bien présents dans l'agitation de la Sorbonne, les évènements de mai 1968 en France sont issus de frustrations plus vastes.


II.5. L'apogée du mouvement
En août 1969 eut lieu le festival de Woodstock, un festival de musique et un rassemblement emblématique de la culture hippie. Il eut lieu à Bethel sur les terres du fermier Max Yasgur aux États-Unis, à une soixantaine de kilomètres de Woodstock dans l'État de New York. 
Organisé pour se dérouler du 15 au 17 août 1969, et accueillir 50 000 spectateurs, il en accueillit finalement plus de 450 000, et se poursuivit un jour de plus, soit jusqu'au 18 août 1969 au matin. 
Le festival accueillit les concerts de 32 groupes et solistes de musiques folk, rock, soul et blues. C'est un des plus grands moments de l'histoire de la musique populaire et a été classé parmi les « 50 Moments qui ont changé l'histoire du Rock and Roll 23. » 
Contrairement aux États-Unis et l'Angleterre, les grands festivals rocks n'ont pas eu en France le même caractère rassembleur. En 1967, le premier spectacle psychédélique à Paris, « la fenêtre rose », n'attire encore que peu de monde. Le premier festival, refusé par plusieurs municipalités françaises, aura finalement lieu à Amougies, en Belgique, fin 1969. En 1971, un festival gratuit est organisé à Auvers-sur-Oise, mais s'il ressemble bien à celui de Woodstock à cause de la pluie et de la boue, il est finalement annulé dans la nuit à cause de divers problèmes techniques alors que 20 000 personnes sont rassemblées. Seul le festival de l'île de Wight eut une ampleur comparable. 
À partir de 1968, les jeunes européens prennent également la route, d'abord vers Ibiza, et vers Amsterdam qui devient la capitale européenne des hippies. C'est là que Yoko Ono et John Lennon organisent en 1969 le premier « Bed-in for Peace ». 
Le Larzac, autre lieu de prédilection du mouvement, rassembla 60 000 personnes en août 1973 dont une grande proportion de hippies pour une manifestation intitulée « ouvriers et paysans, même combat ». 


III. Idéologie du mouvement hippie 
Jack Weinberg, leader du « Free Speech Movement » dans les années 1960, était l'auteur de la célèbre phrase « Ne faites pas confiance à quelqu'un de plus de trente ans » qui traduisait sans équivoque la volonté de se distinguer de la génération précédente.
Les hippies n'avaient pas le désir de contrôler la société, contrairement aux rébellions des générations précédentes, comme les wobblies ou les activistes de la nouvelle gauche. Bien que très critiques, ils ne proposaient pas d'alternative à la société, le mot d'ordre était plutôt « faites ce que vous voulez faire et ne vous préoccupez pas de ce que les autres en pensent » (« do your own thing and never mind what everyone else thinks »). 
Selon Chuck Hollander, expert en drogues pour la National Student Association au début des années 1960 

« S'il existait un code hippie, on pourrait le présenter ainsi : faites ce que vous avez envie de faire, où vous le voulez et quand vous le voulez. Lâchez la société que vous avez connue. Explosez l'esprit de toutes les personnes rigides que vous rencontrez, branchez-les, sinon par la drogue, au moins par la beauté, l'amour, l'honnêteté et la rigolade »


III.1. Rapports à l'idéologie dominante
De manière générale, les hippies contestaient le matérialisme et le consumérisme des sociétés industrielles, et tout ce qui y était lié. Ils rejetaient en particulier les valeurs associées au travail et à la réussite professionnelle, ainsi que le primat des biens technologiques au détriment des biens naturels. Ils aspiraient à une sorte de fraternité universelle pour laquelle ils espéraient trouver idées et techniques dans des sociétés traditionnelles. Ce complexe idéologique, essentiellement constitué en une praxis, n'a pas réellement été théorisé ; jamais non plus il n'a fait l'objet d'une homogénéité pratique parmi celles et ceux qui se reconnaissaient pourtant comme hippies.


III.2. Le refus de l'autorité
Les hippies remettaient en cause l'idée d'autorité, d'abord l'autorité parentale, et tout ce qui en découlait : toute domination de l'un sur l'autre. Cherchant à établir d'autres rapports avec leurs propres enfants, les hippies adoptèrent les pédagogies anti-autoritaires; dans les communautés naquirent des « écoles sauvages » ou « écoles parallèles », et le livre Libres enfants de Summerhill traduit en français en 1971 fut un succès pendant toute la décennie. 
Ils refusaient aussi les frontières et la violence en général; le mot « pigs » (porcs) était régulièrement utilisé à l'encontre des forces de l'ordre. 
Les Yippies sont des représentants notoires de cette prise de position. Un de leurs fondateurs, Jerry Rubin, initiateur des manifestations contre la guerre du Vietnam, fut arrêté et condamné pour conspiration et incitation à l'émeute, il écrivit en particulier Do it! scénarios de la révolution. Perçus comme des « hippies avec des fusils », ils étaient aux États-Unis la frange la plus radicale du mouvement. 


III.3. Le pacifisme : « peace and love »
Peace and love, « paix et amour », est l'expression du pacifisme hippie des années 1960. Un autre slogan, issu de la guerre du Viêt Nam, Make Love, not War, « faites l'amour, pas la guerre » a été repris par le courant hippie pour les mêmes raisons ; l'expression apparaît en 1974 dans la chanson Mind Games de John Lennon. 
Flower Power, « le pouvoir des fleurs », est une autre expression pacifique qui trouve son origine dans le Summer of Love de 1967 à San Francisco. Consigne était alors donnée de « porter des fleurs dans les cheveux », comme l'illustre la chanson de Scott McKenzie San Francisco (Be Sure to Wear Flowers in Your Hair). Les hippies furent dès lors communément appelés flower children, « enfants-fleurs ». L'ensemble de ces expressions cherchaient à traduire une opposition à la guerre et à la violence en général, sans pour autant que les revendications soient toujours plus élaborées ou véritablement théorisées. 


III.4. Le retour à la nature 
Après les premières manifestions pacifiques contre la pollution en 1968 à San Francisco, et leur répression, de nombreux hippies rejoignirent des communautés rurales, et ce retour à la terre amène l'idée d'un plus grand respect de la planète incluant produits bios, utilisation d'énergies renouvelables et recyclage. 
Selon Timothy Leary également, les hippies sont à l'origine du mouvement écologique dans le monde. L'hypothèse Gaïa a été en effet formulée par James Lovelock à cette période où les premières craintes pour l'environnement commençaient à s'exprimer. 


III.5. La liberté sexuelle
La liberté sexuelle fait partie intégrante de l'idéologie hippie. Elle prône la légalisation de la pilule contraceptive et le droit universel à l'avortement, ce qui va à l'encontre, aux Etats-Unis, de l'idéologie conservatrice américaine des autorités religieuses en majorité chrétiennes. Les hippies vivent alors en communauté et ont des pratiques sexuelles diverses s'inspirant parfois du Kama sutra. Le mot d'ordre était « Free Love » (voir amour libre). Un rassemblement de 100 000 personnes à San Francisco en 1967 s'est appelé le « Summer of Love ». Il est généralement considéré que c'est au retour de ce rassemblement que les valeurs et le mode de vie du mouvement hippie ont commencé à vraiment se diffuser.



IV. Les communautés
Selon Jean-Pierre Bouyxou et Pierre Delannoy, « les communautés sont l'expression par excellence du movement : son infrastructure, l'ancrage social sans lequel il aurait vite été réduit à une simple mode aussi extravagante qu'éphémère. Les communautés sont sa signature au bas de l'histoire du XXe siècle. ». Elles se comptait par milliers aux États-Unis vers 1969, au point que dans les Rocheuses les hippies furent près d'élire un des leurs comme shérif. En France, on en dénombrait environ 500 au début des années 1970. 
Il n'y eut pas d'unité d'organisation entre ces communautés; les unes étaient des communautés urbaines, d'autres tentèrent de vivre d'agriculture et d'élevage et certaines n'étaient que des lieux de passage. Confrontées aux problèmes de subsistance, et aux difficultés de la vie en commun en réinventant de nouvelles relations, la plupart eurent une durée d'existence assez brève. 
La plus vaste expérience européenne fut celle de la commune libre de Christiania (Danemark), Copenhague; créée en septembre 1971, elle existe encore en 2009. Au début du xxie siècle, il existait encore une quarantaine de communautés hippies en Allemagne. En France, il n'en resterait qu'une à Charleval en Normandie. 


V. Les portes de la perception et l'influence orientale
Les hippies recherchèrent un sens à la vie dans des spiritualités qu'ils jugeaient plus authentiques que les pratiques religieuses dont ils avaient hérité, s'aidant parfois de substances psychotropes. Le livre Les Portes de la perception (The Doors of Perception) d'Aldous Huxley (1954) fut une inspiration pour beaucoup (il a, entre autres, inspiré le nom du groupe The Doors).

« Aujourd'hui, après deux guerres mondiales et trois révolutions majeures, nous savons qu'il n'y a pas de corrélation nécessaire entre la technologie plus avancée et la morale plus avancée. » - Aldous Huxley, Les Portes de la perception


V.1. Balbutiements du New Age
Selon certains témoins de l'époque, c'est au moment du Summer of Love de 1967 que furent fondés les prémices du New Age. Les hippies avaient commencé à explorer les traditions orientales, le bouddhisme, l'hindouisme et le taoisme et certains ouvrages populaires tentaient d'en faire une analyse syncrétique « libre », une manière d'aborder la spiritualité qui allait devenir la marque du New Age. 
Les hippies trouvaient leur inspiration spirituelle chez des personnalités comme Gautama Bouddha qui, incarnant la négation du monde matérialiste en tant que seule voie possible d'atteindre le bonheur permanent, avait tourné le dos au roi, son père, et voyageait comme un mendiant, François d'Assise, qui abandonna également une famille riche pour vivre dans la pauvreté et dans la nature, et bien sûr le Christ (« a groovy cat » selon l'expression consacrée), ainsi que Gandhi, Aldous Huxley et Tolkien. 
Élève d'Alan Watts, introducteur de la pensée orientale à San Francisco, Gary Snyder, rejoint par Jack Kerouac puis plus tard par Allen Ginsberg, également vont populariser la pratique de la méditation, et plus généralement du Tao et du Bouddhisme Zen. 


V.2. Les psychotropes
Le LSD (ou « acide ») fut découvert en 1943 par Albert Hofmann dans le laboratoire suisse Sandoz mais sera déclaré illégal aux États-Unis le 6 octobre 1966, ainsi que par l'ONU comme stupéfiant dans une convention de 1971. Jusqu'à cette interdiction sur le sol américain, la firme Sandoz met le LSD à disposition des chercheurs sous la forme d'une préparation appelée delysid. Le LSD apparaît d'abord comme prometteur dans le traitement de certaines maladies psychiatriques. Puis, il est popularisé comme étant un traitement dit miraculeux par les médias à partir du milieu des années 1950. Dans les années 1960, il devient un ingrédient du courant hippie. 
L'esthétique psychédélique peut être assimilée aux visions provoquées par le LSD qui provoque, en somme, une déformation de la vision et entraîne dans un état rêveur où réalité et rêve sont confondus. Le psychologue Timothy Leary, le chimiste Augustus Owsley Stanley III et le romancier Ken Kesey ont parmi d'autres encouragé la consommation de LSD. À cette époque, « l'acide » a notamment été distribué gratuitement lors des acid tests des Merry Pranksters. L'écrivain William S. Burroughs est considéré comme l'un des théoriciens de la pratique junkie liée à la mentalité hippie. Dans Junky, il explique en quoi la drogue est une philosophie qui mène à ouvrir les portes de la perception et à découvrir l'« équation de la came ». Le point culminant de l'usage du LSD aux États-Unis fut atteint à l'été 1967, au cours du Summer of Love (« Été de l'amour »), qui vit des milliers de hippies se regrouper à Haight-Ashbury, un quartier de San Francisco. Dans le Golden Gate Park, à proximité de Haight Ashbury, des grandes réunions ou « love-in » (ou « be-in » également) et des concerts gratuits étaient organisés par des groupes comme Grateful Dead, Jefferson Airplane ou Country Joe and the Fish. 
Il est possible de rattacher de nombreux courants artistiques à la consommation de psychotropes, aussi bien en musique (rock psychédélique, acid rock) que dans le dessin et la mode. 
Outre le LSD, le cannabis était aussi massivement consommé par les hippies, en particulier sous sa forme la plus répandue, la Marijuana (qu'ils appelaient maryjane ou thé). 
Pour les hippies, le but de cette consommation de psychotropes est présenté comme une volonté d'ouverture d'esprit et d'abolition des frontières mentales. Une étude des années 1960 de l'Université de Californie du Sud avait dégagé trois tendances dans la communauté hippie de l'époque : Les « groovers » (les fêtards), qui prenaient du LSD pour faire la fête et trouver des partenaires, les « mind trippers » (les touristes de l'esprit), qui portent des vêtements à fleurs et cherchent une thérapie, et les « cosmic conscious » (les mystiques), « planant », dont la consommation de drogue « est par nature eucharistique ». 


V.3. La route
« La route des hippies » (Hippie trail) est une expression utilisée pour évoquer les voyages entrepris par cette génération des années 1960, principalement vers l'Europe et l'Asie. Le voyage se faisait fréquemment par bus ou en auto-stop, les étapes obligées étaient Amsterdam, Londres et les destinations Goa (Inde), Katmandou (Népal) mais aussi la Turquie et l'Iran. Un des objectifs déclarés de ces voyages était la « quête de soi » ou « la recherche de Dieu » mais également la recherche de toutes nouvelles expériences. Des ouvrages comme sur la route de Jack Kerouac, ouvrage fondateur de la Beat Generation ont contribué au mythe de « la route »45.



VI. L'esthétique hippie

VI.1. Le corps et ses usages
En partie par rébellion contre les usages, le hippie portait les cheveux longs, pour les hommes comme pour les femmes. Ces dernières les portant généralement défaits, sans aucun apprêt. La liberté du corps (Body freedom) est complémentaire à la liberté de l'esprit qu'il préconise. Les relations sexuelles libérées, le naturisme sont des valeurs qui sont mises en avant dans son mode de vie.


VI.2. Le vêtement
Les vêtements, aux couleurs vives, se voulaient choquants pour une époque où les tenues étaient assez uniformisées et sombres. Leurs pantalons étaient à « pattes d'éléphants », style lancé par les hippies californiens et l'influence de l'Orient leur avait donné le goût des sandales, des tuniques indiennes avec des motifs très fleuris et colorés, des gilets afghans et du patchouli. Ils portaient de petites lunettes rondes, des bandeaux dans les cheveux, des colliers et des bracelets de perles. Ils pouvaient tout aussi bien être nus quand la situation le permettait. Le blue-jeans est également un vêtement emblématique de la génération hippie, il est souvent porté peint, brodé, cousu, couvert de coquillages, de strass, de bijoux, de fleurs, et toujours avec les pattes d'éléphant. À la fin des années 70, de nombreux aspects vestimentaires hippies seront récupérés par la mode disco, adaptés sous une forme plus urbaine. Par la suite, le port du pantalon en jeans est probablement le seul attribut vestimentaire hippie à avoir résisté au temps et aux diverses modes qui se sont succédé, puisqu'il est toujours resté très présent depuis 40 ans.


VI.3. La musique
La musique est un élément capital et fédérateur des hippies. Le phénomène hippie sécrète une esthétique complète, musicale d'abord (Grateful Dead, Jimi Hendrix, The Doors, Pink Floyd, Crosby, Stills & Nash (and Young), Jefferson Airplane, Gong...) avec les premiers festivals, Monterey en 1967, Woodstock en 1969, l'Île de Wight en 1970, qui rassemblent des centaines de milliers de spectateurs ; mais aussi picturale, théâtrale, etc. Une nouvelle génération de chanteurs apparaît, dont Bob Dylan, représentant un nouveau genre musical : la protest song. 
Les hippies apprécient la country, folk (Bob Dylan) ou le rock psychédélique (Janis Joplin), mais ils trouvent aussi leur inspiration beaucoup plus loin avec notamment Ravi Shankar, joueur de sitar indien qui participa au festival de Monterey. Ces sélections musicales représentent bien le mouvement par une volonté d'ouverture aux différentes cultures et d'affranchissement des règles en vigueur. 



VII. Les réactions
La révolte contre l'ordre établi eut des conséquences sur le mouvement hippie. Outre les poursuites pour usages ou possessions de drogues, des condamnations pour outrage aux moeurs répondirent à leurs provocations en ce domaine. Les communautés connurent diverses tracasseries, qu'elles soient ou non des squats. 
La « société de consommation » tant décriée des hippies s'accommoda par contre fort bien de ce mouvement qu'elle ne voulut voir que comme un effet de mode. Les productions décrivant les hippies furent des succès commerciaux, comme la comédie musicale Hair ou, pour les livres, L'antivoyage de Muriel Cerf. Le film du festival de Woodstock fut présenté à Cannes, et les idoles pop connurent la gloire à Hollywood. 
Après avoir moqué les « Cheveux longs, idées courtes » Johnny Halliday lui même s'afficha un temps en look hippie pour chanter Jésus Christ est un hippie.             

VIII. Le déclin
Le concert gratuit des Rolling Stones à Altamont en décembre 1969, qui se voulait un second Woodstock, rassembla 300 000 personnes à l'est de San Francisco. Mais plusieurs décès lors de cet événement, dont celui de Meredith Hunter, un jeune homme de 18 ans qui fut poignardé par un membre du service d'ordre constitué de Hell's Angels (car il avait pointé un revolver en direction de Mick Jagger), ainsi que l'adoption du style hippie par des personnalités comme Charles Manson et sa famille, condamnés pour meurtres (dont celui de Sharon Tate) dans la région de Los Angeles, portèrent un coup fatal au Peace and Love du mouvement. L'Amérique choquée et une bonne partie des hippies eux-mêmes commencèrent à prendre des distances sans pour autant que le mouvement disparaisse tout à fait. 
Le passage aux « drogues dures » et la mort de Jimi Hendrix, de Jim Morrison et de Janis Joplin, entre autres, à la suite d'abus d'alcool, de médicaments ou par overdose contribua grandement à l'impression de chute. Neil Young écrivit The Needle and the Damage Done (« l'aiguille et les dommages causés ») pour évoquer tardivement le problème. 
Avec la fin de la guerre du Vietnam, les médias perdirent leur intérêt pour les hippies. Avec l'arrivée du heavy metal, du disco et du punk, les hippies commencèrent même à apparaître ridicules. 
La plupart des hippies finirent par abandonner leur envie de régénérer le « vieux monde » et se rangèrent dès la fin des années 1970 et le courant des années 1980. La trentaine venue, ils trouvent du travail, fondent une famille et s'intégrent dans la société de consommation qu'ils dénonçaient auparavant. Une étude américaine estimait que 40% des hippies californiens s'étaient rangés, moins de 30% restant « en marge ». Jerry Rubin, devenu un des premiers actionnaires d'Apple, déclarait en 1985 : « Non, je ne lutte plus contre l'État. Ce n'est plus la peine, ce n'est plus le bon combat .../... La meilleure, la seule façon aujourd'hui de combattre l'État, c'est de le remplacer. Et nous sommes assez nombreux pour le faire. »22. 


IX. L'héritage du mouvement hippie

IX.1. La culture
Dans les arts, la musique et le pop-art marquèrent les esprits. Le slogan Flower Power (« pouvoir des fleurs ») était le symbole de la non-violence. La génération hippie a révolutionné la musique, l'art et a ouvert la voie à l'écologie, à l'action humanitaire, au pacifisme, à la libération sexuelle, au féminisme, entre autres, lesquels sont autant de symboles d'une révolution de la culture et des moeurs, aujourd'hui complètement intégrés dans les sociétés occidentales, sans que celles-ci aient forcément conscience de leurs origines hippies. Au début des années 1990, la rencontre entre les derniers hippies de Goa et les disc-jokey internationaux, fans de musiques électroniques et issus, en partie, de la vague Acid house, a donné naissance à la Trance-Goa ou trance psychédélique (psytrance), régulièrement jouée depuis en rave party. 


IX.2. Les moeurs 
Les hippies sont à l'origine de divers changements dans les moeurs occidentales, le principal concernant la sexualité. En plus de la liberté exprimée dans les relations amoureuses, les premiers sex-shop vendant divers jouets sexuels (Good Vibrations à San Francisco était le premier) et la diffusion des films pornographiques et leurs projections en salle de cinéma sont apparues au sein de la communauté hippie. Ils allèrent jusqu'à demander la légalisation de la prostitution à une époque où la masturbation était publiquement condamnée et où personne n'aurait jamais ouvertement fait la promotion du plaisir. L'amour n'est plus honteux, et l'homosexualité n'est plus un tabou; c'est à cette époque que la première Gay Pride à lieu à New York, et San Francisco demeurera la capitale des deux tendances.


IX.3. Les institutions
Le « mouvement hippie », bien que peu structuré, portait en lui les germes d'un renouvellement inventif de la culture et du mode de vie des années d'après-guerre qui, par la réussite même de ses buts matérialistes, arrivait à un essoufflement particulièrement perceptible par la jeunesse. Dans différents domaines, des idées nouvelles perçaient : l'autogestion, l'écologie et le rejet des religions traditionnelles. 
Il est difficile de déterminer précisément quelle influence peut être exclusivement attribuée aux hippies, mais ils sont, entre autres, crédités de divers apports, dont l'« auto-stop », une mode vestimentaire encore en vogue au xxie siècle, les communautés écologiques et leurs propositions, la promotion d'un usage « récréatif » de drogues, les coopératives etc. 


IX.4. Les faillites
La révolution hippie s'est rapidement éteinte malgré le choc salutaire qu'elle apporta à la société de l'époque. Selon certaines analyses, elle a souffert principalement du manque de discernement dans l'attaque des institutions qui étaient toutes mises dans le même panier. En se coupant de possibles ressources, à cause de ce qui pouvait être perçu comme de la paranoïa, elle ne pouvait que disparaître. La prédominance des drogues dans la culture et les communautés hippies ainsi que les décès qui en ont résulté ont terni l'idéal des premiers temps. L'explosion de liberté s'est faite au détriment d'un projet structuré dont l'absence a fini par provoquer la dissolution du mouvement. 
Le sénateur de New-York, Robert Kennedy, présentait en 1967 la revendication hippie de cette manière : « Ils veulent être reconnus comme des individus dans une société où l'individu joue un rôle de moins en moins important. Voilà une combinaison difficile ». Cet individualisme est pourtant passé dans les moeurs et l'arrivée du néo libéralisme aurait pour certains récupéré, en les dénaturant, les valeurs hippies. 




<TXT=w-HomoSapiens5>

I. Appellation 

I.1. Appellations courantes
Le mot français « homme » est une évolution du latin hominem, forme accusative de homo et se réfère avant toute chose à l'espèce Homo sapiens dans son ensemble. Le mot « homme », dans une deuxième acception, désigne aussi l'individu mâle. L'individu femelle est, quant à elle, dénommé « femme ». 
Certaines langues font la distinction entre l'homme « être humain » et l'homme « individu mâle » : par exemples le latin (homo = être humain et vir = être humain mâle), l'allemand (Mensch = être humain et Mann = être humain mâle). En français, certains dialectes (dont celui de France) utilisent indistinctement le terme « homme » tantôt pour parler du mâle, tantôt pour parler de l'espèce, alors que d'autres (comme celui du Québec) préféreront les termes génériques « personne » et « humain » pour désigner un membre de l'espèce et réserveront la dénomination « homme » pour parler du mâle humain adulte. Les droits de l'homme, par exemple, sont dénommés ainsi dans de nombreux pays francophones, mais, au Québec, on parle plutôt de « droits de la personne ». 
Toutefois, on peut remarquer l'emploi de la majuscule (Homme) pour distinguer l'espèce (Homo Sapiens) de l'être humain mâle (homme). 


I.2. Nom scientifique
Le nom Homo sapiens relève de la terminologie scientifique introduite par Carl von Linné, élaborée pour sa classification systématique des espèces : la dénomination binomiale. En dehors de l'usage qui en est fait pour cette dénomination le mot latin « homo » doit porter une minuscule lorsqu'il est utilisé uniquement en tant que mot latin. Lorsqu'il est utilisé en tant que nom biologique de genre (« Homo »), c'est-à-dire le premier terme de la dénomination, il doit porter la majuscule. La dénomination scientifique complète de l'espèce humaine est, suivant cette terminologie : Homo sapiens, Linné 1758.
La signification des différents éléments de cette dénomination est la suivante :

Homo est un mot latin au nominatif (avec majuscule et en italique) qui signifie « homme » en français. Il désigne ici le genre biologique. 
sapiens est un adjectif latin (avec minuscule et italique), qui signifie en français : intelligent, sage, raisonnable ou encore prudent. Il désigne ici l'espèce. 
Linné est le nom du scientifique qui a nommé et décrit l'espèce. 
1758 est l'année de l'appellation. 

Toutefois, en pratique, en zoologie, le nom et l'année sont rarement précisés. 
Jusqu'en 2003, l'espèce Homo sapiens était subdivisée en deux groupes distincts, considérés comme deux sous-espèces, dont l'une était l'espèce humaine actuelle, et l'autre, une espèce cousine éteinte, celle de l'homme de Néandertal. Comme pour toute sous-espèce la conséquence terminologique a été de créer des noms trinomiaux en rajoutant un adjectif, toujours latin (et en italique), après le nom d'espèce. C'est ainsi que l'espèce humaine était appelée Homo sapiens sapiens. Bien que souvent encore entendue, cette terminologie n'est plus en vigueur pour la majorité des scientifiques. En effet, n'étant pas une terminologie constitutive, mais référentielle, elle est le réceptacle évolutif qui reflète l'état des connaissances et la place de l'homme dans la compréhension que celui-ci a du monde : de nouvelles connaissances ou une nouvelle compréhension pourront produire une nouvelle classification, qui pourra conduire à une nouvelle dénomination. 
Le deuxième atout de cette terminologie est, depuis Linné, d'avoir offert un langage commun. Par delà les noms vernaculaires propres à chaque langue pour désigner l'espèce humaine ou les membres de celle-ci : Human, Mensch, Ser humano... et parfois multiples au sein d'une même langue : l'espèce humaine, l'homme, l'humain ; Homo sapiens se présente comme un vocable de référence, certes de nature scientifique, mais qui a su par ailleurs acquérir une notoriété dépassant celle du jargon. 


II. Origines
Les recherches en paléontologie humaine ou paléoanthropologie, ainsi que des études en génétique aboutissent à l'idée que la population originelle pour tous les humains se situait en Afrique, il y a très approximativement 200 000 ans.

II.1. Paléoanthropologie 
La classification des ossements fossiles dans l'espèce Homo sapiens, est réalisée par le rapprochement des morphologies osseuses comme :

une face réduite (un angle facial entre 82 et 88°) ;
l'absence de bourrelet sus-orbitaire ;
un menton saillant.

De plus ces caractères propres doivent être combinés à d'autres caractères comme un volume cérébral important: entre 1 400 et 1 600 cm3. Par exemple le « récent » fossile de l'homme de Flores n'a pu être attribué à Homo sapiens en raison d'un volume cérébral de seulement 400 cm3. 
Leur datation et la délimitation de zones géographiques de répartition sont de précieux renseignements sur nos origines. Elles permettent de faire des déductions ou d'affiner les hypothèses. La précision de cette science est limitée car elle est dépendante des éléments osseux et matériels mis au jour au fur et à mesure des fouilles. Ces découvertes ne permettent pas aujourd'hui à la paléontologie d'expliquer avec précision où, quand, et comment est né le premier représentant d'Homo sapiens. On sait néanmoins, qu'Homo sapiens trouve son origine dans l'arborescence évolutive des homininés se trouvant en Afrique. Alors que l'homme de Néanderthal a fait son apparition en Europe depuis 250 000 ans, Homo sapiens n'aurait migré depuis l'Afrique vers l'Europe et l'Asie que vers la fin des grandes glaciations vers -40 000 ans. Tous deux ont été contemporains l'un de l'autre, mais les conditions de leur rencontre et les détails de leurs « relations » ne sont pas connus. L'homme de Néanderthal est une espèce éteinte alors que Homo sapiens s'est maintenu, a colonisé tous les continents terrestres, a commencé à s'implanter sur divers astres (la Lune, préparation de l'implantation sur la planète Mars) du système solaire et même à projeter des outils (sondes Voyagers et Pioneers) au-delà de celui-ci. 


II.2. Les plus vieux ossements découverts
C'est en Afrique que les plus vieux ossements ont été découverts. Aujourd'hui, les paléontologues donnent à Homo sapiens un âge d'environ 200 000 ans puisque les plus vieux ossements retrouvés sont deux crânes datés de -195 000 ans, et appelés Omo 1 et Omo 2 ; viennent ensuite ceux de l'homme d'Herto encore appelé Homo sapiens idaltu, datés d'environ -154 000 ans. 
Ensuite viennent les ossements de Qafzeh et Skhul en Israël/Palestine datés respectivement de -97 000 et -80 000 ans. 
Les plus célèbres sont ceux de l'homme de Cro-Magnon, datés de -35 000 ans et découverts en France. 
Jusqu'en 2003, l'espèce Homo sapiens était subdivisée en deux sous-espèces, Homo sapiens sapiens et Homo sapiens neanderthalensis. Les résultats d'analyses génétiques ont conduit la plupart des auteurs à considérer ce dernier taxon comme une espèce à part entière, nommée Homo neanderthalensis. L'homme moderne et ses ancêtres immédiats ne sont plus considérés comme des Homo sapiens sapiens mais comme des Homo sapiens, dont ils sont les seuls représentants. 
Les êtres humains actuels appartiennent à cette seule espèce, et sa subdivision en races est généralement considérée comme non pertinente, d'un point de vue biologique. Le 21 décembre 2005 la planète Terre a vu l'espèce humaine atteindre 6,5 milliards de représentants. 


II.3. Les études génétiques s'accordent avec une origine africaine 
Les comparaisons entre différentes populations humaines actuelles des séquences de l'ADN mitochondrial et du chromosome Y suggèrent fortement que tous les humains actuels ont une origine commune située en Afrique. Les comparaisons avec l'homme de Néanderthal semble confirmer qu'il n'y aurait pas eu de croisement avec cette espèce, mais l'hypothèse n'est pas encore complètement exclue.


II.4. L'apparition de l'espèce humaine
Du point de vue scientifique, l'apparition de l'homme résulte d'une évolution biologique à partir d'espèces ancêtres, d'abord des eucaryotes, puis des vertébrés, des tétrapodes et aussi des mammifères arboricoles présentant une allure générale semblable aux singes actuels. Cette évolution depuis notre ancêtre commun le plus récent avec les chimpanzés est relativement bien documentée grâce aux fossiles, bien que des lacunes importantes existent; le fait que les deux espèces de chimpanzé, Pan troglodytes et Pan paniscus, soient considérés comme les espèces vivantes les plus proches de l'Homme est aussi établie par la phylogénétique. 
Les séparations des lignées ayant mené aux différentes espèces de primates actuels, dont le genre Homo, se sont produites de manière successive. La séparation la plus récente entre la lignée humaine et celle d'une autre espèce de primate a été la bifurcation des Homininés en Hominines (lignée humaine) et Panines (lignée des chimpanzés). Selon David Reich de la Harvard Medical School à Boston, cette séparation s'est faite il y a moins de 6,3 millions d'années. Toutefois, ces travaux indiquent également que cette séparation a été progressive, car la comparaison des séquences des chromosomes X de l'Homo sapiens et du chimpanzé montre des similitudes qui semblent refléter une période de ré-hybridation entre des Hominines et des Panines. Une hybridation significative entre au moins une espèce de chimpanzé d'une part, des espèces d'australopithèque et probablement des espèces d'homme d'autre part, conduisant à des échanges de gènes entre les deux tribus, a dû exister pendant peut-être quatre millions d'années selon les auteurs de ces travaux. 
Les mécanismes orientant cette évolution ne sont pas encore entièrement compris, mais la sélection naturelle semble avoir joué un rôle important : l'environnement aurait guidé notre évolution récente bien que les facteurs environnementaux responsables n'ont pas encore tous été identifiés. 
Les théories scientifiques se sont d'abord centré sur l'évolution de la taille du cerveau qui aurait précédé en temps les autres évolutions adaptatives de l'être humain (théorie du singe au gros cerveau). Toutefois la découverte de Lucy qui avait une démarche déjà bipède mais un cerveau de faible volume vint infirmer cette hypothèse, la bipédie étant de loin plus ancienne voire archaïque tandis que l'augmentation du volume cérébral étant un phénomène plus récent. Des empreintes de pas fossilisées datant de 3,75 millions d'années (trouvées à Laetoli en Tanzanie) montrent une bipédie archaïque. Des empreintes comparables aux nôtres datant de 1,51 à 1,52 millions d'années (trouvées au Kenya à Ileret). 



III. Place sur la Terre
Par sa capacité à maîtriser des techniques lui permettant d'affronter des conditions climatiques difficiles, l'être humain vit dans quasiment tous les milieux terrestres et sous quasiment toutes les latitudes. Seules certaines régions extrêmes, comme l'Antarctique, ne sont pas colonisées de manière permanente. 
On estime qu'en 2009 l'humanité compte 6,789 milliards d'individus. 


IV. Place dans le monde vivant 

IV.1. Classification biologique
Les espèces actuellement les plus proches de l'humain sont les deux espèces de chimpanzé : Pan troglodytes (le chimpanzé commun) et Pan paniscus (le bonobo). Dans leur proximité phylogénétique à l'homme viennent ensuite le gorille et l'orang-outan. Le génome des humains ne diffère que de 0,27 % de celui des chimpanzés, et de 0,65 % de celui des gorilles. Ces chiffres conduisent à estimer que notre lignée s'est séparée de celle des chimpanzés il y a environ cinq millions d'années, et des gorilles il y a environ sept millions d'années. 
La démarche phylogénétique part de l'idée que la vie évolue des formes les plus simples aux plus organisées, avec acquisition de plus en plus de caractéristiques nouvelles, même si des pertes secondaires de caractères peuvent se produire au sein des lignées. Ainsi, l'espèce humaine fait partie, comme toute autre espèce du vivant, de plusieurs groupes emboîtés dont chacun est caractérisé par un caractère nouveau, qui se rajoute à ceux déjà accumulés. Notre espèce est classée dans : 

le groupe des eucaryotes (cellules avec un noyau), au même titre qu'un végétal ; 
Le règne animal, au même titre qu'une limace ; 
Les vertébrés (présence de vertèbres...), au même titre qu'une grenouille ; 
Les mammifères (poils, allaitement...), au même titre qu'une souris ; 
Les primates (pouce opposable...), au même titre que les Lémuriens. 

Dans le groupe des primates, Homo sapiens fait partie des : 

Haplorhiniens (plus de rhinarium, mais un nez), au même titre que les tarsiers ; 
Simiiformes (arrière des orbites crâniennes fermées), au même titre que les ouistitis ; 
Catarhiniens (narines dirigées vers le bas), au même titre que les colobes ou les babouins ; 
Hominoïdes (pas de queue), au même titre que les gibbons ou singes hurleurs ; 
Hominoïdés, au même titre que l'orang-outan ; 
Hominidés, au même titre que les gorilles ; 
Homininés, au même titre que les chimpanzés et bonobos ; 
Hominines, au même titre que les Australopithèques (éteints), Ardipithèques (éteints) et Paranthropus (éteints) ; 
Le genre Homo, au même titre que l'homme de Néandertal ou l'homme de Flores. 

Parmi toutes les espèces cités ci-dessus, aucune n'est « inférieure » à aucune autre. Seuls les degrés de parenté diffèrent, en allant des espèces les plus éloignées jusqu'aux espèces les plus proches de nous.                

IV.2. Préjugé anthropocentrique
On entend souvent : « l'homme descend du singe ». Cette phrase est en fait fausse : l'humain partage avec les singes actuels des ancêtres communs, qu'on ne connaît pas encore. L'Homo sapiens serait en fait l'espèce actuelle la plus proche des chimpanzés, et inversement. Donc, parmi toutes les espèces vivantes actuelles, il n'y aurait aucun ancêtre, mais simplement des espèces qui sont plus ou moins apparentées entre elles. Du point vue scientifique, les humains ne sont pas « plus évolués » que les chimpanzés. Ils ne sont pas « supérieurs » aux autres êtres vivants, ni aux singes, ni aux bactéries ; chaque espèce est adaptée à son milieu. Parler en termes de supériorité d'une espèce relève de jugements de valeur. 
Selon Jean-Marie Schaeffer, on a longtemps estimé, en sociologie et en philosophie, que l'espèce humaine était à part dans le monde vivant. Dans son ouvrage La fin de l'exception humaine, il estime que l'espèce humaine doit être considérée de la même manière que les autres espèces « pour appréhender la complexité de notre psychisme et de nos relations sociales ». 


IV.3. Le propre de l'homme ? 
La notion du propre de l'homme relève à la fois de la philosophie et de la science, notamment la paléoanthropologie et la sociobiologie, et a une grande importance religieuse. 
Les plus anciennes traces de réflexion sur la spécificité de l'homme remontent à l'Antiquité. Par la suite, à de nombreuses reprises, les scientifiques et les penseurs ont tenté de définir le propre de l'homme par des caractéristiques anthropocentriques aujourd'hui dépassées : 

« Ainsi, même dans le cadre des théories modernes de l'évolution, qu'on appelle néodarwinisme ou théorie synthétique de l'évolution - terme inventé pas Julian Huxley - et qui domine la pensée évolutionniste entre 1947 et 1977, les évolutionnistes s'efforcent de réserver une place à part à l'homme, étant entendu que si son corps à évolué, il reste que ce qui fait l'humain échappe aux lois de l'évolution13. » 

Durant les développements de la science moderne, les « spécificités » avancées comme étant propres à l'homme ont tour à tour été remise en question. Ainsi, il fut avancé que le propre de l'homme était l'usage de l'outil, et il fut aussi question de la culture, qui semblait seulement exister chez notre espèce animale. Toutefois, les découvertes récentes montrent que les grands singes manient eux aussi des outils, et sont capable de transmettre des éléments de culture. Le caractère bipède exclusif de l'homme est lui aussi remis en question : la bipédie aurait pu pré-exister parmi l'ancêtre commun des hominoïdes, dans ce cas ce n'est pas la lignée humaine qui aurait acquis la bipédie, mais ce seraient les lignées existantes de grands singes qui l'auraient perdue. Le rire a lui aussi été souvent présenté comme étant le propre de l'Homme mais de nombreuses recherches le montrent comme appartenant également aux grands singes et même aux rats. 
Du point de vue de la biologie, cette question peut sembler peu pertinente si l'on prend l'angle d'approche de la sociobiologie : elle est « évidente » par sa présence. Par contre, la paléoanthropologie apporte une réponse intéressante à la question, tout en se concentrant sur les aspects biologiques de l'Homo sapiens. Une citation de Pascal Picq résume cette position scientifique : 

« L'humain est bien une invention des hommes, qui repose sur notre héritage évolutif partagé, mais n'est pas une évidence pour autant. Homo sapiens n'est pas humain de fait.15 » 



V. Caractéristiques physiques 

V.1. Description 
Homo sapiens peut être présenté sommairement comme étant un mammifère terrestre ; dressé sur deux membres inférieurs, qui constituent la base de son corps, en proportion environ de moitié, prolongés, en haut, par le tronc, le cou, puis la tête. Disposant de deux membres supérieurs, se terminant chacun par une main, ce qui lui permet de saisir et manipuler ; d'une taille à l'âge adulte pouvant aller d'environ 80 cm à environ 2,50 m dans les deux extrêmes du nanisme et du gigantisme (plus couramment de 1,40 à 2 m) ; disposant d'organes sexuels ; à la couleur de peau empruntant les degrés du noir, du marron, du beige ou du rosé, pouvant être recouvert de poils par endroits, de forme allant du frisé au lisse et dont la couleur est, indépendamment de la couleur de la peau, de teintes noire, brune, blonde, rousse ou blanche ; aux yeux aux teintes du marron, du bleu, du vert ou du gris... 
L'évolution vers Homo sapiens se caractérise par les éléments suivants : 

expansion de la boîte crânienne et du volume du cerveau, en moyenne 1 400 cm3 (plus de deux fois celui des chimpanzés ou des gorilles). Pour certains anthropologues, la modification de la structure du cerveau est plus importante encore que l'augmentation de sa taille ; 
diminution de la taille des canines ; 
locomotion bipède, marche ; toutefois pour certains anthropologues, l'aptitude à courir est plus importante que l'aptitude à marcher. 
descente du larynx, ce qui permet le langage articulé. 

Les liens entre ces éléments, leur valeur adaptative, et leur rôle dans l'organisation sociale est sujet à débat parmi les anthropologues. La taille moyenne des hommes, aujourd'hui, en France, est de 1,75 m, et celle des femmes de 1,62 m, pour des masses respectives moyennes de 75 et 61 kg. Les données individuelles sont très variables autour de ces moyennes, avec une forte influence de facteurs environnementaux, des comportements et des régimes nutritionnels. Les moyennes elles-mêmes varient beaucoup selon les populations et les époques.
Les jeunes naissent avec une masse autour de 3 kg, et une taille d'environ 50 à 60 cm, après une gestation de neuf mois. Totalement dépendants à la naissance, leur croissance dure plusieurs années. La maturité sexuelle survient entre 12 et 15 ans. La croissance des garçons continue souvent jusque vers 18 ans (la croissance se termine vers 21-25 ans avec la solidification de la clavicule). L'espérance de vie est très dépendante des conditions matérielles et de la disponibilité de soins médicaux. L'espérance de vie se situe aujourd'hui autour de 75 ans dans les pays les plus riches, et est inférieure à 40 ans dans les plus pauvres. Des cas isolés de longévité approchent 120 ans, et la personne ayant vécu le plus longtemps sans doute possible sur son âge est la française Jeanne Calment, qui a vécu plus de 122 ans. 
L'être humain possède 23 paires de chromosomes (contre 32 pour le cheval).


V.2. Représentation de Homo sapiens 
Bien que les premières manifestations de préoccupations esthétiques ou symboliques soient attribuables à l'homme de Néandertal durant le Paléolithique moyen, les plus anciennes représentations humaines authentifiées comme telles sont le fait d'Homo sapiens et peuvent être datées du Paléolithique supérieur (vers 40 000 à 10 000 ans BP). Ainsi à l'Aurignacien (vers 40 000 à 28 000 ans BP), faciès culturel le plus ancien et attribuable à l'homme anatomiquement moderne en Europe, sont associées les statuettes de Vogelherd, de Geissenklösterle et de Hohlenstein-Stadel qui restituent des figures en ronde bosse représentant des mammouths, des félins, des ours, des chevaux et des hommes. On note aussi dans l'art pariétal, comme à la grotte Chauvet, la représentation de vulves féminines ainsi que d'individus mi-homme mi-bison. Puis au Gravettien (29 000 à 22 000 ans BP) sont sculptées des figures féminines dites vénus paléolithiques. Au Magdalénien (19 000 à 10 000 ans BP), les représentations humaines sur paroi ou sur objet se font plus fréquentes. 
Léonard de Vinci, avec ses dessins d'anatomie, est le premier à étudier le corps humain avec un oeil médical, suivi par Michel-Ange (voir par exemple le « David » ci-dessous) ; ses tableaux s'efforcent de représenter le corps de l'homme avec la précision de la masse organique qui le compose.



VI. Droit

VI.1. En droit International
Le jeudi 30 mars 2006 s'est tenu à l'UNESCO un colloque ayant pour thème « L'espèce humaine peut-elle se domestiquer elle-même ? ». Le directeur général de l'UNESCO, Monsieur Matsuura, avait alors exposé les deux enjeux de cette question : l'enjeu scientifique, mais également l'enjeu éthique, et exposa ainsi la problématique : « Pour la première fois de son histoire, l'humanité va donc devoir prendre des décisions politiques, de nature normative et législative, au sujet de notre espèce et de son avenir. Elle ne pourra le faire sans élaborer les principes d'une éthique, qui doit devenir l'affaire de tous. Car les sciences et les techniques ne sont pas par elles-mêmes porteuses de solutions aux questions qu'elles suscitent. Face aux dérives éventuelles d'une pseudo-science, nous devons réaffirmer le principe de dignité humaine. Il nous permet de poser l'exigence de non-instrumentalisation de l'être humain ». L'espèce humaine ainsi appréhendée dans sa vulnérabilité génétique pose la question de son statut juridique : Est-elle un sujet de droit ? Est-elle protégée en elle-même ? Comment est-elle protégée ? 
Paradoxalement, alors que les conférences insistent de plus en plus sur l'espèce humaine et sur son devenir, les textes internationaux ne protègent pas pour le moment l'espèce humaine par un dispositif qui lui serait expressément rattaché. 
Les quelques rares textes qui font mention de l'espèce humaine le font dans leur préambule, au titre de fondement général aux dispositions du corps du texte, qui ne vise donc pas directement à protéger l'espèce humaine elle-même ; ainsi peut-on lire dans le préambule de la Déclaration sur la race et les préjugés raciaux adoptée par acclamation le 27 novembre 1978 à la vingtième session de la conférence générale de l'Organisation des Nations unies pour l'éducation, la science et la culture à Paris pour fonder la non hiérarchisation de ses membres : alinéa 5 : « Persuadée que l'unité intrinsèque de l'espèce humaine et, par conséquent, l'égalité foncière de tous les être humains et de tous les peuples, reconnue par les expressions les plus élevées de la philosophie, de la morale et de la religion, reflètent un idéal vers lequel convergent aujourd'hui l'éthique et la science, ». Il ne faut ici pas confondre la protection de l'espèce humaine en tant que telle, et l'interdiction de la hiérarchisation de ses membres qui est précisément l'objet des dispositions de la Déclaration. 
La Convention pour la protection des Droits de l'homme et de la dignité de l'être humain à l'égard des applications de la biologie et de la médecine: Convention sur les Droits de l'homme et la biomédecine élaborée au sein du Conseil de l'Europe, convention dite d'Oviedo du 4 avril 1997, fait également référence à l'espèce humaine dans l'alinéa 10 de son préambule : « Convaincus de la nécessité de respecter l'être humain à la fois comme individu et dans son appartenance à l'espèce humaine et reconnaissant l'importance d'assurer sa dignité; ». L'espèce humaine est de premier abord présentée de nouveau comme attribut d'un sujet de droit pour fonder la protection de celui-ci ; toutefois, la problématique du Directeur Général de l'UNESCO trouve dans le corps de la convention une résonance au sein de l'article 13 de la convention, intitulé « Interventions sur le génome humain » situé sous le Chapitre IV relatif au « Génome humain ». En effet, cet article énonce qu' « Une intervention ayant pour objet de modifier le génome humain ne peut être entreprise que pour des raisons préventives, diagnostiques ou thérapeutiques et seulement si elle n'a pas pour but d'introduire une modification dans le génome de la descendance. ». Ce texte se préoccupe explicitement, non pas seulement de la définition génétique de l'individu lui-même, mais également de sa descendance à travers son patrimoine génétique, et, par là même, de l'espèce. La protection ainsi élaborée n'est cependant pas absolue. En effet, le texte ne retient la modification du génome de la descendance comme illicite que dans la mesure où cette modification n'est pas le but poursuivi ; a contrario, si le génome de la descendance n'est pas la motivation directe de la modification du génome, cette modification est licite dans les cas gouvernés par « des raisons préventives, diagnostiques ou thérapeutiques » relatives à la personne subissant l'intervention. 
La procédure se décompose traditionnellement en une Signature par un plénipotentiaire (Chef d'État, Ministre des affaires étrangères...) et une Ratification, qui consiste en une confirmation de cette signature, par l'organe compétent propre à chaque État, qui lie ainsi, de façon effective, l'État au Traité. Ainsi, une convention internationale n'a théoriquement valeur de droit positif que si, après avoir été signée, elle a été ratifiée (en droit français la ratification est le fait du Président de la République, conformément à l'article 52 de la Constitution, après autorisation du Parlement selon les cas énumérés à l'article 53 de la Constitution). La portée de cette protection est donc très relative. 
La valeur juridique de ces traités dépend de la compréhension propre à chaque système juridique de ce qui constitue une atteinte à l'espèce humaine. La France a adopté récemment une des premières législations spécifiques visant explicitement à protéger l'espèce humaine. 


VI.2. En droit français
La loi du 29 juillet 1994 relative au corps humain (une des lois dites bioéthiques) a introduit, dans le droit français, la disposition selon laquelle « Nul ne peut porter atteinte à l'intégrité de l'espèce humaine » (article 16-4 1er alinéa Code civil français). Cette disposition figure parmi les principes généraux devant gouverner les recherches scientifiques et les pratiques médicales (articles 16 à 16-9 c.civ.). D'importants débats existent sur la portée et la signification pratique à donner à cette interdiction : en effet, les alinéas subséquents de l'article 16-4 énoncent les interdictions de l'eugénisme, du clonage reproductif (cette interdiction a été introduite par la loi bioéthique du 7 août 2004), et de la modification des « caractères génétiques dans le but de modifier la descendance de la personne ». Ainsi, le premier alinéa doit-il être interprété indépendamment des autres, ce qui reviendrait à distinguer l'interdiction de porter atteinte à l'intégrité de l'espèce humaine, l'interdiction des pratiques eugéniques et l'interdiction du clonage, auquel cas le premier alinéa demeure énigmatique ? Ou ce premier alinéa doit-il être interprété à la lumière des alinéas subséquents, auquel cas l'intégrité de l'espèce humaine serait atteinte par la réalisation d'actes d'eugénisme ou de clonage ? 
Une réponse semble pouvoir exceptionnellement être recherchée dans la traduction pénale de ces interdictions : en effet, ce sont les mêmes textes qui figurent dans le Code civil français et dans le code pénal, textes qui ont été, de surcroît, introduits par les mêmes lois. Protégée pénalement depuis 1994 à l'article 511-1 du code pénal, dans le livre qui protégeait les animaux des sévices graves (le Livre V du code pénal), l'espèce humaine a reçu par la loi bioéthique du 7 août 2004 une protection renforcée, les dispositions la protégeant ayant été déplacées en partie dans le livre II, lui faisant partager à présent l'intitulé du Titre I qui réprimait les crimes contre l'humanité, soit : « Des crimes contre l'humanité et contre l'espèce humaine », et lui consacrant le Sous-titre II intitulé « Des crimes contre l'espèce humaine » regroupant les articles 214-1 et suivant. 
L'enjeu de ces dispositions est de préserver les spécificités biologiques de l'espèce humaine que sont toutes ses caractéristiques génétiques : 

par la répression des « pratiques eugéniques tendant à l'organisation de la sélection des personnes » (article 214-1 Code Pénal). De plus le Conseil d'État, dans son rapport du 25 novembre 1999 Lois bioéthiques : cinq ans après, précisa qu'il fallait entendre dans cette définition le caractère systématique de la sélection afin de ne pas assimiler les pratiques de procréation médicalement assistée aux pratiques eugéniques : leur caractère non systématique est apprécié par l'exigence de « choix propres [, par nature contingent,] à des couples confrontés à l'annonce d'une maladie d'une particulière gravité ». La pertinence de ce critère est critiquée par la doctrine qui propose comme autre critère de distinction : le cadre thérapeutique ; ou encore, sur la distinction kantienne selon laquelle il faut considérer l'homme non comme un moyen mais comme une fin, distinguer la sélection motivée par le sentiment d'empathie envers l'être à naître atteint d'une « maladie d'une particulière gravité reconnue comme incurable aux moment du diagnostic » (articles 2131-1, 2131-4, 2131-4-1, 2141-2 Code de la Santé publique), de la sélection motivée par un sentiment utilitariste de cet être perçu comme devant permettre l'amélioration de l'espèce humaine. 
par la répression du clonage reproductif (article 214-2 Code pénal), comme portant atteinte au caractère sexué de la reproduction humaine (consistant en la rencontre de gamètes de patrimoine génétique différent), et portant atteinte, à grande échelle, à la diversité biologique de l'espèce humaine (qui est un de ses facteurs d'adaptation). Le clonage thérapeutique, consistant en la création d'un embryon humain à partir de cellules d'une personne malade, destiné à fournir des cellules souches prélevées puis cultivées pour fournir un tissu ou un organe génétiquement compatible avec le patient, ou implantées dans le corps de celui-ci pour que son organisme reconstitue des cellules défaillantes, n'est pas réprimé au titre de la protection de l'espèce humaine, mais au titre de la protection de l'embryon dans le Livre V du code pénal(art. 511-17 et 511-18 Code Pénal). Par ailleurs l'infraction de clonage thérapeutique est un délit (puni d'un maximum de 7 ans d'emprisonnement et 100 000E d'amende), alors que l'infraction de clonage reproductif est un crime (puni, tout comme le crime d'eugénisme, d'un maximum de 30 ans de réclusion criminelle et de 7 500 000 E d'amende). Cette différence de traitement est toutefois elle aussi critiquée dans la mesure où d'un point de vue anthropologique, toujours selon la distinction kantienne, le clonage thérapeutique déclasse la perception de la vie humaine au rang de médicament (à ne pas confondre avec le bébé médicament qui consiste, pour un couple ayant un enfant malade et désirant avoir un deuxième enfant, à saisir l'opportunité que peut offrir la compatibilité génétique des cellules du petit frère pour sauver l'aîné, par le prélèvement de cellules sur le cordon ombilical, le don de sang ou encore de moelle épinière, ce qui n'entrave nullement l'accès sain à la vie de cet enfant), donc de moyen, ce qui peut apparaître au moins aussi grave que le clonage reproductif (argument anthropologique proposé par Mme Marie-Angèle Hermitte, Directeur d'étude à l'École des hautes études en sciences sociales); toutefois d'autres auteurs justifient cette différence par le caractère d'utilité publique, d'intérêt général (pour les personnes nées atteintes aujourd'hui et demain d'une maladie grave et incurable), que peut revêtir la motivation de procéder à de telles recherches, contre le clonage reproductif motivé par le seul intérêt égoïste des couples d'avoir un enfant (Mikaël Benillouche, Maître de conférence à la faculté de droit de l'université de Picardie). 

Les crimes contre l'espèce humaine peuvent être considérés comme le deuxième ensemble d'infractions les plus grave du système juridique français, après les crimes contre l'humanité, apparaissant en deuxième position (après les crimes précités) dans l'énonciation des infractions dans le code pénal, et l'action publique se prescrivant, par exception au droit commun (10 ans pour les crimes), par un délais de 30 ans (ce délai ne commençant par ailleurs à courir qu'à la majorité de l'enfant qui serait né du clonage), l'action publique relative aux crimes contre l'humanité étant, quant à elle, imprescriptible. On peut, par ailleurs, voir dans les crimes contre l'espèce humaine le complément de la protection de l'homme initiée par les crimes contre l'humanité, ces derniers protégeant l'homme dans sa dimension métaphysique : le respect de son humanité et de sa dignité, et les crimes contre l'espèce humaine protégeant l'homme dans sa dimension matérielle : sa définition génétique et sa spécificité biologique.


VII. Recherche 
Une équipe internationale de chercheurs a démontré que l'arrivée d'Homo Sapiens sur le Sahul (Australie, Nouvelle Guinée, Tasmanie) avait pris quelque 20 000 générations. Ces hommes sont à l'origine des Aborigènes d'Australie, étant restés quelque 50 000 ans isolés des autres populations. Cette étude a été réalisée en comparant les ADNmt et les chromosome Y de centaines d'Aborigènes et de Malaisiens. Cela réfute formellement la thèse qui postulait qu'Homo Erectus aurait convergé vers un homo sapiens tout comme le développement qui avait lieu en afrique.



<TXT=w-Liberté6>
I. Le concept de liberté en philosophie

I.1. Un concept clef de métaphysique 
La question de la liberté peut être considérée comme une question métaphysique par excellence dans la mesure où elle concerne le statut de l'être humain au sein de la nature. La liberté qualifie en effet la relation de l'être humain en tant qu'agent et du monde physique, relation notamment considérée dans son rapport à un déterminisme supposé ou réel. Cette question concerne donc particulièrement l'immanence et la transcendance de la volonté humaine par rapport au monde. 
La liberté s'oppose en général (ce n'est donc pas toujours le cas) au déterminisme, au fatalisme et à toute doctrine qui soutient la thèse de la nécessité du devenir. Le concept de liberté divise très schématiquement les philosophes en deux camps : ceux qui en font le fondement de l'action et de la morale humaines (Épicure, Descartes, Kant), et ceux qui nient une quelconque transcendance de la volonté par rapport à des déterminismes tels que la sensibilité (Démocrite, Spinoza, Nietzsche) : 

« Il existait deux opinions sur lesquelles se partageaient les anciens philosophes, les uns pensant que tout se produit par le destin, en sorte que ce destin apportait la force de la nécessité (Démocrite, Héraclite, Empédocle, Aristote étaient de cet avis), les autres pour qui les mouvements volontaires de l'âme existaient sans aucune intervention du destin ; Chrysippe, en position d'arbitre officieux, me paraît avoir choisi la position intermédiaire ; mais il se rattache plutôt à ceux qui veulent voir les mouvements de l'âme libérés de la nécessité. » 

- (Cicéron, Du destin, §39)
On dirait aujourd'hui qu'il y a une opposition entre physicalisme et mentalisme, i.e. entre la causalité physique (physicalisme) à laquelle tous les êtres peuvent être réduits et la causalité mentale (mentalisme), qui peut être une théorie matérialiste, tout en reconnaissant une action propre du mental. Dans le premier cas, il s'agit d'expliquer comment on peut naturaliser la volonté, sans reconduire un dualisme métaphysique classique, et comment il est encore possible de parler d'action et de responsabilité, alors que l'on en a supprimé la condition ; dans le second cas, il s'agit plutôt d'expliquer comment une causalité mentale est possible qui évite aussi ce dualisme souvent difficile à rendre intelligible. Un des points les plus intéressants que met ainsi en lumière cette opposition, c'est le caractère souvent difficile à déterminer du concept de liberté. 


I.2. Origine et analyse du problème
Le problème de la liberté surgit naturellement quand la raison humaine cherche à unifier les différents éléments de sa représentation du monde. En effet, si l'explication philosophique comprend la réalité dans son intégralité, au moins idéalement (et au contraire des sciences qui ont une partie seulement du monde pour objet), alors un effort d'unification de notre connaissance par une causalité unique est exigible, et cela afin d'éviter les contradictions qui découlent de l'hypothèse de l'existence de plusieurs causalités (psychique et physique) : il semble en effet impossible de penser l'interaction de deux causalités hétérogènes. Ce problème a particulièrement sollicité la réflexion des philosophes de l'antiquité. La physique hellénistique est ainsi nettement déterministe. Mais cette unité causale a soulevé et soulève encore de nos jours des problèmes : si on unit les trois parties de la connaissance (physique, éthique, logique), et aujourd'hui les sciences humaines et les sciences de la nature, comment résoudre l'antagonisme entre destin et liberté ? Le problème qui se pose est essentiellement d'ordre moral. Epicure fut contraint d'inventer le clinamen, et les stoïciens inventèrent des raisonnements très subtils pour tenter d'échapper à ce qui ressemble à une conséquence inévitable de ce qu'on appelle aujourd'hui le physicalisme. 
L'unité de nos représentations serait alors une unité logique. Mais la question se pose : si tout dépend du destin, comment certaines choses peuvent-elles encore dépendre de nous ? Ou bien la nature est seule maîtresse des choses, ou bien l'homme est maître lui aussi au sein de la nature. Cette contradiction dans notre connaissance est la troisième antinomie kantienne : suis-je libre, ou suis-je conduit par le destin ? La nature est ici entendue comme un pur enchaînement causal ; il s'agit alors de concilier les deux affirmations : responsabilité morale et actes déterminés. 
Si on nie la causalité naturelle, on fait apparaître un concept de liberté qui implique la nouveauté absolue dans l'ordre de la nature : la liberté humaine doit pouvoir ouvrir des possibles en produisant des actions non-déterminées, indépendantes notamment des inclinations de notre sensibilité. Notre volonté n'a alors aucune cause antécédente. Mais dans ce cas, la liberté n'est pas une réalité intelligible : la liberté sort du néant, elle constitue une sorte de miracle, d'où le caractère presque indicible de ce concept, puisque la liberté semble être dans ce cas au-delà de la portée de l'intellect humain. 
Ainsi, en cherchant à unifier nos connaissances, soit on fait de l'homme un être déterminé, dont la volonté est immanente à la nature (donc on cherche à naturaliser l'humain), soit on fait de l'homme un être transcendant, irréductible en particulier à sa nature animale. 


I.3. Définition et critiques
Une définition du sens commun serait : la liberté est de faire ce qu'on désire sans rencontrer d'obstacle. C'est l'absence de contrainte et l'indépendance, comme, par exemple, le vagabond non assujetti à un ordre social (Arthur Rimbaud, Jack Kerouac, etc). Carmen, dit, dans l'Opéra de Georges Bizet : « Ce que je veux, c'est être libre et faire ce qui me plait », « avoir pour pays l'univers et pour loi sa volonté ». Il faut se défaire de la définition courante de la liberté : "Le pouvoir de faire ce que l'on veut." En faire une simple absence de limites, c'est se condamner à n'y voir qu'une illusion. 
C'est l'ivresse de la liberté : 

Un certain sentiment de liberté peut accompagner l'acte volontaire, et même lorsque l'action est empêchée, il nous reste le sentiment que c'est nous qui décidons de la direction de notre volonté ;
Le sentiment de la liberté peut naître de l'allègement des contraintes sociales, par exemple dans le temps festif (consommation excessive, démesurée), par opposition au temps ouvré (travail et production). La hiérarchie sociale semble renversée, comme dans les saturnales ou le carnaval.

Mais cette liberté n'est pas la liberté au sens philosophique.
En effet, contre la liberté indépendance, il existe au moins deux types de critiques :

Une critique moraliste : cette liberté relève de la licence, i.e. de l'abandon au désir. Or, il n'y a pas de liberté sans loi (Rousseau, Emmanuel Kant), car la liberté de tous serait en ce sens contradictoire : les désirs universalisés s'annuleraient. La loi est donc nécessaire et il faut limiter l'extension de la liberté pour garantir son exercice. Ces limites sont dans l'intérêt même de la liberté, pour éviter la tyrannie, les conflits et l'esclavage. « On pourrait, sur ce qui précède, ajouter à l'acquis de l'état civil la liberté morale qui seule rend l'homme vraiment maître de lui; car l'impulsion du seul appétit est esclavage, et l'obéissance à la loi qu'on s'est prescrite est liberté. » (Rousseau, Le contrat social).

On remarque que dans cette conception philosophique de la liberté, les limites ne sont pas des limites contraignant la liberté de la volonté humaine ; ces limites définissent en réalité un domaine d'action où la liberté peut exister, ce qui est tout autre chose. 

Une critique déterministe : s'abandonner à ses désirs, n'est-ce pas leur obéir, et dès lors un tel abandon ne relève-t-il pas d'une forme déguisée de déterminisme ? Nous serions alors victimes d'une illusion de libre arbitre : nous aurions une fausse conscience de la liberté de notre volonté parce que nous ignorons les véritables causes qui nous font agir. Ainsi, Spinoza écrit dans L'Éthique : « Telle est cette liberté humaine que tous se vantent de posséder et qui consiste en cela seul que les hommes ont conscience de leurs appétits et ignorent les causes qui les déterminent. Un enfant croit librement appéter le lait, un jeune garçon irrité vouloir se venger et, s'il est poltron, vouloir fuir. Un ivrogne croit dire par un libre décret de son âme ce qu'en suite, revenu à la sobriété, il aurait voulu taire. De même un délirant, un bavard, et bien d'autres de même farine, croient agir par un libre décret de l'âme et non se laisser contraindre ».

Nietzsche reprendra cette critique : '« Aussi longtemps que nous ne nous sentons pas dépendre de quoi que ce soit, nous nous estimons indépendants : sophisme qui montre combien l'homme est orgueilleux et despotique. Car il admet ici qu'en toutes circonstances il remarquerait et reconnaitrait sa dépendance dès qu'il la subirait, son postulat étant qu'il vit habituellement dans l'indépendance et qu'il éprouverait aussitôt une contradiction dans ses sentiments s'il venait exceptionnellement à la perdre. » 
Ces deux critiques mettent en lumière plusieurs points importants. En premier lieu, la liberté ne peut se réduire à l'indépendance par rapport au monde extérieur ; il faut également une autonomie intérieure réelle par laquelle nous nous donnons volontairement des règles d'actions. Ainsi, alors que l'indépendance concerne les causes externes (définissant ce que je peux), l'autonomie concerne les causes qui sont la source de la volonté (définissant ce que je veux). La réflexion philosophique intériorise le problème et cherche à en trouver les conditions internes, en niant que la liberté soit dépendante en quoi que ce soit du monde extérieur. 
En second lieu, il n'est pas certain que tout lien soit contraire à l'indépendance. Être relié n'est pas toujours négatif, car l'intersubjectivité est peut-être plus fondamentale que l'indépendance du moi, dans la mesure où le moi est relation aux autres. Ainsi, pour Friedrich Nietzsche (et de même pour Hegel), le toi est antérieur au moi. Il ne semble donc pas possible de concevoir une liberté indépendance comme un état monadique, où l'individu serait une totalité fermée, atome qui n'aurait que des relations qui lui seraient extérieures ou étrangères. Les relations humaines seraient donc à la fois des sources de conflits et d'aliénation, et des conditions de liberté sociale et politique. 
Paul Valéry développe pour sa part l'idée que « la liberté est l'un de ces détestables mots qui ont plus de valeur que de sens, qui chantent plus qu'ils ne parlent, qui demandent plus qu'ils ne répondent, de ces mots qui font tous les métiers ». 


I.4. Les sens philosophiques fondamentaux du mot liberté
Pour faciliter l'exposition et la compréhension du problème philosophique de la liberté, il est commode de partir de quelques modèles fondamentaux, modèles qui sont soit des conceptions majeures, soit des moments importants de l'histoire de la pensée occidentale (cette liste n'est donc pas fermée) :

La liberté comme libre arbitre de la volonté ;
La liberté d'indifférence ;
La liberté transcendantale ;
La liberté morale ;
La liberté jaillissement ;
La liberté existentielle.



Libre arbitre: propriété de la volonté (actus proprius), faculté de choix qui associe raison et volonté. C'est l'union de la spontanéité et de l'intelligence.

Spontanéité : c'est le fait de trouver à l'intérieur de soi le principe de ses mouvements. Tous les animaux (en tant que mus par de désirs internes) sont en ce sens des êtres animés de manière spontanée.
Intelligence : par l'intelligence, faculté de choix, nous agissons par nous-mêmes en connaissance de cause ; nous avons un discernement de nos actes.

La liberté, c'est donc la spontanéité éclairée par la raison ; cette conception de la liberté n'est pas incompatible avec certaines formes de naturalisme.


Liberté d'indifférence

Selon Descartes, c'est « le plus bas degré de la liberté ».



Liberté transcendantale : c'est la faculté par laquelle l'individu peut disposer de lui-même et déterminer sa volonté en l'absence de toute contrainte physique, c'est-à-dire indépendamment de la causalité naturelle (chez Kant par exemple). Est dit libre l'homme qui se gouverne selon sa raison. Cela sous-entend que l'individu doit être en mesure de faire preuve de discernement et d'un grand sens critique : l'homme libre se donne à lui-même des normes cognitives. 
Cette liberté a deux conditions : l'indépendance et la spontanéité.

Indépendance: notre arbitre est indépendant à l'égard des contraintes des penchants de la sensibilité. Si l'homme est affecté par des penchants qui inclinent son arbitre, il peut les mettre de côté, les suspendre, pour agir d'après d'autres motifs issus de la raison. Dans ce cas, l'arbitre et la raison transcendent le monde en dépassant la sensibilité. C'est une condition fondamentale de la liberté : l'activité de la volonté met en cause la passivité de notre rapport sensible au monde.
Spontanéité de la raison : c'est la faculté de créer du nouveau, d'ouvrir des possibles : la raison permet de poser un acte non déterminé par des causes passées. Il y a alors invention et surgissement de nouveauté. Dans ce cas, la raison se donne à elle-même sa loi, elle légifère sans rien emprunter à la nature.

Si cette liberté existe, alors il y a une différence radicale entre l'homme et la nature.

Liberté morale
Liberté jaillissement
Liberté existentielle



I.5. Origine et développement du concept de liberté 
La liberté telle que nous l'entendons (comme propriété métaphysique ou comme condition transcendantale de la volonté) était ignorée des Anciens. Cela tient d'abord au fait que la volonté n'est pas pour eux une faculté à part du psychisme, et que le psychisme n'est pas lui-même une entité séparée de l'exemplifier par un cheval (mais ce point devrait sans doute être discuté d'après des thèses récentes sur l'intelligence et la sensibilité animales). 
Une conséquence importante de cette conception ancienne de l'âme, c'est que l'action, ou du moins un certain type d'actions, a, pour les Grecs, une dignité moindre ; ce que montre par exemple l'esclavage et l'artisanat. Par nature, un être qui travaille n'est pas libre (Aristote, Politiques) car son activité déforme son corps et altère en conséquence les qualités de son âme. Ce qui a de la valeur, la finalité par excellence de l'activité humaine, c'est la pensée, l'activité de l'intellect, conçue comme la finalité et le vrai bien de l'âme : la liberté de l'homme serait donc dans la contemplation qui nécessite d'ailleurs des conditions de vie d'hommes libres. Cette liberté n'est pas contraire à la nature et à sa nécessité, puisqu'elle est la réalisation parfaite de l'essence de l'homme (il ne faut donc pas confondre l'emploi qui est fait ici du mot liberté avec d'autres emplois qui sont faits ailleurs dans l'article). 
Le christianisme vient ensuite modifier cette conception, avec l'idée d'un dieu qui est volonté et qui crée, l'idée d'un dieu artisan (cf. Paul de Tarse). Cette idée de l'artisan se rencontre déjà chez Platon, mais sous une forme qui n'est pas créationniste : la théologie antique fait plutôt de Dieu un intellect non impliqué dans la création de la matière, même s'il peut y être engagé, par exemple pour y mettre de l'ordre. L'action va donc prendre de la valeur, ou changer de valeur, dans la mesure où le libre arbitre est maintenant métaphysiquement valorisé : cette valorisation a une origine morale, en particulier pour l'explication du péché. Le prix à payer de la théodicée (pour conserver la volonté juste de Dieu), c'est la malédiction de la liberté humaine, qui fait de l'homme un coupable par nature. 
Le liberum arbitrium chrétien apparaît nettement chez Augustin d'Hippone (De Libero arbitrio). Sa finalité était de fonder une théodicée ; ce concept permet en effet de disculper Dieu de la responsabilité du mal (c'est là l'invention de l'intériorisation du péché dénoncée par Friedrich Nietzsche). La motivation est donc théologique et non anthropologique. Par la suite, le libre-arbitre deviendra un trait fondamental de l'anthropologie de Thomas d'Aquin. 
On voit, par ce bref historique, que le problème de la liberté en Occident n'est pas séparable de l'histoire du concept de Dieu. Ceci est encore valable même au xxe siècle, chez Sartre par exemple (voir plus bas), lorsqu'il renverse le rapport de l'essence et de l'existence. 


I.6. Connaissance et expérience de la liberté, enjeux 
Les différentes conceptions vues ci-dessus nous font connaître plusieurs conceptions de la liberté. Mais le problème de savoir s'il y a quelque chose de tel que la liberté reste entier. Il y a un problème épistémique de la liberté, qui peut être envisagé d'un point de vue théorique et d'un point de vue pratique.

I.6.1. La connaissance théorique de la liberté
S'il y a quelque chose comme la liberté, quelle sorte de chose est-ce ? Est-ce une substance, une essence, une faculté, un acte, etc. ? Les auteurs examinés plus haut nous ont déjà fourni quelques réponses possibles. 
Comment en a-t-on connaissance ? Avoir connaissance de quelque chose comme la liberté, cela ne suppose-t-il pas en même temps d'avoir la preuve de son existence ? La liberté serait donc dans ce cas observable et devrait faire partie des phénomènes. Pourtant si la liberté se manifeste en tant que phénomène empirique, il faut bien qu'elle se conforme aux lois de la nature. Or, cela semble bien être une contradiction. Il semble que rien de tel que la liberté ne puisse être donné dans le monde ; mais il serait sans doute plus exact de conclure que la liberté, comme objet de connaissance, nous échappe, et qu'elle n'est jamais un objet de notre expérience. 
Cette difficulté peut être contournée de plusieurs manières : 

on peut nier le problème, en disant que la liberté n'existe pas. Le problème ne se pose donc pas, puisque dans cette perspective, il ne s'agit que d'un non sens métaphysique. Cette première solution implique que l'on réduise la volonté à une causalité naturelle, ou qu'on la nie ; par exemple, pour Friedrich Nietzsche, il n'y a ni volonté ni non volonté, mais notre action n'est qu'une résultante de processus physiologiques.
mais on peut chercher à la sauver en faisant un être transcendant l'expérience et une condition de cette expérience. Cette seconde solution paraît contradictoire : en faisant de la liberté un être transcendant, ne retire-t-on pas en fait à l'homme toute liberté en la situant au-delà de son expérience, bien qu'elle soit pensée comme une condition ? Elle semble inintelligible et l'on risque de ne plus savoir si l'on est libre ou non. La liberté, dans ce cas, peut faire l'objet d'une foi rationnelle, dans la mesure où nous jugeons qu'elle est une nécessité morale, et qu'on ne saurait s'en passer sans refuser du même coup toute dignité à l'homme. La liberté pourrait ainsi être comprise comme une illusion transcendantale, i. e. comme un concept de la raison que cette dernière ne peut pas ne pas penser, bien qu'aucun objet et aucune action ne viennent (et ne puissent) confirmer son existence.

Le transcendantalisme et le déterminisme semblent donc s'entendre pour retirer la liberté de l'expérience humaine.


I.6.2. L'expérience de la liberté
Les problèmes théoriques soulevés par le concept de liberté amènent à se demander si la conscience de la liberté, ou l'expérience que nous en avons, porte d'une manière certaine sur une réalité ? 
Si oui, à quelle genre de réalité a-t-on affaire ? L'expérience semble manquer de consistance pour le déterminer. En effet, si la conscience que nous avons de la liberté n'en est pas une connaissance, la liberté est soit une réalité métaphysique soit un concept vide. 
Si conscience et connaissance sont deux choses différentes, avoir conscience de quelque chose ne garantit pas son existence. Il faut donc plus que la conscience pour savoir si effectivement nous sommes libres. Ainsi, il peut sembler que non, notre expérience de la liberté ne porte pas sur une liberté, mais sur un type d'être dont la nature est hors de notre portée. 
C'est pourquoi, pour certains philosophes, vouloir prouver la liberté par des faits ou des raisonnements est une absurdité : « un homme qui n'a pas l'esprit gâté, n'a pas besoin qu'on lui prouve son franc arbitre ; car il le sent. » (Jacques Bénigne Bossuet). La « preuve » de la liberté se ramènerait donc à « l'épreuve » « d'un vif sentiment interne » (Leibniz), qui suffirait à en faire une donnée immédiate de la conscience (Bergson). 
La liberté serait donc d'abord un objet d'une intuition immédiate et interne. Mais on retombe alors dans les difficultés évoquées au début de cet article : le sentiment de la liberté, ou son intuition, n'est ni clair ni probant. L'aliéné ou l'homme ivre peuvent s'imaginer agissant de leur propre chef ; bien plus, même un homme tenu pour sain d'esprit est susceptible de se faire de graves illusions sur son propre compte. 
C'est pourquoi le problème métaphysique de la liberté tire en fait son importance des enjeux moraux qui en découlent. 


I.6.3. Enjeux métaphysiques et moraux de la liberté 
L'ensemble de cette problématique et les différentes conceptions des philosophes du passé permettent de voir plus précisément en quoi la liberté est un concept métaphysique fondamental : ses conséquences morales sont en effet considérables.

Établir la possibilité de la liberté, c'est sauver la responsabilité et la valeur de l'homme, du moins dans une perspective humaniste.
La liberté, comme condition des notions morales, donne un sens aux choix moraux en bien comme en mal : ou, autrement dit, l'essence de la liberté, c'est le devoir.
La liberté, comme obligation, soumission à une loi que l'on se donne, découle du devoir.
La liberté donne un sens à l'existence humaine : renoncer à la liberté, c'est renoncer à la qualité d'homme.

On voit bien ici en quoi une détermination métaphysique, en apparence très spéculative et difficile, peut se montrer décisive pour la vie, pour l'existence concrète. En effet, on pose ou on nie que la liberté soit un attribut essentiel : la liberté est ou non constitutive de la nature humaine. Nier la liberté, ce serait donc supprimer l'essence de l'homme. Pratiquement, la question serait de savoir si cela revient à dire que nier la liberté est une perspective dans laquelle on ne voit pas de contraintes morales qui empêchent quiconque de nier aussi l'humanité d'un autre homme. « Tout est permis » dit Nietzsche, assumant cette négation anti-humaniste de l'essence de l'homme. Mais les doctrines de ce genre ont-elles nécessairement ces conséquences ? Nier la liberté, cela implique-t-il qu'il ne soit pas interdit de nier, opprimer, torturer ou détruire l'autre ? Si, en effet, la liberté implique l'existence du devoir comme sa condition, sa suppression entraînerait peut-être la suppression d'une distinction entre le bien et le mal : 

Renoncer à sa liberté, c'est renoncer à sa qualité d'homme, aux droits de l'humanité, même à ses devoirs. Il n'y a nul dédommagement possible pour quiconque renonce à tout. Une telle renonciation est incompatible avec la nature de l'homme; et c'est ôter toute moralité à ses actions que d'ôter toute liberté à sa volonté. (Rousseau, Le Contrat social).





II. Sociologie de la liberté
Sommes-nous plus libres sans les autres ? Comment penser la liberté par rapport aux libertés ? La liberté pour tous est-elle une véritable liberté ? La réalisation de la liberté, sa pratique politique, crée de nombreuses tensions. 
L'autonomie politique est incarnée par la figure du citoyen, qui abandonne son indépendance naturelle pour se soumettre volontairement à des lois qui sont, au moins idéalement, les mêmes pour tous (Hobbes, Rousseau). C'est à cette condition que, selon cette théorie, les hommes peuvent être libres ensemble. Mais les lois peuvent être ressenties comme une aliénation de leur liberté par les individus. 

II.1. Le paradigme du « bon sauvage »
Il existe cependant un point de vue opposé à cette vision de l'éducation comme moyen de la liberté. Ainsi au xviiie siècle, Jean-Jacques Rousseau défendait un paradigme du bon sauvage, considérant l'éducation comme une domestication de l'homme, et la société comme un carcan. Ce point de vue, qui sera développé par Sigmund Freud dans son essai Malaise dans la civilisation (1929), a été discuté dès la Révolution française. Un ouvrage comme Sa Majesté des mouches de William Golding suggère au contraire que l'homme privé des contraintes sociales n'en devient pas nécessairement meilleur. 


II.2. Libertés individuelles 
On distingue au niveau de l'individu plusieurs « types » de libertés :

La liberté naturelle : l'homme a le droit naturel d'employer ses facultés comme il l'entend ;
La liberté civile : elle s'inscrit dans le cadre d'un homme citoyen étant libre de ses actes, tant que ceux-ci ne nuisent pas à autrui et ne sont contraires à aucune Loi. Cette liberté est très délicate d'application, en particulier en ce qui concerne le droit de la concurrence, puisque toute création de commerce nuit par principe aux commerces antérieurs existant dans le voisinage. On y associe souvent la maxime suivante : « La liberté des uns s'arrête là où commence celle des autres » ;
La liberté individuelle : dans la même optique que la précédente, elle reconnaît à l'homme le droit d'aller et venir librement sur le territoire national, ce qui inclut la possibilité d'y entrer ou d'en sortir. Cette liberté a été étendue en Europe grâce aux accords de Schengen, permettant la libre circulation des personnes dans l'espace de la Communauté Européenne ; 
La liberté de culte ainsi que la liberté de conscience : la liberté de culte permet à chaque individu de pratiquer la religion de son choix, la liberté de conscience permet de ne pas avoir de croyance religieuse. La déclaration des droits de l'homme et du citoyen en fixe pour limite : l'absence de trouble à l'ordre public. 
La liberté d'opinion consiste en la liberté de pensée associée à la liberté d'expression : elle permet à chacun de penser et d'exprimer ses pensées sans censure préalable, mais non sans sanctions, si cette liberté porte préjudice à quelqu'un. Elle va de pair avec la liberté de la presse, qui est celle d'un propriétaire de journal de dire ce qu'il veut dans son journal. 
La liberté économique : elle permet à chacun de percevoir des revenus de son travail et de pouvoir affecter ces derniers librement : liberté de travailler et de consommer. Nul ne peut se voir refuser par principe un emploi pour des considérations autres que de qualification professionnelle (par exemple sexe, origine ethnique, âge ou religion). 



II.3. Libertés collectives
La liberté n'est pas qu'individuelle, elle existe aussi à un niveau global, plus collectif, avec par exemple la liberté de la presse, qui permet une libre publication, sans subir de censure. 
Le mouvement ouvrier au xixe siècle distingue liberté formelle et liberté réelle. La notion de liberté collective repose en partie sur cette distinction.[réf. nécessaire] 
Les différentes libertés collectives :

la liberté de la presse : elle permet à chacun de publier librement ses pensées ou ses opinions, sans être sujet à la censure ou à tout autre mesure arbitraire ou autoritaire ;
la liberté de réunion : elle permet aux individus de se réunir librement pour débattre de leurs opinions ;
la liberté syndicale : elle permet aux salariés de former et d'adhérer ou non à des organisations syndicales pour les représenter et faire valoir leurs droits et revendications.

Paradoxalement, la notion de liberté peut parfois à ce niveau, nuire à la liberté de l'individu. Comme dans le cas de la liberté de la presse, par exemple. Ainsi les moyens techniques et financiers importants nécessaires aujourd'hui aux organes d'information, en particulier radiophoniques ou audiovisuels, tendent à la formation de cartels pratiquant l'autocensure, réduisant par là même le pouvoir de contrôle et de critique de l'individu sur ces vecteurs d'information... L'abolition de la censure n'est donc plus un gage de liberté, car si les publications ne sont plus soumises à des décisions arbitraires, les vecteurs de l'information deviennent en revanche de moins en moins accessibles à la grande masse des individus, réduisant de ce fait leur capacité à exprimer leurs opinions, ainsi que la variété des points de vue exposés. C'est pour cette raison que la presse est considérée comme le quatrième pouvoir (à l'instar des pouvoirs exécutif, législatif et judiciaire). 


II.4. Liberté et technologies de l'information et de la communication
D'après leurs détracteurs, les brevets logiciels entraîneraient la formation de trusts surpuissants juridiquement qui seraient peu compatibles avec les libertés individuelles. 
Des mouvements tels que l'Open Source ou Logiciel libre tendent à favoriser l'accès à la connaissance et aux techniques de manière universelle, ce qui, à l'heure de la mondialisation, représente un élément pour la défense des libertés individuelles dans les pays émergents, les rendant techniquement indépendants des pays « déjà développés ». 
L'accès à l'internet pose de nombreuses questions éthiques concernant, entre autres, les libertés individuelles, mais aussi collectives. Il n'est pas souhaitable, notamment pour une entreprise qui souhaite protéger son capital intellectuel, de divulguer des informations sur le réseau internet mondial. Il est nécessaire d'établir des règles, et différents niveaux d'accès et de confidentialité pour les partenaires et les parties prenantes. 



<TXT=w-Linux7>

I. Histoire

I.1. Autour de la naissance de Linux
En 1991, les compatibles PC dominent le marché des ordinateurs personnels et fonctionnent généralement sous les systèmes d'exploitation MS-DOS, Windows et OS/2. Le microprocesseur Intel 80386, vendu depuis 1986, commence à être abordable. En 1991, aucun de ces trois systèmes n'exploite correctement les capacités 32 bits et de gestion mémoire du 80386. 
Le projet GNU est connu pour avoir produit de nombreux logiciels libres, dont des commandes Unix, l'éditeur de texte Emacs et le compilateur C GCC. Ces logiciels sont généralement utilisés sur des stations de travail fonctionnant sous UNIX propriétaire, car le noyau de système d'exploitation Hurd n'est qu'à l'état de projet. 
En juin 1991, la Berkeley Software Distribution (BSD) sort la Networking Release 2 (Net/2), qui constitue un système UNIX BSD libre presque complet. Mais un procès lancé par Unix System Laboratories contre Berkeley Software Design fait peser des doutes sur le statut de cette distribution pendant presque deux ans. 
Le système d'exploitation Minix est développé par le professeur Andrew Tanenbaum pour l'enseignement. Il est inspiré de UNIX, gratuit, ses sources sont disponibles mais non libres, et la simplicité est privilégiée par rapport aux performances. 


I.2. Naissance du noyau Linux
En 1991, l'étudiant finlandais Linus Torvalds, que la faible disponibilité du serveur Unix de l'université de Helsinki indispose, entreprend d'écrire un noyau de système d'exploitation qu'on appellera plus tard « noyau Linux ». 
Linus Torvalds fait alors son apprentissage sur le système d'exploitation Minix. Comme l'auteur de Minix refuse d'intégrer les contributions visant à améliorer Minix, Linus décide d'écrire un remplaçant de Minix. Il commence par développer un simple émulateur de terminal, qu'il utilise pour se connecter via modem au serveur informatique de son université. Linus désire alors surtout comprendre le fonctionnement de son ordinateur, un compatible PC basé sur un microprocesseur Intel 80386. Après l'ajout de diverses fonctionnalités dont un système de fichiers compatible avec celui de Minix, Linus oriente son projet vers quelque chose de plus ambitieux : un noyau aux normes POSIX. 
Le 5 octobre 1991, il annonce sur le forum Usenet news:comp.os.minix la disponibilité d'une ébauche version 0.02 de son système d'exploitation, la version 0.01 ayant eu une diffusion plus que confidentielle. Le message en question ainsi que sa traduction sont disponibles sur Wikisource. 
Depuis, des centaines de passionnés et des entreprises, petites ou géantes, sont venus participer au projet, dont Linus Torvalds est toujours le coordonateur. Eric S. Raymond décrit dans un essai retentissant le modèle de développement du noyau Linux et d'une partie des logiciels libres. 
Initialement appelé Freax par son créateur, le projet trouve son nom définitif grâce à Ari Lemmke, administrateur du serveur FTP ftp.funet.fi, qui héberge le travail de Linus Torvalds dans un répertoire nommé Linux. C'est la première apparition d'un terme composé à partir de Linus et UNIX, qui deviendra par la suite une marque déposée au nom de Linus Torvalds. Le manchot Tux, dessiné par Larry Ewing en 1996, devient le symbole du projet. 


I.3. Diffusion de Linux
Parmi les étapes marquantes, on peut d'abord citer le lancement en octobre 1996 par Matthias Ettrich de l'environnement graphique KDE puis en août 1997 par Miguel de Icaza de son concurrent GNOME, les deux étant basés sur le système de fenêtrage X11 issu des travaux du Massachusetts Institute of Technology. Dans l'iceberg qu'est un système d'exploitation grand public basé sur le noyau Linux, les environnements de bureau, comme GNOME, KDE ou encore XFCE en forment la partie émergée, en contact direct avec l'utilisateur. 
Il y a également la prise en compte progressive de l'intérêt commercial de Linux dont on peut citer quelques manifestations spectaculaires : le lancement en février 1998 de l'Open Source Initiative ; l'annonce en juillet 1998 du support d'Oracle Corporation qui porte et supporte sa célèbre base de données sous GNU/Linux ; l'entrée en bourse de Red Hat le 11 novembre 1999 ; celle de VA Linux le mois suivant qui marque le sommet d'une impressionnante bulle spéculative ; le support massif apporté par le géant IBM qui y dépense son 1er milliard en 2001 4., emploie en 2005 près de 300 développeurs du noyau Linux, et organise à partir de 2003 la riposte légale lors de l'attaque du SCO Group qui affirmait posséder des droits d'auteurs sur le noyau Linux (voir l'article SCO contre Linux) ; l'acquisition en octobre et novembre 2003 de Ximian puis de SuSE par l'entreprise américaine Novell. 
C'est dans le monde des serveurs informatiques que GNU/Linux a eu le plus d'impact, notamment avec le très populaire LAMP. Sur les serveurs, GNU/Linux a souvent été utilisé pour remplacer d'autres Unix et se retrouve être le seul acteur majeur avec Windows. 
Dans les systèmes embarqués, GNU/Linux est fréquemment utilisé avec les outils uClibc et BusyBox qui ont été développés pour le matériel particulièrement limité en capacité mémoire. En outre, le fait de pouvoir compiler le noyau Linux avec des options spécialement adaptées au matériel cible donne aux développeurs de nombreuses opportunités d'optimisation. 



II. Principes et idéologie

II.1. Logiciel libre
La principale originalité de GNU/Linux par rapport à d'autres systèmes d'exploitation concurrents - comme Mac OS, Microsoft Windows et Solaris - est d'être constitué d'un noyau libre et de logiciels libres. 
Un logiciel libre n'est pas nécessairement un logiciel gratuit, et inversement tout logiciel non-commercial n'est pas forcément libre. Ce ne sont pas non plus des logiciels libres de droits : c'est en vertu de leurs droits d'auteurs que les contributeurs d'un logiciel libre accordent les quatre libertés, qui sont d'utiliser le logiciel sans restriction, d'étudier le logiciel, de le modifier pour l'adapter à ses besoins et de le redistribuer sous certaines conditions précises. 
Certaines licences sont basées sur le principe de copyleft, c'est-à-dire de réciprocité : une oeuvre dérivée d'un logiciel sous copyleft doit à son tour être libre. C'est le cas de la licence libre la plus utilisée, à commencer par le noyau Linux lui-même : la licence GNU GPL écrite par Richard Stallman. 
L'ouverture du code source, l'un des quatre critères correspondant à la notion de logiciel libre, a des avantages théorisés entre autres par Eric Raymond en matière de correction rapide des bogues qui sont la plaie de l'informatique, et notamment la correction des failles de sécurité. C'est le refus du principe de sécurité par l'obscurité. 


II.2. Interopérabilité
Linux n'aurait pu se développer sans la présence de protocoles standardisés utilisés sur Internet. Un bon nombre de logiciels libres sont d'ailleurs des implémentations de référence, comme Apache. 
Les partisans des logiciels libres sont donc des partisans constants de l'interopérabilité. Ils mettent en avant les formats ouverts, des formats de données dont les spécifications techniques sont publiques et sans restriction d'accès ni de mise en oeuvre, afin de ne pas dépendre d'un seul logiciel. 
Citons dans cette optique Mozilla Firefox qui tente de respecter scrupuleusement les recommandations émises par le World Wide Web Consortium, Jabber qui a donné naissance au standard XMPP reconnu par l'Internet Engineering Task Force dans le domaine de la messagerie instantanée ou encore les suites OpenOffice.org et KOffice qui ont lancé le récent standard OpenDocument dans le domaine de la bureautique. 
Dans d'autres domaines, il n'existe pas d'organisme ou d'accord de standardisation reconnu. Le marché est alors morcelé entre divers vendeurs qui ont chacun leur technologie ou sous la domination d'un acteur économique prédominant qui ferme ses formats ou protocoles. 
Le premier cas de figure prévaut dans la guerre des messageries instantanées et est réglé par des logiciels multiprotocoles comme Pidgin ou Kopete. Les formats des suites Microsoft Office successives et le protocole Common Internet File System qui permet de partager fichiers et imprimantes entre différents ordinateurs d'un réseau Microsoft Windows tombent dans la deuxième catégorie. Ces formats et protocoles sont souvent pas ou mal documentés. L'interopérabilité passe alors nécessairement par la rétro-ingénierie. 
Cela peut nécessiter un travail titanesque, pouvant être par ailleurs illégal aux États-Unis mais légal en Europe (tant qu'on reste dans le cadre de l'interopérabilité) ; aujourd'hui, OpenOffice.org permet de lire la très grande majorité des fichiers aux différents formats .doc, et le logiciel Samba permet de participer aux réseaux Windows. 
Plus problématiques du point de vue des logiciels libres sont les formats et protocoles nécessaires à l'interopérabilité, mais verrouillés techniquement et/ou légalement : gestion des droits numériques, brevets logiciels, Directive EUCD, Digital Millennium Copyright Act... 
Unifix Linux 2.0 de la société allemande Unifix (et Linux-FT de Lasermoon) sont également certifiés POSIX.1 FIPS 151-26,7 (Federal Information Processing Standard). Noyau 1.2.139 
Sur le site Debian, ils expliquent « les normes de POSIX ne sont pas gratuites et la certification POSIX.1 (et FIPS 151-2) est très chère » 


II.3. Communautés
De nombreuses associations, connues sous le nom de Linux Users Group, Groupe d'Utilisateurs Linux (LUG ou GUL), cherchent à promouvoir GNU/Linux et par extension, les logiciels libres, par le biais de rencontres où des démonstrations de GNU/Linux sont faites, des formations, et pour ceux qui le souhaitent des installations sur leur ordinateur. 
De nombreuses communautés existent sur Internet afin d'aider les débutants comme les professionnels. Citons le site lea-linux, le site d'informations collaboratif Linuxfr.org et le site Linux-Québec, qui aide les utilisateurs québécois comme français dans leur apprentissage des bases de GNU/Linux grâce à un réseau IRC très actif. Et les projets Proselux et Parrains.Linux permettent aux linuxien(ne)s de se rencontrer pour s'entraider. De même il existe de nombreux sites regroupant des Tutoriels ainsi que des HowTos, comme lea-linux ou encore LinuxTuto. 



III. Distributions
Les logiciels libres sont produits de manière collaborative, souvent indépendamment les uns des autres, et peuvent être librement redistribués. Il s'ensuit une particularité du monde GNU/Linux : la séparation fréquente entre ceux qui produisent les logiciels et ceux qui les distribuent. 
On appelle distribution Linux une solution prête à être installée par l'utilisateur final comprenant un noyau Linux, des programmes d'installation et d'administration de l'ordinateur, un mécanisme facilitant l'installation et la mise à jour des logiciels comme RPM ou APT ainsi qu'une sélection de logiciels produits par d'autres. 
Une distribution peut par exemple choisir de se spécialiser (ou non) sur GNOME ou KDE. Elle est également responsable de la configuration par défaut du système (graphisme, simplicité...), du suivi de sécurité (installations de mise à jour) et plus généralement de l'intégration de l'ensemble. 
La diversité des distributions permet de répondre à des besoins divers qu'elles soient à but commercial ou non ; orientée serveur, bureautique ou embarqué ; orientée grand-public ou public averti ; généraliste ou spécialisée pour un usage spécifique (pare-feu, routeur réseau, grappe de calcul...) ; ou encore certifiées sur un matériel donné. 
Parmi les plus célèbres distributions, on peut citer Slackware, apparue en 1993, qui est aujourd'hui la plus ancienne distribution encore en activité, toujours maintenue par Patrick J. Volkerding ; Debian, éditée par une communauté de développeurs ; Red Hat, éditée par l'entreprise américaine du même nom qui participe également au développement de Fedora Core ; ou encore SuSE, à l'origine une traduction allemande de Slackware, qui a depuis évoluée en intégrant certains sous-système issus de Redhat. 
De nombreuses autres distributions plus ou moins spécialisées existent, étant pour la plupart dérivées des projets sus-cités. Par exemple voici quelques distributions spécialisées « environnement de bureau » : Ubuntu, éditée par Canonical Ltd qui est dérivée de Debian ; Mepis également basée sur Debian ; Zenwalk dérivée de Slackware ; Mandriva, dérivée de Red Hat, aujourd'hui éditée par la société française de même nom et impliquée dans plusieurs projets libres. Il existe également des distributions dites LiveCD, dont l'une des plus célèbres est Knoppix, qui offrent la possibilité de démarrer un système d'exploitation Linux complet et d'accéder à de nombreux logiciels à partir du support (CD ou DVD) sans installation préalable sur le disque dur, et sans altérer son contenu. Cette souplesse d'utilisation a fait qu'elles sont devenues un support très populaire de démonstration d'utilisation de Linux, et sont même utilisées comme outils de maintenance système. 


IV. Contrats OEM et détaxe Windows
Un des enjeux qui se posent pour les distributions Linux est de nouer des partenariats avec des fabricants d'ordinateurs afin qu'il devienne facile de trouver un ordinateur préinstallé sous Linux. Car même si certaines distributions affirment avoir rendu l'installation d'un système Linux aussi simple que celle des systèmes d'exploitation concurrents, le simple fait d'avoir à être au courant qu'une alternative existe, d'être prêt à accepter des changements dans ses habitudes et d'avoir à installer soi-même le système constitue un désavantage indéniable par rapport à la situation privilégiée dont jouissent les distributeurs de Microsoft Windows et de Mac OS X. Le système de Microsoft est en effet omniprésent et Apple est en même temps le fabricant des Macintosh. 
À défaut, les usagers de Linux réclament de pouvoir être remboursés, lors de l'achat d'un ordinateur neuf, de la part du prix correspondant au système d'exploitation et aux logiciels qu'ils n'ont pas l'intention d'utiliser, comme la loi de certains pays le permet. Si la société Apple s'est montrée plusieurs fois coopérative face à de telles demandes, le remboursement de Microsoft Windows est en général long et difficile bien qu'actuellement une série de jugements a permis à certains consommateurs de se faire rembourser par les fabriquants. Devant la difficulté d'obtenir ce remboursement basé sur le CLUF, dès 1998, les associations Linuxfrench et AFUL ainsi que Roberto Di Cosmo ont lancé en réaction une action pour la détaxe Windows. 
Cette situation existe en Europe et en Amérique du Nord, mais pas dans certains pays d'Amérique du Sud où les distributions de Linux ont plus de parts de marché que Windows. 


V. Part de marché
D'après l'entreprise IDC spécialisée dans les études de marchés, 24 % des serveurs et 3 % des PCs étaient vendus avec Linux en 2004. IDC prévoit que le marché total des ordinateurs Linux sera de 35,7 milliards de dollars en 2008.15 Ces chiffres de ventes ne comptabilisent évidemment pas les entreprises et les particuliers qui choisissent d'installer eux-mêmes Linux après l'achat d'un matériel fourni sans Linux. 
Une étude de XiTi réalisée régulièrement sur les systèmes utilisés par leurs visiteurs de 19 000 sites web professionnels, donne, en février 2009, 93.82 % de part de marché à Windows (62,18 % à Windows XP et 28,90% à Windows Vista), 4,59 % à Mac OS X et 1,24 % à Linux.16. 
Part de Linux :

janvier 2007 : 0.7 % 
août 2007 : 0.8 % 
décembre 2007 : 0.9 % 
avril 2008 : 1.0 % 
août 2008 : 1.2 % 
janvier 2009 : 1.2 %

Il existe d'autres approches et d'autres sources. Le fabricant de cartes graphiques canadien ATI, largement minoritaire sur le marché Linux en raison du manque de support 3D de ses cartes (il développe pourtant ses propres pilotes pour Linux), sur ce système d'exploitation, estime que Linux représente 3 % de ses ventes. Suite à son rachat par le fondeur AMD, ATI a ouvert les spécifications de ses cartes début 2008 afin que les développeurs de Mesa 3D puissent mieux intégrer la gestion de ses cartes. 
Les tableaux statistiques de w3schools donnent 2,2 % de parts de marché pour Linux en mars 2002 et 4.0 % de parts de marché en février 2009.17. 
Il est important de préciser s'il s'agit de la part de marché des postes client.


VI. Interfaces

VI.1. La ligne de commande
De par la filiation avec UNIX, la ligne de commande est toujours disponible dans Linux. 
Certaines distributions, notamment celles spécialisées dans les serveurs ou certaines tâches d'administration, utilisent uniquement la ligne de commande, en particulier pour sa faible consommation de ressources, due à l'absence d'interface graphique, mais surtout sa puissance d'action, liée à l'interopérabilité des commandes et la possibilité de générer des scripts. 
Pendant longtemps, de nombreuses opérations de configuration nécessitaient son utilisation, ce qui n'est plus vrai avec les distributions récentes dédiées à l'utilisation familiale. 
Les aides en ligne mentionnent cependant souvent la démarche à suivre en ligne de commande, même lorsqu'une configuration graphique est possible : cette méthode est plus universelle dans le monde Linux, et souvent plus facile à expliquer pour la personne qui aide, et son interlocuteur n'a qu'à copier-coller l'indication. 
Une interface graphique bien conçue permet de nos jours d'accomplir la grande majorité des tâches bien plus agréablement, mais ce n'est pas toujours le cas, particulièrement lorsque la tâche a un aspect répétitif ou non prévu. 
La ligne de commande, qui tire sa puissance de sa possibilité de combiner à l'infini des sous-tâches automatiques, et qui permet presque naturellement d'automatiser la tâche ainsi accomplie, peut alors se révéler plus efficace que l'interface graphique. 
Scientifiques, ingénieurs et développeurs comptent parmi ses plus fréquents utilisateurs. 
Interface graphique et ligne de commande peuvent aussi se compléter l'une et l'autre : KDE est livré avec un terminal très ergonomique, et offre un mécanisme efficace (DCOP) pour piloter et donc automatiser toutes ses applications graphiques depuis la ligne de commande. 
Apple très réputé pour ses interfaces graphiques, MacOS étant le premier système commercialisé avec la gestion des fenêtre et de la souris, a également intégré un terminal en ligne de commandes compatible UNIX sur MacOS X. 


VI.2. Gestionnaires X Window
L'emploi du terme générique Linux est trompeur s'agissant de l'utilisation d'un ordinateur personnel. Il existe en réalité trois interfaces distinctes, aux caractéristiques bien différentes et formant chacune un tout autonome : l'approche traditionnelle centrée autour d'un gestionnaire de fenêtres d'une part, l'environnement KDE et l'environnement GNOME d'autre part. 
Cependant, comme toutes ces interfaces sont basées sur X Window, leurs applications peuvent cohabiter et elles offrent des points communs dont l'affichage de fenêtres à distance (y compris via des protocoles compressés et chiffrés comme ssh et nox) et le copier-coller simplifié : un texte sélectionné par la souris est automatiquement copié, un clic milieu (ou un clic molette, ou sur les 2 boutons en même temps) suffit alors pour coller le texte à l'endroit désiré. Il n'y a donc jamais besoin du clavier pour effectuer un copier/coller sous X. 
Traditionnellement l'interface d'un système d'exploitation basé sur le noyau Linux était une interface sobre voire spartiate, centrée autour d'un gestionnaire de fenêtres (il en existe de nombreux comme Window Maker ou IceWM) et d'une suite assez hétéroclite d'applications. 
La fenêtre xterm permettant une utilisation en ligne de commande n'est en général jamais loin, l'informaticien appréciant ses puissantes possibilités d'utilisation qui proviennent de la filiation de GNU/Linux avec UNIX. 
L'inconvénient d'un tel système est le temps nécessaire à personnaliser un tel environnement, et surtout la non-standardisation des applications ainsi utilisées. Les applications que l'on peut voir sur la copie d'écran de droite (XMMS, RealPlayer, Mozilla Firefox, xterm, gaim, konqueror) suivent chacune leurs propres conventions : aspect, comportements, raccourcis claviers différents ; les copier-coller et glisser-déposer sont aléatoires... 
Si individuellement des applications comme vim ou emacs peuvent effectivement avoir des aspects brillants, l'ensemble disparate de toutes ces applications en fait un système difficile à appréhender. Le temps consacré à apprendre une application et les réflexes ainsi acquis ne peuvent être appliqués aux autres applications, un avantage énorme qu'apporte la standardisation de comportement des interfaces comme l'avait montré le Macintosh. À titre d'exemple, le raccourci clavier utilisé pour quitter une application peut être : Ctrl + Q ou Ctrl + C - Ctrl + X ou Ctrl + C ou juste q ou Esc ou encore :qa! ou bye ou quit ou exit... 
L'utilisation d'un tel environnement régresse nettement ces dernières années avec la maturité des alternatives présentées ci-dessous. Elle perdure néanmoins chez des utilisateurs qui se sont faits à un tel système, ou qui l'apprécient car il leur permet d'utiliser un Linux récent même sur des ordinateurs anciens. 


VI.3. Les environnements de bureau
L'état des lieux du précédent chapitre est décrit dans un manifeste daté de 1996 ayant poussé Matthias Ettrich à fonder en réaction le projet KDE, puis Miguel de Icaza à fonder le projet GNOME l'année suivante, qui s'inspirent de Mac OS et de Windows sur le plan de l'ergonomie logicielle et de la standardisation des comportements. 
Ces deux projets sont devenus les fédérateurs de Linux sur le poste de travail. 
Chacun offre en effet :

aux programmeurs, un environnement de programmation très productif ainsi que des recommandations d'interfaces (en anglais : guidelines) permettant de produire plus vite des applications plus simples à utiliser ; 
aux traducteurs, une infrastructure. Ces deux environnements et leur myriade de logiciels sont traduits en plusieurs dizaines de langues19 ; 
aux artistes, des espaces de travail20 pour exercer leurs talents ; 
aux spécialistes d'ergonomie, la possibilité de le rendre plus simple et cohérent21 ; 
aux applications externes, un environnement de référence dans lequel s'intégrer22 ; 
et par conséquent, à l'utilisateur, un environnement complet, intégré et homogène ainsi qu'une suite d'applications essentielles : explorateur de fichiers, navigateur web, lecteur multimédia, client de messagerie, carnet d'adresses, lecteur PDF, gestionnaire d'images. 

Ces deux environnements de bureau ont atteint récemment une maturité certaine, citons l'année 2003 pour KDE, un peu plus tard pour GNOME. Très actifs, ces deux projets ont néanmoins l'intention de s'améliorer nettement pour leurs prochaines versions majeures ; les efforts dans ce sens sont concentrés au sein des projets Appeal pour KDE, et ToPaZ pour GNOME. 
Techniquement, ils reposent tous deux sur de nombreuses technologies communes, au premier rang desquelles le système de fenêtrage X11. Pour éviter de dupliquer certains efforts, une zone informelle de collaboration entre ces projets du nom de Freedesktop a été mise en place. 
C'est dans l'approche de l'ergonomie (celle-ci étant relative au type d'utilisateur) et dans la conception du rôle d'un environnement du bureau qu'ils diffèrent : l'environnement KDE pousse loin la volonté d'intégration entre les applications, possède de très nombreuses fonctionnalités avancées et joue la carte de la configuration tout en veillant à avoir des bons choix par défaut ; GNOME se veut plus épuré et se consacre sur les tâches essentielles (reprenant la philosophie making things just work). Chacun plaît, par conséquent, à un public différent. 
On peut noter également la montée en puissance d'un troisième environnement de bureau appelé XFCE, qui vise à fournir un environnement complet basé sur GTK+ comme GNOME, tout en restant plus léger que ce dernier ou KDE. 


VII. Offre en logiciels
La communauté du Libre a produit un grand nombre de logiciels utilisables dans de nombreux domaines. 
Des exemples de logiciels donnés à titre indicatif : 

la bureautique avec OpenOffice.org, 
Internet avec Mozilla Firefox, Konqueror, IceWeasel, Gnuzilla, Mozilla Thunderbird, Pidgin ou BitTorrent, 
le multimédia avec Xine, MPlayer, VLC media player, XMMS ou Amarok, 
le graphisme, avec GIMP, Inkscape ou Scribus, 
la 3D avec Blender.

La plupart des distributions Linux proposent un programme permettant de naviguer dans une liste de milliers de logiciels libres testés et préconfigurés spécialement pour une distribution. Ces programmes libres sont alors téléchargés et installés en un clic de souris, avec un système de signature électronique garantissant que personne ne leur a ajouté de virus ou de spyware. 
Certains logiciels propriétaires importants ont également une version Linux. C'est le cas de Opera, Macromedia Flash Player, Acrobat Reader, NeroLinux ou Skype par exemple. 
La notion de portabilité désigne la capacité d'un programme à être utilisé sous différents systèmes d'exploitation ou architectures. 
Enfin, il est possible d'utiliser des logiciels faits pour Microsoft Windows sur un poste Linux grâce à une implémentation de l'API Windows sous Linux comme WINE. Des offres commerciales basées sur WINE comme CrossOver Office permettent d'utiliser presque sans problèmes des logiciels tels Microsoft Office et Adobe Photoshop issus du monde Windows. 

VII.1. Jeux vidéo
Il existe de nombreux jeux disponibles sous Linux, gratuits ou payants, libres ou propriétaires. L'offre comporte aussi bien des petits jeux de bureautique (cartes, démineur, échecs, golf) que des jeux commerciaux récents (Enemy Territory: Quake Wars) 
Certains jeux sont conçus pour tourner nativement sous Linux (Quake 3 par exemple), et d'autres peuvent être lancés à l'aide de programmes implémentant l'API Windows sous Linux. Il en existe plusieurs implémentations, dont certaines spécialement pour les jeux, permettant ainsi de faire fonctionner de nombreux jeux conçus pour Windows, dans des environnements comme Cedega et WINE (ex.World of Warcraft). Le dernier recours des joueurs linuxiens consiste tout simplement à utiliser parallèlement Windows sur le même ordinateur grâce au multiboot ou à la virtualisation. 


VII.2. Programmes shell
Les programmes les plus connus en mode texte accessibles depuis la ligne de commande comprennent vim, emacs, sed, apt... Une certaine partie d'entre eux peut aussi s'utiliser par l'intermédiaire d'une interface graphique. 
Par ailleurs, les programmes fonctionnant en mode console sont relativement nombreux. Les raisons sont multiples :

Historique (à l'origine, Linux était dépourvu d'environnement graphique). 
Efficacité (les programmes qui n'utilisent pas l'environnement graphique demandent moins de ressources). 
Rapidité (ouvrir une console pour y taper une commande est souvent bien moins long que de passer par les divers menus d'un gestionnaire de fenêtres, ou d'un environnement graphique) 
Meilleur contrôle. 
etc.

L'utilisation de ces programmes peut s'avérer difficile pour une personne n'étant pas habituée à travailler en mode texte, des personnes venant de Windows par exemple. D'un autre côté, ils sont relativement prisés par les utilisateurs avancés des systèmes de type UNIX. 


VII.3. Bibliothèques libres
Les logiciels qui utilisent une bibliothèque libre peuvent fonctionner sur Linux et sur toutes les plates-formes où la bibliothèque est implantée. Ces bibliothèques peuvent ajouter une surcouche graphique sur des applications texte déjà existantes comme c'est le cas de Vim, mais elles servent surtout à développer des logiciels accessibles aux non-informaticiens et disposant des fonctionnalités autorisées par les interfaces graphiques, comme le glisser-déposer, les manipulations à la souris, etc. 
D'autres applications comme Blender ou Google Earth sont un cas à part car ils utilisent la bibliothèque OpenGL destinée à la base à l'implémentation ainsi qu'à la gestion de programmes utilisant la 3D (mais aussi la 2D). 


VII.4. Émulation
Plusieurs logiciels d'émulation existent permettant de simuler le fonctionnement de systèmes d'exploitation concurrents ou des environnements de jeu.

VII.4.1. Émulation d'ordinateurs
Les programmes Steem et ARAnyM émulent une bonne partie des applications écrites pour les machines Atari, notamment les Atari ST et Atari TT, UAE (Unix Amiga Emulator) permet d'émuler le Commodore Amiga, Basilik les anciens Mac 68000 d'Apple. Tous ces émulateurs émulent les microprocesseurs de la famille 68000 de Motorola qui équipaient ces ordinateurs, ainsi que les coprocesseurs spécialisés de l'Amiga. 
MESS (souvent associé a MAME) permet d'émuler de la même façon un grand nombre de micro-ordinateurs 8bits. Il existe également des émulateurs spécialisés pour chacun de ces micro-ordinateurs. Euphoric pour les Oric, FMSX pour les MSX, mais aussi des émulateurs Spectrum, Commodore, etc. 


VII.4.2. Utilisation d'application pour Microsoft Windows
Des applications développées pour Windows peuvent tourner sous un système Linux via les applications Wine et son dérivé commercial Cedega qui réimplémente le fonctionnement des principales API de Microsoft Windows. Le microprocesseur n'est pas émulé, seul les fonctions des APS sont remappées à la volée sur les API utilisées nativement dans Linux. Par exemple : DirectX utilise OpenGL, la gestion de l'impression est relayée à CUPS ou LPR, des périphériques USB à libusb, les tablettes graphiques à XInput, etc. Cela permet dans de nombreux cas des performances proche de l'execution native, tout en évitant les problèmes de certains pilotes de périphériques inhérent à Windows. Dans certains cas spécifiques, les performances de certaines applications peuvent se trouver dégradées. De nombreux utilitaires, applications de tous domaines et jeux tournent parfaitement, mais pas tous. Le site de Wine référence les applications fonctionnant et celles posant problèmes. 


VII.4.3. Virtualisation
En outre, Linux ouvre également la possibilité d'obtenir une parfaite séparation entre plusieurs environnements virtuels tournant sur un seul ordinateur physique, en prenant en compte les modules de virtualisation présents dans les processeurs récents comme AMD-V sur AMD et Intel-VT (ou IVT) sur Intel. Ces environnements de virtualisation permettent d'exécuter des environnements différents ou plusieurs environnements similaires sur une même machine, tout en assurant une certaine sécurité dans la séparation des accès. Ce système est utilisé depuis longtemps par les mainframes d'IBM. IBM a d'ailleurs porté Linux sur celles-ci afin de permettre à ses clients de continuer à les utiliser avec un système plus moderne. 
Virtualbox est plutôt orienté poste de travail, permettant de faire tourner un système Windows (par exemple) dans une fenêtre et ainsi de garder la stabilité du système Linux, tout en utilisant certaines applications disponibles sur ces systèmes. Cela permet de migrer l'environnement de travail en douceur et sans problèmes. Le système hôte Linux n'est pas affecté par le système virtuel. Il reste donc utilisable, même en cas de problèmes que pourrait rencontrer le système virtuel. 
Xen et VMware sont quant à eux plus orientés serveur, ils donneront de meilleurs performances concernant l'exécution de machines virtuelles pour délivrer des services. 
Il en existe également d'autres, comme Qemu ou encore Bochs qui lui émule aussi le processeur, rendant le système invité beaucoup plus lent. 




VIII. Prise en charge du matériel
La prise en charge de l'équipement matériel est l'une des critiques principales faites à Linux. En effet, tous les matériels pour micro-ordinateurs ne sont pas forcément pris en charge directement par Linux et les pilotes développés par les constructeurs et compatibles avec Linux ne sont pas toujours disponibles. Certains fabricants fournissent systématiquement des pilotes pour Microsoft Windows et Mac OS X, alors que sous Linux, la communauté est souvent obligée de les développer elle-même, souvent par rétro-ingénierie. Parfois, la communauté préfère développer des pilotes libres stables bien que des pilotes propriétaires développés par les constructeurs existent (c'est le cas pour les cartes graphiques ATI ou NVidia). Dans cette optique, les pilotes nécessaires pour faire fonctionner pleinement un ordinateur sont intégrés à la plupart des distributions Linux. Ce sont les périphériques de second niveau qui risquent dans certaines circonstances de ne pas avoir de pilotes disponibles, notamment certaines imprimantes, modems, webcams, etc. Cependant les utilisateurs de Windows ou MacOS sont parfois également confrontés à des problèmes de pilotes lorsqu'ils installent une nouvelle version de leur système et l'absence de code source empêche une recompilation des pilotes propriétaires. Enfin, il arrive qu'il n'y ait des pilotes que pour Linux, et pas pour Windows ou Mac (supercalculateurs, serveurs internet haut de gamme, consoles de jeu PlayStation, anciens périphériques dont le support à été arrêté par les constructeurs...). Le matériel ancien peut être généralement recyclé sous Linux, car la pérennité des pilotes libres est également l'un des points forts de Linux. 
La première raison de cette situation est le faible impact de Linux chez les particuliers, ce qui n'incite pas les fabricants à investir dans le développement de pilotes pour cet environnement. La seconde raison est le refus de certaines distributions (Fedora ou Debian, par exemple) d'embarquer des pilotes sous licences propriétaires, même quand ceux-ci existent, ce qui oblige l'utilisateur à les trouver et à les installer manuellement. Enfin, l'absence d'une API fixe dans le noyau Linux oblige les fabricants à délivrer des binaires des pilotes adaptés à chaque version du noyau[réf. nécessaire]. 
Les utilisateurs qui travaillent sur plusieurs plates-formes et qui ont besoin de ces pilotes peuvent trouver des versions développées par de tierces parties, mais de tels pilotes ne supportent généralement qu'un ensemble rudimentaire de fonctions, et n'apparaissent qu'après la sortie du matériel, avec un certain temps de latence. Il existe cependant des mécanismes pour faire fonctionner certains pilotes développés pour d'autres systèmes d'exploitation (comme NdisWrapper). 
Les webcams sont, par exemple, particulièrement concernées par cette absence de pilotes, mais le protocole USB video device class ou UVC permet de répondre à ce problème avec de nombreuses webcams supportant ce protocole 29. Aujourd'hui de plus en plus de grands constructeurs font des efforts pour développer ou fournir les informations pour le développement de pilotes libres pour Linux, comme Creative Labs pour ses webcams ou cartes sons 30, Intel (processeurs, chipsets 3D, cartes réseau, etc.) ou des assembleurs (l'Américain Dell 31 et le Taïwanais Asus, poussé par Intel 32, ainsi que les Chinois Lineo ou Everex vendent par exemple des ordinateurs avec Linux préinstallé, mais de nombreux autres composants nécessitent de vérifier la disponibilité de pilotes avant l'achat, s'ils sont destinés à une utilisation sous Linux. 
Aujourd'hui, Intel a une véritable stratégie pour s'imposer sur le marché des Ultra-Mobile PC en proposant aux constructeurs une plateforme construite autour de GNU/Linux (projet Moblin), c'est le cas récent de la machine EeePC d'Asus et de plusieurs machines du même créneau. 


IX. Utilisation

IX.1. Serveur
En raison de la parenté de GNU/Linux avec UNIX, GNU/Linux s'est imposé sur le marché des serveurs informatiques très rapidement. Un point crucial a été la possibilité d'utiliser un système d'exploitation de type UNIX sur du matériel compatible PC, beaucoup moins cher que les solutions à base d'UNIX propriétaire et de matériel spécifique. De nombreux logiciels serveurs très demandés et très utilisés (serveur HTTP, base de données, Groupware, serveur de messagerie électronique...) sont disponibles gratuitement, en général sans aucune limitation, et fiables, la part de marché de Linux dans ce domaine a en conséquence crû rapidement. 
GNU/Linux ayant une réputation de stabilité et d'efficacité dans la maintenance, il remplit les exigences posées à tout système d'exploitation pour serveurs. De plus, la modularité d'un système basé sur le noyau Linux permet l'exploitation de serveurs dédiés à une tâche particulière. Le portage du noyau Linux sur de nombreux composants hardwares fait que Linux est aujourd'hui utilisable sur toutes les architectures utilisées dans ce domaine. Le matériel utilisable est en conséquence considérable. Les derniers IBM eServer p5 et IBM eServer i5 sont par exemple supportés par IBM avec un système d'exploitation Linux et permettent d'y exécuter plusieurs systèmes Linux en parallèle. 
La part de marché des serveurs Linux s'établit en 2004 à environ 10 % avec une forte croissance annuelle de 50 % [Information de l'article allemand ; sources à trouver]. Il est utilisé dans à peu près tous les domaines. Un des exemples les plus connus est résumé par l'acronyme LAMP, où Linux propulse un serveur web Apache associé à la base de données MySQL et au langage de programmation PHP (alternativement : Perl ou Python). Linux est également souvent utilisé comme serveur de fichiers, le plus souvent dans les réseaux Windows grâce au serveur Samba, moins souvent sous NFS ou AppleShare. 


IX.2. Sécurité réseau
Linux, qui jouit d'une bonne réputation en matière de sécurité et de performance (passage à l'échelle) est très utilisé dans le domaine des réseaux informatiques, par exemple comme passerelle, comme routeur ou comme pare-feu.


IX.3. Ordinateur central
La disponibilité du code source, et la possibilité qui en découle d'adapter le système à une tâche précise, a permis à Linux de faire son entrée dans les centres de calculs. Sur ce marché des ordinateurs centraux, gros ordinateurs ultra-fiables optimisés pour le traitement massif de données, omniprésents dans les banques, les sociétés d'assurances et les grandes entreprises, Linux fait de plus en plus concurrence aux systèmes UNIX propriétaires qui étaient autrefois la norme.


IX.4. Grappes de serveurs
Linux a été très tôt utilisé dans le domaine des grappes de serveurs (en anglais : clusters), par exemple par le moteur de recherche Google dès le milieu des années 1990. Dans cette configuration, associée à la notion de grille de calcul, de simples ordinateurs tournant sous une distribution spécialisée de Linux travaillent indépendamment au sein d'un grand réseau d'ordinateurs.


IX.5. Superordinateurs
Les superordinateurs sont conçus pour atteindre les plus hautes performances possibles avec les technologies connues, en particulier en termes de vitesse de calcul. En novembre 2006, selon TOP33 Linux fait tourner 74% des cinq cents plus puissants ordinateurs du monde (contre 20 % pour UNIX) dont les plus puissants, les deux serveurs Blue Gene d'IBM (40 960 et 131 072 processeurs). En novembre 2007, c'est plus de 85 % des superordinateurs qui utilisent Linux, contre 6% pour UNIX et 1,20 % pour Windows. En novembre 2008 c'est 87.8 % des superordinateurs qui se trouvent sous Linux contre 4.60 % pour UNIX. http://www.top.org/stats/list/32/osfam 


IX.6. Embarqué
Linux se trouve aussi au coeur de nombreux appareils informatiques ou d'électronique grand public, et parfois sans que l'usager le sache. Il s'agit notamment d'équipement réseau et de petits appareils numériques destinés à la consommation de masse, équipés en général d'un processeur spécialisé économe en énergie et d'une mémoire flash. 
Le succès de Linux dans ce domaine tient, ici comme ailleurs, à ce que les fabricants apprécient de pouvoir d'une part adapter le logiciel à leurs besoins (consommation, interface, fonctions annexes, etc.), d'autre part de bénéficier de l'expérience et du travail d'une communauté active. Linux est aussi apprécié dans ce domaine pour sa fiabilité, sa résistance aux attaques des pirates informatiques sur les réseaux et bien sûr sa gratuité. 
Des forums de coopération spécialisés aident les fabricants de ces produits en mettant à disposition instructions, programmes et exemples de codes, et en s'efforçant de standardiser les interfaces de programmations de Linux dans l'embarqué. L'OSDL a lancé le 17 octobre 2005 la Mobile Linux Initiative pour accélérer la progression de Linux dans ce domaine. 


Réseaux et communication
Linux fait tourner plusieurs routeurs dont certains modèles de Linksys, ainsi que divers terminaux fournis par des fournisseurs d'accès Internet (comme la Freebox en France). 


Téléphones et assistants personnels
Linux se retrouve également sur une gamme de téléphones portables ("Linux phones" : Motorola, etc.), sur l'assistant personnel Sharp Zaurus et les tablettes Internet Nokia 770, Nokia N800 et Nokia N810. Dans le domaine des assistants de navigation personnels, les systèmes GPS autonomes de TomTom sont conçus à partir d'une plate-forme Linux. 


Multimédia
Linux est utilisé dans des lecteurs de salon DivX, des téléviseurs et des décodeurs TNT, sur des baladeurs audio comme ceux de SanDisk et sur les baladeurs multimédias d'Archos. 


Console de jeu
La GP2X de GamePark, console de jeux vidéo portable 
La PlayStation 3 de Sony utilise un système d'exploitation avec le noyau Linux développé spécialement pour la machine. 
La Pandora (console), console de jeu "open source" qui a un système d'exploitation Linux compilé pour processeurs ARM (r).





X. Linux et la sécurité
Les raisons pour lesquelles Linux est réputé avoir une bonne sécurité informatique sont diverses et dépendent également du domaine d'utilisation. 
Ainsi, sur le poste de travail, Linux bénéficie d'une stricte séparation des privilèges, ce qui dans la pratique n'est souvent pas utilisé avec des systèmes concurrents. Une des conséquences est qu'un ver ou virus informatique ne peut accéder qu'à une partie des ressources et fonctionnalités d'un système Linux, mais ni aux données importantes du système, ni aux données d'éventuels autres utilisateurs. 
Par comparaison avec d'autres systèmes grand-public, Linux, et avant lui UNIX, s'est propagé d'abord parmi des gens possédant un solide bagage technique et sensibles aux problèmes de sécurité informatique d'autant plus que les premières vulnérabilités informatiques sont apparues sous UNIX (1972) ainsi que les premiers virus et vers. Le développement de Linux s'est, par conséquent, déroulé dans un contexte où la sécurité était une question critique, comme en témoigne le nombre de logiciels de qualité dans ce domaine qui sont libres et originaires du monde Linux/UNIX. 
Dans le domaine des serveurs, le degré de sécurité dépend, par comparaison, avant tout du degré d'expérience qu'a l'administrateur système. Là, Linux marque des points grâce à sa liberté d'utilisation, qui permet sans risque et sans surcoût de tester abondamment divers scénarios sur d'autres ordinateurs, et d'y acquérir ainsi une expérience utile. 
Il existe une série de distributions spécifiquement axées sur la sécurité, et des initiatives telles que SELinux de la National Security Agency pour atteindre des niveaux de protection toujours plus hauts. Mais aussi, une série de distribution axée sur l'anti-sécurité, comme Damn Vulnerable Linux, pour sensibiliser les experts et les aspirants, aux problématiques de sécurité sur ce système d'exploitation. 
Un autre argument avancé est la variété des plates-formes matérielles supportées, ainsi que les solutions logicielles. Une faille de sécurité touchant le plus populaire client email ne touchera qu'une fraction des linuxiens ; par contraste, une faille touchant Outlook Express peut toucher d'un coup une proportion énorme des utilisateurs de Windows. Cette thèse est développée dans un rapport écrit par des sommités du domaine comme Bruce Schneier pour le compte de la CCIA et reprise par la société Gartner dans un document. Une partie est traduisible ainsi : 

« La plupart des ordinateurs tournent sous Microsoft(TM), et, par conséquent, la plupart des ordinateurs du monde sont vulnérables aux mêmes virus et aux mêmes vers au même moment. Le seul moyen d'éviter cela est d'éviter la monoculture logicielle dans le domaine des systèmes d'exploitation pour les mêmes raisons raisonnables et évidentes pour lesquelles on évite la monoculture en matière d'agriculture. Microsoft exacerbe ce problème via une panoplie de pratiques visant à verrouiller ses utilisateurs à sa plate-forme. L'impact sur la sécurité de ce verrouillage est réel et représente une menace pour la société. »

Enfin, le fait que Linux et nombre de logiciels tournant sous Linux soient des logiciels libres permet que son code source soit étudié d'un oeil critique par quiconque désirant le faire, que ce soit pour effectuer des adaptations, dans un cadre éducatif, pour répondre aux intérêts privés d'une entreprise/institution ou par simple intérêt personnel (pour en exploiter les vulnérabilités par exemple). En relation avec cela, on entend souvent l'argument que les failles de sécurité sont corrigées plus rapidement, affirmation approuvée et réfutée par diverses études, en fonction généralement de leur source de financement. Enfin, la liberté des logiciels rend inutile le recours au piratage des logiciels, aux cracks ou autres sites warez très populaires parmi les adeptes des autres systèmes d'exploitation, et qui constituent un vecteur d'infection des ordinateurs. 
Reste que Linux n'est pas totalement insensible aux problèmes de sécurité, comme l'a montré le ver Slapper en septembre 2002, premier du genre à toucher un nombre notable d'ordinateurs sous Linux, avant tout des serveurs web tournant sous Apache (6 000 à l'apogée du ver). 
De plus Linux reste un système d'exploitation vulnérable comme tous les autres, ainsi près de 4 900 vulnérabilités ont été recensées entre 2003 et 2008, celles-ci sont réparties sur les différentes distributions disponibles 37. Celles-ci ont été, pour la plupart, corrigées assez rapidement, tandis que d'autres subsistent. 

X.1. Gestion numérique des droits
La gestion numérique des droits (DRM) concerne le domaine du multimédia, et notamment la musique et les vidéos qui peuvent être achetées sur Internet. Certaines oeuvres sont protégées par des verrous numériques, visant à contrôler l'utilisation de l'oeuvre, par exemple en limitant le nombre d'écoutes ou de copies possibles. Ces DRM nécessitent l'emploi d'une technologie particulière, qui est la propriété exclusive du fabricant et vendeur desdits DRM, ce qui explique que la lecture d'une oeuvre protégée se trouve liée à l'utilisation d'un programme spécifique. Les deux plus grand fabricants de systèmes de gestion des droits numériques, Microsoft et Apple, conditionnent l'usage des oeuvres protégées par leurs systèmes à l'utilisation respective de Windows Media Player, et de iTunes. Ces sociétés vendant leur propre système d'exploitation, elles ne souhaitent pas proposer de version de leurs programmes pour Linux. Ainsi, il n'est souvent pas possible pour les utilisateurs de Linux d'acheter en ligne de la musique sur un site de téléchargement payant, ou d'écouter de la musique déjà achetée et téléchargée. 
Il existe aussi des DRM sur les CD audio, mais ceux-ci sont beaucoup moins standardisés et moins courants. La plupart sont conçus pour fonctionner avec les systèmes d'exploitation de Microsoft et sont donc susceptibles d'être totalement inefficace pour un utilisateur de Linux. 
Il ne s'agit pas de limitations techniques, puisque des systèmes de gestion libres existent 38. Voir aussi Linus Torvalds, selon lequel Linux et la gestion des droits ne sont pas incompatibles.39 


X.2. Critiques
Brad Spengler développeur chez grsecurity accuse Linux de parfois centrer ses efforts sur les fonctionnalités au détriment de la sécurité. Il prétend que Linus Torvalds lui aurait dit ne pas être intéressé par l'ajout d'options de sécurité utiles pour éviter des débordements de tampon, car cela ralentirait le chargement des applications. 
Il reproche l'absence d'une personne chargée officiellement de la sécurité, avec qui il serait possible de communiquer en privé en toute sécurité. À la place la seule solution est d'envoyer un e-mail sur une liste de diffusion relative aux questions de sécurité où les failles découvertes sont parfois utilisées à des fins malveillantes avant qu'une mise à jour de sécurité ne soit diffusée, alors que les usagers de Linux ne sont pas au courant de l'existence de cette faille. 
Enfin il remet en cause l'implantation du système LSM depuis la version 2.6 du noyau qui aurait été implanté par laxisme et qui faciliterait l'insertion de rootkits invisibles au sein du système en les faisant passer pour des modules de sécurité, mais cela est devenu impossible depuis la version 2.6.2442. D'autres développeurs du noyau reprochent à ce système de consommer des ressources non négligeables et de permettre le détournement de la licence GPL du noyau en y ajoutant des composantes propriétaires. 



<TXT=w-Mouvementrastafari8>

I. Racines du mouvement
La religion chrétienne est extrêmement présente en Jamaïque (plus de 80% de la population), notamment avec les églises anglicane, méthodiste, baptiste, catholique romaine, l'Église de Dieu et, depuis les années 1970, l'Église éthiopienne orthodoxe. 
L'évangile (gospel) est chanté avec ferveur le dimanche dans toute l'île. Face à l'émancipation de la mentalité esclavagiste, puis du colonialisme, se sont créés, au début du xxe siècle, différents mouvements « éthiopianistes » où l'interprétation occidentale de la Bible est parfois remise en cause. 
Les traditions des cultes africains interdits par les maîtres ayant survécu sous forme d'Obeah (sorte de vaudou local illégal et redouté), du Kumina, et mélangées à la Bible, de la Pocomania ou 


II. Fondements du mouvement moderne
Lorsque le Jamaïcain Marcus Garvey émigre à Harlem, où il devient un des premiers meneurs importants de la cause noire, il fait souvent allusion à l'Éthiopie dans ses discours. Il écrit ainsi dans son principal ouvrage Philosophy & Opinions : 

« Laissons le Dieu d'Isaac et le Dieu de Jacob exister pour la race qui croit au Dieu d'Isaac et de Jacob. Nous, les Noirs, croyons au Dieu d'Éthiopie, le Dieu éternel, Dieu le Fils, Dieu le Saint-Esprit, le Dieu de tous les âges. C'est le Dieu auquel nous croyons, et nous l'adorerons à travers les lunettes de l'Éthiopie. »

Marcus Garvey est pour beaucoup le premier prophète noir du mouvement rastafarien. Il annonce la fin des souffrances du peuple noir et son retour aux racines : l'Afrique. 
En 1924, le révérend James Morris Webb prononce un discours cité par le quotidien conservateur Daily Gleaner : « Regardez vers l'Afrique, où un roi noir sera couronné, qui mènera le peuple noir à sa délivrance ». 
La presse coloniale dénonce alors cette doctrine éthiopianiste « vulgaire » qu'ils attribuent à Garvey. Mais le 2 novembre 1930, en Éthiopie, Tafari Makonnen, le Ras Tafari, est coiffé de la couronne sacrée du negusä nägäst (roi des rois) sous le nom de Haïlé Sélassié Ier (« Puissance de la Trinité »). Il est le chef d'une des premières nations officiellement chrétiennes de l'histoire, l'Abyssinie. Selon le livre sacré Gloire des Rois (Kebra Nagast), retraçant l'histoire de son antique dynastie, Sélassié serait le descendant direct du Roi Salomon et de la Reine Makeda de Saba. 
Des représentants prestigieux des pays occidentaux assistent au sacre très médiatisé de Sélassié, qui est perçu par une communauté d'agriculteurs éthiopianistes de Sligoville (Jamaïque), le Pinacle, dirigé par Leonard Percival Howell (véritable fondateur du mouvement Rastafari), comme étant l'accomplissement de la prophétie attribuée à Garvey. 
En effet, le « Roi des Rois, Seigneur des Seigneurs » (1° Timothée 6:15) de la Bible ressemble beaucoup aux titres traditionnels millénaires de Sa Majesté Impériale Haïlé Sélassié Ier : « Empereur d'Éthiopie, Roi des Rois, Seigneur des Seigneurs, Lion Conquérant de la Tribu de Juda, élu de Dieu, Lumière de l'Univers ». Puisant à la fois dans le marxisme, le christianisme, la culture africaine et plus tard l'hindouisme, Howell considère Sélassié (ou « Jah », de Jéhovah) comme le messie et propose dès lors une interprétation afrocentriste de la Bible. 
Cultivant le chanvre, considéré comme un sacrement (fumé dans les chalices) et le diffusant dans l'île, il est arrêté pour sédition en 1933, puis il est interné à l'asile à plusieurs reprises, alors que le Pinacle est détruit maintes fois par la police. Différents mouvements éthiopianistes de libération, comme le mouvement Bobo de Prince Emmanuel, se développent parallèlement en Jamaïque. Ils prennent pourtant peu à peu un nom générique, Rastafari, et visent, en partie, à restituer à l'homme noir le rôle important qu'il a joué dans la civilisation, à commencer par la Bible, où les ancêtres Juifs de Sélassié seraient naturellement, comme lui, Noirs : Moïse, Jésus, etc. 
Progressivement, et selon le voeu de Jésus et des Naziréens (Nombres 6-5), beaucoup de Rastafariens ne se coupent ni la barbe ni les cheveux, (lien) une coiffure souvent comparée à la crinière du Lion de Juda sacré. Des « locks » (noeuds, boucles) ou « dread (épouvante) locks » se forment ensuite naturellement dans leurs cheveux crépus. 
Ce signe de reconnaissance deviendra une mode internationale à partir de 1976. Proches de la terre, généralement les Rastas ne boivent pas d'alcool, le vin étant proscrit (Nombres 6-3), ne touchent pas aux morts (beaucoup de Rastas ne font même jamais allusion à la mort, mais au contraire « chantent la vie »), sauf ceux de leur proche famille (Lévitique 21-1), et le corps humain est considéré comme l'église (Corinthiens 3-16, 17), rejetant ainsi le principe même des temples ou des églises. 
Désireux de se maintenir en bonne santé, ils suivent en principe un régime spécial qu'ils appellent "I-tal" (vital) (Génèse 1:29 et 9:4), qui se compose de riz, de fruits, de racines, de graines et de légumes. Ce régime exclut toute nourriture non biologique. 
Quant au nom "Rasta", il provient de celui, divin, de Sélassié : le Ras (tête, correspond étymologiquement et protocolairement à son titre de duc) Tafari (son prénom). Leurs couleurs sont celles de l'Éthiopie impériale (rouge, or et vert, couleurs de l'Afrique frappées du Lion de Juda). 
Dès lors, les Rastas, incompris, blasphématoires, fumeurs de chanvre (la ganja, « l'herbe de la sagesse » qui aurait poussé sur la tombe de Salomon) deviennent des parias maltraités. En 1954, le Pinacle est rasé, et ils s'installent à Kingston, à Back-o-Wall. Le nom de ce ghetto provient de sa situation géographique : il est attenant au mur d'un cimetière, et nombre de Jamaïcains craignent de s'y installer par peur des « duppy » (fantômes). 


III. Haïlé Sélassié
Suite à la prophétie annonçant le couronnement d'un roi en Afrique, l'avènement au pouvoir du monarque Haïlé Sélassié, sous le titre biblique de « Roi des rois, Seigneur des seigneurs, Lion conquérant de la tribu de Juda, Lumière du Monde » est apparu pour les rastas comme la révélation d'un envoyé de Jah, qui les mènerait à la libération de leurs souffrances. Ainsi, il est communément affirmé qu'Haïlé Sélassié, à l'image de Jésus, est Jah incarné, Homme et Dieu. 
Cette croyance est très importante dans la philosophie rasta, bien que souvent difficilement acceptée, y compris parmi les gens proches du mouvement. Ainsi l'artiste-producteur Yabby You, bien que très mystique, a-t-il toujours refusé cette divinité. La légende raconte qu'il tire son surnom de Jesus Dread du fait qu'il demandait aux chanteurs travaillant pour lui de mentionner Jésus au lieu de Selassié dans leur paroles... 
Haïlé Sélassié lui-même n'a jamais reconnu le culte rasta envers sa personne, bien qu'il ait montré sa reconnaissance envers les rasta en effectuant des donations de terre en Éthiopie, puis en effectuant un voyage mémorable en Jamaïque en 1966. Cette terre se nomme Shashamane : Haile Sélassié offre cette terre dans les années 50 à tous les membres de la diaspora noire qui désireront rentrer en Afrique par le biais de l'Ethiopian World Federation (EWF) dont il est le fondateur. Ce fut un acte pour remercier les Noirs américains et caribéens présents lors de son couronnement à Addis-Abeba et qui essayèrent de sensibiliser l'opinion au sort de l'Éthiopie après l'invasion des troupes italiennes dans le pays. Ce terrain serait ainsi devenu pour certains Rastas le symbole du rapatriement en Afrique. 
Ainsi, aux dignitaires rastas rencontrés lors de sa visite en Jamaïque, répondant au désir de ceux-ci de retourner en Afrique, a-t-il fait la proposition suivante : « Ne rentrez en Afrique que lorsque vous aurez libéré tous les Jamaïcains oppressés dans leur pays. » 
Enfin, la vie et la mort d'Haïlé Sélassié possèdent une dimension symbolique forte, en particulier dans sa mort et les péripéties qui ont suivi. Pour les rastas, Hailé Sélassié n'a pas disparu (Jah Live de Bob Marley). Voir sa page pour plus de détails sur la mort de Sélassié et ses différentes sépultures. 

III.1. Visite d'Haïlé Sélassié
Haïlé Sélassié fait une visite officielle en Jamaïque en avril 1966. 
A son arrivée, des milliers de Rastas lui réservent, à sa surprise, un impressionnant accueil. Le mouvement prendra par la suite encore plus d'ampleur, bien que Sélassié, bienveillant avec les Rastas, ne prétende lui-même jamais être le dieu vivant. 
Cette visite a eu une forte répercussion sur l'importance et la popularité du mouvement Rasta. En effet, les autorités n'ont pas été en mesure de sécuriser la foule lors de l'arrivée de l'avion officiel sur le sol Jamaïcain. Celle-ci était tellement importante et excitée à l'idée de voir enfin le Roi des Rois, qu'il a fallu chercher un médiateur pour la canaliser. Celui-ci sera incarné par Mortimer Planno, très connu à l'époque pour ses enseignements Rasta, qui toucheront beaucoup Bob Marley entre autres. Ainsi, Mortimer Planno sera dorénavant présent à chaque sortie publique d'Haïlé Sélassié durant ce voyage. 
Il va sans dire qu'une telle chose n'était absolument pas prévue par le protocole, et a consisté en une manifestation importante de la présence des Rastas. 
D'autre part, cette visite a été pour beaucoup de Jamaïcains l'occasion de se confronter aux différentes croyances véhiculées par le mouvement, et de s'en faire sa propre idée. Ainsi, lors de cette visite, Rita Marley, en observant la main d'Haïlé Sélassié, est persuadée d'y avoir vu les stigmates du Christ. Bob Marley devint rasta cette même année 1966. De retour en Éthiopie Haïlé Sélassié Ier s'adresse à ses confidents en ces termes : « Il y a un gros problème en Jamaïque...» En effet le roi d'Éthiopie n'a jamais reconnu le culte rasta envers sa personne. Ce qui est interprété par de nombreux rastas (et avec cet humour qui leur est propre) comme la manifestation d'une dignité toute divine. À l'occasion de ce voyage Selassié s'assit autour d'une table avec 32 rastas représentant chacun une communauté. La discussion est centrée sur le thème du retour en Afrique. Sélassié leur offrira à cette occasion une terre éthiopienne, shashamany, jusqu'alors réservé aux Falashas (juifs éthiopiens). Mais seuls quelques rastas (principalement de la communauté des Twelwes Tribes Of Israël) reviendront aux pays de leurs ancêtres. 



IV. Propagation du mouvement après la fin des années 1960
Back-o-Wall est rasé le 12 juillet 1966 avec violence. De plus en plus de musiciens de rocksteady puis de reggae, jusque-là généralement proches de la soul américaine et des églises, transmettent le message rebelle rasta avec leurs chansons. 
Le style des trois tambours nyahbinghi joué lors des cérémonies rastas (grounations) se répand (Bob Marley en tirera une chanson, Selassie Is The Chapel). À partir de 1970, un courant rasta majoritaire traverse le reggae. Bob Marley fait découvrir au monde cette culture qui met en valeur l'histoire d'Afrique, méconnue malgré son extraordinaire richesse. Les Rastas commencent alors à obtenir le respect dans leur pays malgré une répression utilisant la prohibition de la détention de chanvre, punie de bagne malgré une pratique répandue dans toute la population de l'île. 
D'autre part, l'industrie musicale s'ouvre enfin au message Rasta dans la production de chansons Conscious aux paroles ouvertes au message des Rastas. Ainsi, jusqu'alors méprisé par les producteurs et distributeurs de l'île, le message Rasta commence, après qu'un certain nombre de rastas, dont certains expulsés de Back-o-Wall se sont installés dans les ghettos de Kingston, comme Trenchtown, et après la visite d'Haïlé Sélassié, à se faire sentir auprès de la population déshéritée de l'île. 
Alors qu'auparavant, les producteurs, à l'instar de Duke Reid, les refusaient catégoriquement, certains, comme Clement Seymour Dodd, dit Coxsone, ouvrent leur production aux compositions comportant un message spirituel et engagé, contrairement aux chansons d'amour qui prévalaient durant l'époque du rocksteady. Son studio, Studio One se met alors à produire des groupes et artistes aux paroles inspirées du message Rasta, comme The Gladiators, The Abyssinians, ou encore Dennis Brown et bien d'autres encore. Le fait que Coxsone ait été un des seuls à tolérer la consommation de chanvre dans son studio n'est certainement pas étranger à la présence à Studio One de ces groupes initiateurs du reggae roots. 


V. Évolutions récentes
Si les Rastas perdent de l'influence chez les jeunes Jamaïcains après la disparition de Marley en 1981, ils restent très présents et font un retour massif, unanime, dans le reggae à partir de 1994 avec Garnett Silk, Buju Banton, Tony Rebel, Mutabaruka, Sizzla, etc. De nombreuses et différentes tendances rasta cohabitent en Jamaïque et sont parfois contradictoires. Les Bobo Ashanti, les Emmanuelites, les Ites, notamment, ainsi que des courants chrétiens plus traditionnels. 
Les positions des individus se réclamant rastas vont du racisme le plus primaire issu de la lutte contre l'esclavage et le colonialisme, ou d'un ethnocentrisme noiriste militant, garveyiste à outrance, parfois teinté de racisme, jusqu'à une philosophie universaliste profonde, où la recherche de sa propre identité, de son acceptation, de la tolérance et de la nature humaine rejoint les philosophies et ascèses orientales. 
L'organisation des Douze Tribus d'Israël tente de fédérer les Rastafariens, mais sans réel succès. En 1997, un parti d'obédience Rasta cherche même à se présenter aux élections. 
Pacifiques mais fiers, affichant généralement une certaine arrogance, les Rastas dénoncent la société païenne (les personnes sans conscience de l'aspect sprirituel de la vie et de la nature en général), Babylone, et répandent leur culture dans le monde entier. 
La foi rasta permet avant tout à beaucoup de Jamaïcains pauvres de retrouver une dignité et un sens à leur vie difficile, en restant détachés de l'identité coloniale et ancrés dans leurs racines africaines. L'idée universelle de base étant d'« être soi-même » et de « se connaître ». 
La culture et les préceptes Rasta tendent à se cristalliser en une nouvelle religion organisée, qui serait ainsi la plus importante née au vingtième siècle. Pour de nombreux Rastas, cette tendance est une dérive. 


VI. Croyances et culture rasta
La culture rasta est un tout formé par l'agrégation d'un certain nombre de croyances, de coutumes et de traditions. Il est ainsi vain de proposer une caractérisation exhaustive et universelle de la culture rasta. Celle-ci est au contraire basée sur la différence et se revendique comme une unité dans la diversité. 
Cependant, il existe des points de repères caractérisant les croyances rasta, principalement le port des dread locks, la consommation de ganja, et les habitudes alimentaires, bien que ces caractéristiques ne soient pas adoptées par tous. Contrairement aux idées reçues, le Reggae n'est pas en soi une marque caractéristique des croyances rasta, mais bien un vecteur servant le message, selon le concept ancestral très courant dans ces cultures : la transmission orale. Le genre musical le plus proche des rastas est plutôt le Nyabinghi. Enfin, une grande partie de la culture rasta est directement inspirée de la Bible, comme le concept de Babylone. 

VI.1. L'influence biblique
Les rastas respectent la version de la bible acceptée par les anglicans (King James Bible), mais remettent en question certains passages, considérant que celle ci a été réécrite à l'avantage des blancs. Ils utilisent donc la Holy Piby, version de la bible réécrite au début du XXe siècle par Robert Aathlyi Rogers, dont le but est de prouver que le Christ ainsi que l'ensemble des enfants d'Israël sont noirs. 
Les fondements de la culture rasta se trouvent dans la Bible. En effet, rasta est une spiritualité revendiquant son attache aux fondements de la Bible, Ancien et Nouveau Testaments. Les rastas se reconnaissent dans la Bible et s'en inspirent constamment. Ainsi, la coutume veut que la première occupation d'un rasta au lever soit la lecture d'un chapitre de la Bible, selon l'adage : « A chapter a day keeps the devil away », soit : un chapitre par jour tient le diable éloigné. 
Certains passages de la Bible sont très importants dans les croyances rasta. Ainsi, le deuxième exode à Babylone, et la première destruction du temple de Jérusalem est pour les rastas l'incarnation de leur exil d'Afrique, esclaves des Babyloniens modernes que furent les colons britanniques. Ainsi s'explique le concept de Babylone, qui est la métaphore de l'exploitation des Juifs par les Babyloniens. Puis, par extension, le concept va s'étendre à tous les aspects qu'ils rejettent dans la société importée par les colons, comme le matérialisme, l'argent, le capitalisme, la police... Ici aussi, les limites du concept sont assez floues et peuvent varier d'un rasta à un autre... 
Toujours en s'inspirant de la Bible (Jérémie 51), les rastas pensent souvent que la civilisation occidentale a perdu les valeurs fondamentales (la nature, le respect, l'amour de l'autre...) au profit d'une société basée sur l'argent, la réussite personnelle et de plus en plus éloignée de la nature. Ainsi, de la même façon que Dieu avait détruit la cité de Babylone qui avait péché par excès d'orgueil, les rastas prophétisent la chute du système (« shitstem ») de Babylone. 
Les textes de la Bible sont le fondement des croyances rasta, comme celui de Rivers of Babylon, psaume 137. 
Cependant ils pensent que la Bible ne représente que la moitié de leur histoire : « Half the story has never been told ». L'autre moitié résiderait dans le coeur de chacun. 


VI.2. Le voeu de Nazarite, et le port des dreadlocks
Un très bon exemple de l'influence Biblique est le voeu de Nazarite. Les Rasta, pour expliquer leur mode de vie, se réfèrent souvent au voeu de Nazarite, comme présenté dans la Bible (Nombres 6:1-21). Ce voeu, à caractère temporaire, sanctifie la personne le suivant pour une certaine période durant laquelle cette personne devra suivre certaines règles de vie. Ces règles sont pour la plupart celles auxquelles se réfèrent les Rasta dans leur mode de vie. Elles sont, pour les plus caractéristiques : 

ne pas se couper, ni se coiffer les cheveux, ce qui entraîne l'apparition de dreadlocks ;
ne pas consommer de viande ;
ne pas consommer de produit de la vigne.

Enfin, ce voeu est censé revêtir un caractère temporaire, et le texte des Nombres précise ensuite quand et comment le voeu doit s'achever. En particulier, un Nazarite ne devra pas croiser un homme mort, sous peine de devoir rompre son voeu. On retrouve cette idée dans un certain nombre de chansons, illustrée par cette phrase : « rasta don't go to no funeral », soit « le rasta n'assiste à aucune funéraille ». D'une manière générale, la mort constitue un tabou pour les rastas, et ils n'abordent ce thème que d'une façon très spirituelle et assez difficile à appréhender pour le non-initié. 
L'application stricte de ce voeu au mode de vie Rasta n'est pas sans porter à discussion. Avant tout, ce texte et les modalités d'applications du voeu de Nazarite, comme pour beaucoup de textes de l'Ancien Testament, pose la question du décalage temporaire et culturel. En effet il n'y a qu'à consulter les démarches à effectuer pour rompre le voeu pour comprendre qu'il ne saurait s'appliquer identiquement de nos jours. Ensuite, ce voeu est bien censé être temporaire (sept ans), alors que le mode de vie Rasta lui devrait pouvoir se pratiquer toute sa vie durant. 
Ainsi, un autre point caractéristique des Nazarites est le port des dreads, port qui est source de beaucoup de polémiques. Le débat de savoir si les dreads sont nécessaires à un Rasta est encore important de nos jours. Ainsi, certains Rastas pensent qu'un Rasta sans dreads n'en est pas un, d'autres, comme les membres des Twelve Tribes of Israël ou les Morgan Heritage (notamment avec le titre Don't haffy dread to be rastaman) pensent au contraire que Rasta est avant tout une philosophie de vie et qu'il est tout à fait possible d'être un Rasta sans porter de dreads, tandis que beaucoup de dreadlocks ne sont pas forcément le signe d'un Rasta. 
Enfin, il faut rappeler que le port des dreads est une mode qui s'est instaurée dans les ghettos de Kingston, par une génération de rastas apparue après la destruction du Pinacle. Le port des dreads n'était pas initialement la marque des adeptes de rasta, qui étaient alors les barbus car ils se laissaient pousser la barbe. Ainsi la réponse à la nécessité du port des dreads doit être trouvée par chacun ; mais de nombreux rastas pensent que cette coiffure ne codifie plus l'appartenance à leur mouvement. 


VI.3. Concepts et pensées
Il n'existe aucune doctrine rasta écrite, ni même de synthèse générale. Les concepts de la spiritualité rasta sont plutôt variés et de tradition orale. Le Kebra Negast qui retrace l'histoire de la dynastie salomonide éthiopienne, jusqu'au Négus (Hailé Selassié), est un ouvrage très considéré par les éthiopiens amhara, et de référence pour les rasta, qui se considèrent éthiopiens. Un grand nombre des concepts de la philosophie rasta (paix et amour) ont directement inspiré les artistes reggae dans les textes de leurs chansons. On peut proposer quelques exemples très importants. 

VI.3.1. Le vocabulaire Rasta
Le mouvement Rasta est un mouvement de rébellion et de libération des consciences. Ainsi, le vocabulaire et le parler font intimement partie des champs de bataille du mouvement. C'est ainsi que les Rasta ont développé un nombre important de jeux de mots plus ou moins évidents qui sont autant de façons de marquer et de frapper les esprits sur les concepts qu'ils soutiennent. Ceci tend à créer un patois propre à la culture rasta, permettant aux différents initiés de se reconnaître et de communiquer entre eux. On peut en proposer une liste non exhaustive:

VI.3.1.a. and I, ou l'unité dans la diversité
L'usage du pronom I et surtout du pronom I and I pour désigner le locuteur est une habitude extrêmement répandue parmi les rastas. En effet, ceux-ci considèrent chaque personne comme étant l'élément d'un tout. Ainsi, dans la tradition, la moitié de la Bible n'a pas été écrite, et réside dans le coeur de l'Homme. De cette manière, si un rasta écoute son coeur, quoi qu'il connaisse de la Bible écrite, il saura reconnaître et écouter le message divin. 
Les deux I représentent ainsi le soi commun pour le premier, et, pour le second, le soi divin, en connexion avec Jah. Beaucoup d'autres expressions rasta font ainsi référence à ce concept, comme « each and everyone », et le fameux « stick a bush », qui a inspiré un titre homonyme des Gladiators, littéralement : every hoe has its stick in the bush, soit chaque feuille a sa place sur le buisson, chaque feuille a sa diversité, mais est membre du même arbre, dans lequel coule la même sève. 
Ce concept est fondamental pour expliquer l'unité rasta malgré les différentes croyances et idées.


VI.3.1.b. Isms, Skisms
Bien que corollaire du concept précédent, il paraît important d'éclaircir cette notion tant elle est importante et parce qu'elle justifie la négation de l'emploi du terme rastafarisme, pourtant correct en langue française. 
De la même manière que les rastas considèrent l'unité à travers la diversité, ils rejettent tout le vocabulaire en -isme, comme capitalisme, communisme, christianisme, etc. En effet, ces mots sont vus comme la manière qu'a Babylone de regrouper les gens et d'établir des barrières entre eux, rendant toute communication vaine, et entraînant la méconnaissance et l'intolérance. « We don't want your Ism-Skisms » signifiant que l'on refuse les catégorisations, qui sont sources de schismes entre individus. 


VI.3.1.c. Autres mots du vocabulaire rasta
Les rastas vont ainsi inventer un grand nombre de mots qui reflètent leur façon de voir le monde:

Inity au lieu de Unity, le pronom « you » marquant l'exclusion. Mais aussi « I » comme « high », élevé, subtil, défoncé : "Car le I est droit, et le U est tordu" (Barry Chevannes). 
Overstand au lieu d'Understand, « understand » signifiant littéralement « se tenir en dessous » et donc « se soumettre ». 
Shitstem au lieu de System
Politricks pour Politic
Iration pour création





VI.4. Retour en Afrique - « Rapatriement »
Pour les Rastas, leurs racines sont en Afrique, dont ils ont été arrachés pour être mis en esclavage dans la Babylone moderne. Ainsi, l'accomplissement des Écritures implique le retour à la terre promise, qui est pour eux l'Éthiopie. 
Cette référence à l'Éthiopie comme terre promise et non à la Palestine s'explique par plusieurs références, bibliques comme traditionnelles. Tout d'abord, les Rastas se souviennent de la Reine de Saba, Makheda, reine éthiopienne ayant visité le roi Salomon, dont elle aurait eu un fils, Menelik, selon la tradition. De même, l'Arche de l'Alliance, contenant les tables de la Loi et le bâton d'Aaron, dont la Bible perd la trace après Salomon, se trouverait aujourd'hui dans une chapelle de l'église orthodoxe Éthiopienne, apportée directement par Ménélik Ier. Salomon a confié l'arche d'alliance à son fils ainé, selon la tradition hébraïque, pour qu'il la préserve des convoitises. Menelik est reparti de Jerusalem, accompagnés de plusieurs prêtres de haut rang, dont les Falashas, Juifs noirs d'Éthiopie sont les descendants. 
Enfin, la prophétie annonçant le couronnement d'un roi en Afrique, accomplie par l'avènement au pouvoir de Haïlé Sélassié, acheva de confirmer l'Éthiopie comme la terre promise, Zion, le Sion (prononcé Zayan en anglais) chanté par les psaumes. 
Il faut également noter que la version anglaise de la Bible utilise le terme « Æthiopia » pour désigner ce qui est aujourd'hui le continent africain et non le mot Afrique qui désignait la province romaine d'Afrique en latin. L'origine du mot « Ethiopia » n'est pas connue avec certitude. Selon les sources, elle pourrait venir du en grec ancien Aithiops (XXXXX), signifiant « au visage brûlé », ou bien être dérivé de Ityopp'is un fils de Koush inconnu de la Bible, qui selon la légende fonda la ville d'Axoum. Voir l'article Éthiopie pour plus de détails. 


VI.5. Rastas et Hippies
Le mouvement rasta est souvent vu comme une variante locale de la grande vague hippie qui eut lieu dans le monde occidental au cours des années 1970. Le message rasta se retrouve alors vu comme une manifestation d'amour et de paix universelle, comme prôné par les hippies. 
Bien que fondamentalement un message de paix et d'amour, le message rasta ne peut absolument pas se résumer à eux seuls. En effet, le mouvement rasta est avant tout un mouvement d'émancipation des consciences, et, surtout de dénonciation des dérives d'un système. De même que le reggae est une musique de rebelle, comme chanté par Bob Marley, le message rasta est avant tout un message de rupture et de rébellion spirituelle. 
Si cette rébellion spirituelle est souvent assimilée à une forme d'action pacifique à l'image des mouvements de Gandhi ou de Martin Luther King, ce n'est pas vrai en général. Peter Tosh, souvent qualifié du Malcom X rasta, ne disait-il pas que tout le monde veut la paix alors que lui désire la justice ? (« Everyone is crying out for peace, none is crying out for justice » - Equal Rights, 1977). 
Enfin, les rastas ont un fort attachement aux textes sacrés, à la méditation religieuse et recherchent en permanence à se rapprocher du lien ancestral qui les unit à l'Afrique et à leurs origines. En particulier, le traitement des femmes et des homosexuels est abordé d'une manière qui serait qualifiée de traditionaliste. 
Il ne s'agit pas non plus de voir dans les rastas de dangereux rebelles prêts à prendre les armes pour détruire la société moderne en vertu de valeurs obscurantistes, car ce n'est absolument pas le cas. Les rastas sont en majorité de paisibles personnes. Simplement, et la musique le montre bien, le message rasta est plus proche d'un message de paix universel que d'un message de résistance, comme le reggae est plus proche du punk que du rock progressif... 
Une autre différence notable entre le mouvement rasta et le mouvement hippie, se trouve dans l'origine sociologique de leurs adeptes : si les hippies sont généralement des jeunes issus de la classe moyenne voire des classes les plus aisées, les rastafari sont quasi uniquement originaires des ghettos et des classes défavorisées. 



VII. Ouverture de la culture rasta au reste du monde
Initialement confiné au sein des communautés rasta, le message s'est petit à petit répandu dans le monde. La première étape déterminante a été l'ouverture aux jeunes des ghettos de Jamaïcains, formés par l'exode rural, et remplis de jeunes essayant d'échapper à la délinquance, ne pas devenir des rude boys. La musique étant, à cette époque, très importante dans la culture populaire, le message s'est ensuite naturellement adapté aux compositions de l'époque. On est ainsi progressivement passé du rock steady, aux paroles axées sur les relations amoureuses puis à une musique plus spirituelle, le roots reggae. On constate très bien ce changement avec des artistes comme Ken Boothe, Bob Marley ou encore Max Romeo. 
Enfin, l'avènement du reggae comme musique populaire internationalement a permis l'expansion du message dans le monde entier séduisant des gens de tous les continents. Ceci n'est pas sans poser des questions, en particulier sur la pertinence du message reçu, et sur son adaptation aux autres populations. En effet, les racines africaines d'un rasta noir sont peut-être plus évidentes que celles d'un européen blanc... De plus, une critique souvent formulée à l'encontre des jeunes gens européens blancs portant les dreadlocks est la dilution du message, celui-ci se teintant d'une couleur hippie plutôt éloigné du message d'origine. Ainsi, la question de la possibilité de s'affirmer rasta lorsque l'on est blanc et européen est toujours ouverte, tout individu ayant la possibilité de ressentir un besoin inconscient de revenir à un mode de vie et de penser plus authentiques. Rasta ne se borne pas à des limites ethniques, le mouvement se base sur une « livity », manière de vivre et de se comporter qui remonte à la création de toute chose dont celle de l'Homme. La pensée, la spiritualité Rasta se veut universelle. 
Ainsi, il serait erroné de considérer que la philosophie rasta n'est pas reconnue en dehors de la Jamaique, et il est tout à fait possible de s'en inspirer de manière plus ou moins importante. Par exemple Max Cavalera, ancien chanteur du groupe de metal Sepultura et actuel chanteur de Soulfly s'inspire largement de la philosophie rasta dans ses paroles (I and I, Tribe, etc.) alors qu'il est blanc et qu'il pratique une musique, en dépit de quelques emprunts, très éloignée du reggae. 

VII.1. Rastafari soufi
Le mouvement Baye Fall (Originaire du Sénégal) est une branche de la confrérie des Mourides (l'un des nombreux courants du Soufisme) fondée par Cheykh Ibrahima Fall, lui même adepte de Cheikh Ahmadou Bamba. Culte qui voue un pouvoir total et une croyance absolue en Dieu et au marabout (guide spirituel/représentant de Dieu sur terre "khalifatoul lahi fil ard"). 
Ce mouvement développe une croyance au soufisme qui se rapproche de la manière rastafari, le représentant de Dieu sur Terre n'est pas Haïlé Selassié mais le marabout du mouvement. On n'y retrouve pas les notions d'exil comme les jamaïcains et peu de conceptions sont similaires au mouvement rasta, cependant, l'islam se reconnaissant comme la vraie héritière de la religion des rois David et Salomon, on peut y voir, à cause des coutumes qui se ressemblent, une sorte de rastafarisme se disant "musulman". 
Forme de religion détachée de toute possessions matérielles, ou l'on fait les choses pour Dieu et non pas pour ou en fonction des autres. Tout se partage, le don de soi est naturel, et la foi en l'humain est essentielle. 
Mode de vie confondu totalement avec le mode de vie religieux, qui se rapproche du mode de vie rasta, mais avec une plus grande importance vouée au culte religieux. A la différence des musulmans, les Baye Fall n'ont pas d'obligations de prières, le travail au service du Cheikh est élevé au titre de culte religieux et le cheikh qui est leur maître est désigné pour les conduire vers le Tout Puissant. 
Le dreadlocks demeurent l'une des plus grandes particularités de ces religieux, plusieurs versions expliquent son origine : C'est une initiation du Cheikh Ahmadou Bamba. Il avait pris l'habitude de conserver ses cheveux. Ses disciples ont décidé de perpétuer cette habitude. 
Ces chevelures tirent leur origine des prétendus "saints" venus après les fondateurs du soufisme. 
Les saints n'avaient pas de moyens pour se coiffer, leurs cheveux poussaient alors jusqu'à prendre la forme de dreadlocks. 
A l'instar des vrais rastas (et non pas certains porteurs de dreadlocks non-croyants qui utilisent des produits pour entretenir leurs cheveux), la coiffure Baye Fall est naturelle et entretenue de façon naturelle. Ce mouvement fait de plus en plus d'adeptes en Afrique de l'ouest (Mali, Côte d'Ivoire, Sénégal, ...). 

<TXT=w-Réchauffementclimatique9>

I. Évolution passée des températures et conséquences

I.1. Cycles climatiques
Le climat global de la Terre connait des modifications plus ou moins cycliques de réchauffements alternant avec des refroidissements qui diffèrent par leur durée (de quelques milliers à plusieurs millions d'années) et par leur amplitude. Depuis 800 000 ans, le climat terrestre a connu plusieurs de ces cycles. Plusieurs cycles de 100 000 ans environ se sont répétés au cours de cette période. Chaque cycle commence par un réchauffement brutal suivi d'une période chaude de 10 000 à 20 000 ans environ, appelée période interglaciaire. Cette période est suivie par un refroidissement progressif et l'installation d'une ère glaciaire. À la fin de la glaciation, un réchauffement brutal amorce un nouveau cycle. Nous vivons actuellement depuis plus de 10 000 ans dans une période interglaciaire (voir figure). 
Grâce à l'étude des carottages de glace et plus précisément de l'analyse de la composition isotopique de l'oxygène piégé dans la glace, les températures atmosphériques des cycles glaciaires de l'ère quaternaire ont pu être reconstituées. La carotte glaciaire la plus profonde a été forée dans le cadre du projet Epica, en Antarctique, à plus de 3 500 mètres de profondeur et permettant de remonter l'histoire du climat en Antarctique jusqu'à 800 000 ans. Les carottes de glace contiennent des bulles d'air et des indications sur la teneur en gaz de l'atmosphère d'autrefois, ce qui montre que les températures globales sont liées à la quantité de gaz à effet de serre dans l'atmosphère[réf. nécessaire]. 
Les variations du climat sont corrélées avec celles de l'insolation, des paramètres de Milankovic, de l'albédo, des cycles solaires et des concentrations dans l'atmosphère des gaz à effet de serre comme le dioxyde de carbone et des aérosols. 


I.2. Amplitudes des variations climatiques
Au cours du quaternaire, l'amplitude thermique a été de l'ordre de 10 °C, mais avec des hausses de température n'ayant jamais dépassé de plus de 4 °C la température moyenne annuelle de la fin du xxe siècle. 
En revanche pour les cycles plus anciens, comme durant le Permien, la température moyenne globale a atteint 22 °C soit 8 °C de plus par rapport à la moyenne actuelle, comme on peut le voir sur le graphique ci-contre. Durant ces périodes chaudes qui ont duré plusieurs dizaines de millions d'années, la Terre était dépourvue de calottes polaires. 


I.3. Temps historiques
À l'intérieur des grandes fluctuations climatiques terrestres, se trouvent des variations plus brèves et plus limitées en intensité. Ainsi, au cours du dernier millénaire, est apparu une période chaude aux xe et xie siècles appelée « optimum climatique médiéval » : c'est l'époque où les navigateurs vikings découvrent et baptisent le Groenland (littéralement « Pays vert ») et fondent des colonies à l'extrême sud de l'île. De même, l'époque des Temps Modernes (1550-1850) connut une période de refroidissement que les historiens appellent le « petit âge glaciaire » caractérisé par des hivers très rigoureux, dont le terrible hiver 1708-1709. Cette année là, les céréales manquèrent dans la plus grande partie de la France, et seuls la Normandie, le Perche et les côtes de Bretagne ont pu produire assez de grain pour assurer les semences. Dans la région parisienne le prix du pain atteignit, en juin 1709, 35 sous les neuf livres au lieu de 7 sous ordinairement. De nombreux arbres gelèrent jusqu'à l'aubier, et la vigne disparut de plusieurs régions de la France. Du 10 au 21 janvier, la température sous-abri se maintint à Paris aux environs de -20 °C, avec des minima absolus de -23 °C les 13 et 14 janvier ; le 11, le thermomètre s'abaissa jusqu'à -16 °C à Montpellier et -17 °C à Marseille. 
Selon les reconstitutions de températures réalisées par les climatologues, la dernière décennie du xxe siècle et le début du xxie siècle constituent la période la plus chaude des deux derniers millénaires (voir graphique). Notre époque serait même un peu plus chaude (de quelques dixièmes de degrés) que ne le fut l'optimum climatique médiéval. 
Les mesures terrestres de température réalisées au cours du xxe siècle montrent une élévation de la température moyenne. Ce réchauffement se serait déroulé en deux phases, la première de 1910 à 1945, la seconde de 1976 à aujourd'hui. Ces deux phases sont séparées par une période de léger refroidissement. Ce réchauffement planétaire semble de plus corrélé avec une forte augmentation dans l'atmosphère de la concentration de plusieurs gaz à effet de serre, dont le dioxyde de carbone, le méthane et le protoxyde d'azote. 
L'élévation de la température moyenne du globe entre 1906 et 2005 est estimée à 0,74 °C (à plus ou moins 0,18 °C près), dont une élévation de 0,65 °C durant la seule période 1956-2006. 
La température moyenne planétaire de 2001 à 2007 est de 14,44°C soit 0,21°C de plus de 1991 à 2000. À ce rythme l'augmentation est de 2,5°C en 100 ans. 



II. Observations liées au réchauffement climatique actuel
Plusieurs changements ont été observés dans le monde qui semblent cohérents avec l'existence d'un réchauffement climatique planétaire. Cependant, le lien entre ce réchauffement et les observations faites n'est pas toujours établi de façon sûre. En France c'est l'ONERC qui coordonne les observations.

II.1. Le climat 
Selon le troisième rapport du GIEC, la répartition des précipitations s'est modifiée au cours du xxe siècle. En particulier, les précipitations seraient devenues plus importantes aux latitudes moyennes et hautes de l'hémisphère Nord, et moins importantes dans les zones subtropicales de ce même hémisphère. D'autres experts estiment toutefois les données actuelles trop rares et incomplètes pour qu'une tendance à la hausse ou à la baisse des précipitations puisse se dégager sur des zones de cette ampleur. On observe également depuis 1988 une diminution notable de la couverture neigeuse printanière aux latitudes moyennes de l'hémisphère nord. Cette diminution est préoccupante car cette couverture neigeuse contribue à l'humidité des sols et aux ressou 


II.2. La fonte de la banquise
Plusieurs études indiquent que les banquises sont en train de se réduire. Le satellite spécialisé CryoSat-215, qui sera mis en orbite en 2009 après l'échec du premier satellite CryoSat en 2005, fournira des informations plus précises sur les quantités de glace polaire.

II.2.1. En Arctique
Des observations par satellite montrent que ces banquises perdent de la superficie dans l'océan Arctique. Par ailleurs, un amincissement de ces banquises, en particulier autour du pôle nord, a été observé. L'âge moyen des glaces sur la période 1988-2005, est passé de plus de six ans à moins de trois ans. La réduction de l'étendue moyenne de la banquise arctique depuis 1978 est de l'ordre de 2,7 % par décennie (plus ou moins 0,6 %), son étendue minimale en fin d'été diminuant de 7,4 % par décennie (plus ou moins 2,4 %)12. Le réchauffement dans cette région est de l'ordre de 2,5 °C19 (au lieu de 0,7 °C en moyenne sur la planète), et l'épaisseur moyenne des glaces a perdu 40 % de sa valeur entre les périodes 1958-1976 et 1993-1997. 2007 marque un minimum de la banquise en été. Cette année-là, les observations satellitaires constatent une accélération de la fonte de la banquise arctique, avec une perte de 20 % de la surface de la banquise d'été en un an22. Les observations menées pendant l'expédition Tara dirigée sous l'égide du programme européen Damoclès (Developping Arctic Modelling and Observing Capabillities for Long-term Environmental Studies) de septembre 2006 à décembre 2007 indiquent que les modifications entamées dans l'océan Arctique sont profondes et irréversibles. Par ailleurs, le Groenland a vu ses glaciers se réduire de 230 à 80 milliards de tonnes par an de 2003 à 2005, ce qui contribuerait à 10 % de l'élévation du niveau des mers. 


II.2.2. En Antarctique
En Antarctique, les mesures par satellites, faites depuis 1979 ne montrent pas actuellement de diminution de surface, contrairement à la banquise Arctique. Cependant, on observe un certain nombre de phénomènes exceptionnels. Ainsi, 3 500 km2 de la banquise Larsen B, (l'équivalent en surface des deux tiers d'un département français), se sont fragmentés en mars 2002, les premières crevasses étant apparues en 1987. Cette banquise était considérée comme stable depuis 10 000 ans. Au mois d'avril 2009, la plaque Wilkins, dont la superficie était naguère de 16 000 km2 s'est également détachée. 



II.3. Le recul des glaciers de montagnes 
À quelques exceptions près, la plupart des glaciers montagnards étudiés sont en phase de recul. 
Les glaciers de l'Himalaya reculent rapidement et pourraient disparaître dans les cinquante prochaines années, selon des experts réunis à Katmandou pour une conférence sur le réchauffement climatique le 4 juin 2007[réf. nécessaire]. Les températures dans cette région ont crû de 0,15 °C à 0,6 °C tous les 10 ans au cours des 30 dernières années. De nombreux travaux documentent ce recul et cherchent à l'expliquer. Un tel recul semble tout à fait cohérent avec un réchauffement du climat. Cependant, cette hypothèse n'est pas certaine, certains glaciers ayant commencé à reculer au milieu du xixe siècle, après la fin du petit âge glaciaire. L'avancée ou le recul des glaciers sont récurrents et liés à de nombreux facteurs, parmi lesquels les précipitations ou le phénomène El Niño jouent un rôle important. Par exemple le recul actuel de la mer de Glace à Chamonix découvre des vestiges humains du Moyen Âge, preuve que le glacier a déjà reculé davantage que de nos jours à une période historiquement proche. 
Il faut également souligner la quasi-absence de données sur les glaciers himalayens. Par exemple, des données fiables n'existent que pour 50 glaciers indiens, sur plus de 9 500.33 


II.4. Les pratiques agricoles 
Le climat, et en particulier les températures, ont un effet sur la date des récoltes agricoles. Dans de nombreux cas, les dates de vendanges sont régulièrement avancées, comme en Bourgogne. De plus ces phénomènes peuvent être décrits sur plusieurs décennies car ces dates de vendanges ont été consignées dans le passé et archivées. De tels documents sont utilisés pour déterminer les températures à des périodes où les thermomètres n'existaient pas ou manquaient de précision. Un réchauffement climatique depuis le xxe siècle est clairement établi par l'étude de ces archives (ainsi, la date de début des vendanges à Châteauneuf-du-Pape a avancé d'un mois en cinquante ans).


II.5. Chorologie
Plusieurs équipes de chercheurs ont observé une modification de l'aire de répartition de différentes espèces animales et végétales. Dans certains cas, en particulier lorsque cette aire se déplace vers le nord ou vers de plus hautes altitudes, le réchauffement climatique planétaire est parfois proposé comme cause de ces modifications. Par exemple, l'extension actuelle de l'aire de répartition de la chenille processionnaire du pin, qui a atteint Orléans en 1992 et Fontainebleau en 2005, pourrait être due au réchauffement climatique. 


II.6. Cyclones tropicaux
Le consensus scientifique dans le dernier rapport AR4 du GIEC est que l'intensité des cyclones tropicaux va probablement augmenter (avec une probabilité supérieure à 66%). 
Une étude publiée en 2005, remise en question depuis par une seconde étude, indique une augmentation globale de l'intensité des cyclones entre 1970 et 2004, le nombre total de cyclones étant en diminution pendant la même période. Selon cette étude, il est possible que cette augmentation d'intensité soit liée au réchauffement climatique, mais la période d'observation est trop courte et le rôle des cyclones dans les flux atmosphériques et océaniques n'est pas suffisamment connu pour que cette relation puisse être établie avec certitude. La seconde étude publiée un an plus tard ne montre pas d'augmentation significative de l'intensité des cyclones depuis 198642,43. Ryan Maue, de l'université de Floride, dans un article intitulé "Northern Hemisphere tropical cyclone activity", observe pour sa part une baisse marquée de l'activité cyclonique depuis 2006 dans l'hémisphère nord par rapport aux trente dernières années. Il ajoute que la baisse est probablement plus marquée, les mesures datant de trente ans ne détectant pas les activités les plus faibles, ce que permettent les mesures d'aujourd'hui. Pour Maue, c'est possiblement un plus bas depuis cinquante ans que l'on observe en termes d'activité cyclonique. 
Par ailleurs, les simulations informatiques ne permettent pas dans l'état actuel des connaissances de prévoir d'évolution significative du nombre de cyclones lié à un réchauffement climatique. 


II.7. Le réchauffement des océans et l'élévation du niveau de la mer
On observe un réchauffement des océans, qui diminue avec la profondeur. On estime que les océans ont absorbé à ce jour plus de 80 % de la chaleur ajoutée au système climatique. Ce réchauffement entraîne une montée du niveau de la mer par dilatation thermique des océans. Différentes données obtenues à l'aide de marégraphes et de satellites ont été étudiées. Leur analyse suggère que le niveau de la mer s'est élevé au cours du xxe siècle de quelques dizaines de centimètres, et qu'il continue à s'élever régulièrement. Le GIEC estime que le niveau de la mer s'est élevé de 1,8 mm par an entre 1961 et 2003,48. Cette élévation du niveau de la mer peut aussi être observée indirectement par ses conséquences sur l'environnement, comme c'est le cas au Nouveau-Brunswick. 
Dans le cadre du "système ARGO", 3000 balises automatiques ont été réparties dans tous les océans en 2007 et permettront de suivre la température et la salinité des océans jusqu'à 2000 mètres de profondeur. En Atlantique Nord, des chercheurs de l'Ifremer Brest ont confirmé les tendances au réchauffement dans les couches de surface.


II.8. Le réchauffement des eaux et la masse corporelle des poissons
Une étude menée conjointement par le Centre français de recherche pour l'ingénierie de l'agriculture et de l'environnement (Cemagref) et par l'Institut Leibniz pour les sciences marines de Kiel, rendue publique le 20 juillet 2009 dans les Comptes-rendus de l'Académie américaine des Sciences, a conclu que la masse corporelle de certains poissons d'eau douce des fleuves et rivières européens (truites, barbeaux...), ainsi que certaines populations de la mer Baltique et de la mer du Nord, a diminué en moyenne de moitié en un quart de siècle, et ce en raison de l'élévation, due au phénomène de réchauffement climatique, de la température des eaux.



III. Causes 

III.1. Origine humaine
Selon le GIEC, le réchauffement climatique est largement attribué à un effet de serre additionnel dû aux rejets de gaz à effet de serre produits par les activités humaines, et principalement les émissions de CO252,53. L'origine humaine des gaz à effet de serre est confirmée entre autres par l'évolution des composantes isotopiques du carbone dans l'atmosphère. Les concentrations actuelles de CO2 dépassent de 35 % celles de l'ère préindustrielle, surpassant de loin les taux des 600 000 dernières années. Elles sont passées de 280 ppm à l'époque pré-industrielle à 379 ppm en 2005, et celles de méthane ont augmenté de 150 %55. 
On assiste à une augmentation de 40 % de la vitesse de croissance du CO2 dans l'atmosphère, augmentant de +1,5 ppm par an de 1970 à 2000, et de +2,1 ppm par an entre 2000 et 2007. 
Des experts du GIEC ont confirmé le 2 février 2007 que la probabilité que le réchauffement climatique soit dû à l'activité humaine est supérieure à 90 %3. Leurs conclusions sont tirées des résultats d'expériences avec des modèles numériques. En particulier, l'augmentation de la température moyenne mondiale depuis 2001 est en accord avec les prévisions faites par le GIEC depuis 1990 sur le réchauffement induit par les gaz à effets de serre. Enfin, un réchauffement uniquement dû à l'activité solaire n'expliquerait pas pourquoi la troposphère verrait sa température augmenter et pas celle de la stratosphère. 
L'hypothèse d'un lien entre la température moyenne du globe et le taux de dioxyde de carbone dans l'atmosphère a été formulée pour la première fois en 1894 par Svante Arrhenius. Mais c'est en 1979, lors de la première conférence mondiale sur le climat, à Genève, qu'est avancée pour la première fois sur la scène internationale l'éventualité d'un impact de l'activité humaine sur le climat. 

III.1.1. Effet de serre additionnel
L'effet de serre est un phénomène naturel : une partie du rayonnement infrarouge émis par la Terre vers l'atmosphère terrestre reste piégée par les gaz dits « à effet de serre », qui augmentent ainsi la température de la basse atmosphère (troposphère). Ces gaz sont essentiellement de la vapeur d'eau, et une infime partie est d'origine humaine. Sans cet effet, la température de surface de la Terre serait en moyenne de -18 °C ! Actuellement ce phénomène naturel se renforce car la quantité de gaz à effet de serre a augmenté ces dernières années, en particulier le CO2, naturellement en très faible concentration dans l'atmosphère par rapport à la vapeur d'eau ou au diazote (N2), ce qui déséquilibre le bilan radiatif de la Terre. Il a été prouvé par l'étude isotopique du carbone dans l'air que cette augmentation des quantités de gaz à effet de serre est due à la combustion de matière carbonée fossile. 
Selon les conclusions du rapport de 2001 des scientifiques du GIEC, la cause la plus probable de ce réchauffement dans la seconde moitié du xxe siècle serait le « forçage anthropique », c'est-à-dire l'augmentation dans l'atmosphère des gaz à effet de serre résultant de l'activité humaine. Selon les prévisions actuelles, le réchauffement planétaire se poursuivrait au cours du xxie siècle mais son amplitude est débattue : selon les hypothèses retenues et les modèles employés, les prévisions pour les 50 années à venir vont de 1,8 à 3,4 °C. 


III.1.2. Confrontations des modèles et des observations 

III.1.2.a. Hypothèses
Les modèles numériques ont été utilisés pour estimer l'importance relative des divers facteurs naturels et humains au travers de simulations menées sur des supercalculateurs, pour identifier le ou les facteurs à l'origine de la brutale hausse de température. Plusieurs hypothèses ont été testées :

les fluctuations cycliques de l'activité solaire ; 
la rétention de la chaleur par l'atmosphère, amplifiée par les gaz à effet de serre ; 
la modification de la réflectivité de la surface terrestre - l'albédo - par la déforestation, l'avancée des déserts, l'agriculture, le recul des glaces, neiges et glaciers, mais aussi par les cirrus artificiels créés par les traînées des avions et l'étalement urbain ; 
les émissions volcaniques. 

Certaines de ces causes sont d'origine humaine, comme la déforestation et la production de dioxyde de carbone par combustion de matière fossile. D'autres sont naturelles, comme l'activité solaire ou les émissions volcaniques.


III.1.2.b. Résultats
Les simulations climatiques montrent que le réchauffement observé de 1910 à 1945 peut être expliqué par les seules variations du rayonnement solaire (voir changement climatique) En revanche pour obtenir le réchauffement observé de 1976 à 2006 (voir graphique), on constate qu'il faut prendre en compte les émissions de gaz à effet de serre d'origine humaine. Les modélisations effectuées depuis 2001 estiment que le forçage radiatif anthropique est dix fois supérieur au forçage radiatif dû à des variations de l'activité solaire, bien que le forçage dû aux aérosols soit négatif. Le point essentiel est que le forçage radiatif net est positif. 


III.1.3. Consensus scientifique

Dans son rapport de 2001, le GIEC conclut que les gaz à effet de serre anthropogéniques « jouent un rôle important dans le réchauffement global »60. 
En 2003, l'American Geophysical Union affirme que « les influences naturelles ne permettent pas d'expliquer la hausse rapide des températures à la surface du globe »61. 
Le 7 juin 2005, les académies des sciences des pays du G862 et celles des trois plus gros pays en voie de développement consommateurs de pétrole63 ont signé une déclaration commune à Londres, affirmant que le doute entretenu par certains à l'endroit des changements climatiques ne justifie plus l'inaction et qu'au contraire, il faut « enclencher immédiatement » un plan d'action planétaire pour contrecarrer cette menace globale64. 
Enfin, en 2007, le 4e rapport du GIEC, annonce que la probabilité que le réchauffement climatique soit dû aux activités humaines est supérieure à 90 %3. 

De nombreux scientifiques estiment même que ce rapport n'est pas assez clair et qu'il faudrait dès maintenant un programme international pour réduire drastiquement les deux sources principales de gaz à effet de serre, le transport routier et les centrales à charbon.



III.2. Critiques de l'hypothèse d'une origine humaine 
Bien qu'il existe un fort consensus dans la communauté scientifique sur le rôle prédominant des activités humaines dans le réchauffement climatique du dernier demi-siècle, sa probabilité étant estimée à plus de 90 %3 par le dernier rapport du GIEC en 2007, des personnalités contestent tout ou partie de cette thèse et attribuent le réchauffement à des causes naturelles, liées à l'activité du Soleil. Par ailleurs, des critiques et controverses portent également sur les conséquences du réchauffement (voir le paragraphe Poursuite du réchauffement climatique plus bas) et les actions à mener pour lutter contre (voir la section Réponse des États plus bas). 



IV. Prévisions 

IV.1. Modèles climatiques
La prévision par les scientifiques de l'évolution future du climat est possible par l'utilisation de modèles mathématiques traités informatiquement sur des superordinateurs. Ces modèles, dits de circulation générale, reposent sur les lois générales de la thermodynamique et simulent les déplacements et les températures des masses atmosphériques et océaniques. Les plus récents prennent aussi en considération d'autres phénomènes, comme le cycle du carbone. 
Ces modèles sont considérés comme valides par la communauté scientifique lorsqu'ils sont capables de simuler des variations connues du climat, comme les variations saisonnières, le phénomène El Niño, ou l'oscillation nord-atlantique. Les modèles les plus récents simulent de façon satisfaisante les variations de température au cours du xxe siècle. En particulier, les simulations menées sur le climat du xxe siècle sans intégrer l'influence humaine ne rend pas compte du réchauffement climatique, tandis que celles incluant cette influence sont en accord avec les observations. 
Les modèles informatiques simulant le climat sont alors utilisés par les scientifiques pour prévoir l'évolution future du climat, mais aussi pour cerner les causes du réchauffement climatique actuel, en comparant les changements climatiques observés avec les changements induits dans ces modèles par différentes causes, naturelles ou humaines. 
Ces modèles sont l'objet d'incertitudes de nature mathématique, informatique, physique, etc. Les trois principales sources d'incertitude mentionnées par les climatologues sont : 

La modélisation des nuages ;
La simulation de phénomènes de petite échelle, comme les cellules orageuses, ou l'effet du relief sur la circulation atmosphérique ; 
La modélisation de l'interface entre les océans et l'atmosphère. 

De façon plus générale, ces modèles sont limités d'une part par les capacités de calcul des ordinateurs actuels, et le savoir de leurs concepteurs d'autre part, la climatologie et les phénomènes à modéliser étant d'une grande complexité. L'importance des investissements budgétaires nécessaires sont aussi un aspect non négligeable de la recherche dans le domaine du réchauffement climatique. Malgré ces limitations, le GIEC considère les modèles climatiques comme des outils pertinents pour fournir des prévisions utiles du climat.


IV.2. Poursuite du réchauffement climatique
Pour les climatologues regroupés au sein du GIEC (IPCC en anglais), l'augmentation des températures va se poursuivre au cours du xxie siècle. L'ampleur du réchauffement attendu le plus probable est de à 1,8 à 3,4 °C. 
L'ampleur du réchauffement prévu est incertaine ; les simulations tiennent compte : 

des incertitudes liées aux modèles (voir plus haut) ;
des incertitudes sur le comportement de l'humanité au cours du xxie siècle.

Afin de prendre en compte ce dernier paramètre dans leurs prévisions, les climatologues du GIEC ont utilisé une famille de 40 scénarios d'émission de gaz à effet de serre détaillés dans le rapport SRES. Dans certains scénarios, la croissance de la population humaine et le développement économique sont forts, tandis que les sources d'énergie utilisées sont principalement fossiles. Dans d'autres scénarios, un ou plusieurs de ces paramètres sont modifiés, entrainant une consommation des énergies fossiles et une production de gaz à effet de serre moindres. Les scénarios utilisés comme hypothèse de travail pour l'élaboration du troisième rapport du GIEC (2001) ne prennent pas en compte l'éventualité d'une modification intentionnelle des émissions de gaz à effet de serre à l'échelle mondiale. 
Les incertitudes liées au fonctionnement des modèles sont mesurées en comparant les résultats de plusieurs modèles pour un même scénario, et en comparant les effets de petites modifications des scénarios d'émission dans chaque modèle. 
Les variations observées dans les simulations climatiques sont à l'origine d'un éparpillement des prévisions de l'ordre de 1,3 à 2,4 °C, pour un scénario (démographique, de croissance, de « mix énergétique mondial », etc.) donné. Le type de scénario envisagé a un effet de l'ordre de 2,6 °C sur le réchauffement climatique simulé par ces modèles et explique une bonne partie de la marge d'incertitude existant quant à l'ampleur du réchauffement à venir. 
Les prévisions d'augmentation de température pour l'horizon 2 100 données par le GIEC (SPM du AR4 2007) s'échelonnent de 1,1 à 6,3 °C. Les experts du GIEC affinent leurs prévisions en donnant des valeurs considérées comme « les meilleures estimations », ce qui permet de réduire la fourchette de 1,8 à 4,0 °C. Et en éliminant le scénario A1F1, considéré comme irréaliste, l'augmentation de température serait comprise entre 1,8 et 3,4 °C. 
Les scientifiques du GIEC considèrent que ces prédictions sont les meilleures prédictions actuellement possibles, mais qu'elles sont toujours sujettes à des réajustements ou à des remises en cause au fur et à mesure des avancées scientifiques. Ils considèrent qu'il est nécessaire d'obtenir des modèles plus réalistes et une meilleure compréhension des phénomènes climatiques, ainsi que des incertitudes associées. 
Cependant, de nombreux climatologues pensent que les améliorations à court terme apportées aux modèles climatiques ne modifieront pas fondamentalement leurs résultats, à savoir que le réchauffement planétaire va continuer et que son ampleur sera plus ou moins importante en fonction de la quantité de gaz à effet de serre émis par les activités humaines au cours du xxie siècle, et ce en raison de l'inertie des systèmes climatiques à l'échelle planétaire. 
Les derniers articles scientifiques montrent que l'année 199874 a été la plus chaude de toute l'histoire de la météorologie, que le réchauffement s'accélère - 0,8 °C en un siècle, dont 0,6 °C sur les trente dernières années -, mais aussi d'après l'analyse de sédiments marins, que la chaleur actuelle se situe dans le haut de l'échelle des températures depuis le début de l'holocène, c'est-à-dire 12 000 ans. 



V. Conséquences environnementales prévues
Les modèles utilisés pour prédire le réchauffement planétaire futur peuvent aussi être utilisés pour simuler les conséquences de ce réchauffement sur les autres paramètres physiques de la Terre, comme les calottes de glace, les précipitations ou le niveau des mers. Dans ce domaine, un certain nombre de conséquences du réchauffement climatique sont l'objet d'un consensus parmi les climatologues.

V.1. La montée des eaux 
Une des conséquences du réchauffement planétaire sur lesquelles s'accordent les scientifiques est une montée du niveau des océans. Deux phénomènes engendrent cette élévation :

l'augmentation du volume de l'eau due à son réchauffement (dilatation thermique) ; 
l'apport d'eau supplémentaire provenant de la fonte des calottes glaciaires continentales.

Selon le troisième rapport du GIEC, le niveau de la mer s'est élevé de 0,1 à 0,2 m au xxe siècle. La montée du niveau des eaux est due principalement au réchauffement des eaux océaniques et à leur dilatation thermique. L'effet de la fonte des glaciers ne se ferait sentir qu'à beaucoup plus long terme, et celle des calottes polaires à l'échelle de plusieurs siècles ou millénaires. De même que pour les températures, les incertitudes concernant le niveau de la mer sont liées aux modèles, d'une part, et aux émissions futures de gaz à effet de serre, d'autre part. 
L'élévation entre 1993 et 2003 est estimée à 3,1 mm par an (plus ou moins 0,7 mm). L'élévation prévue du niveau de la mer en 2100 est de 18 à 59 cm, selon le 4e rapport du GIEC. Elle pourrait être de 2 mètres en 2300. 
Une montée des eaux de quelques centimètres n'a pas d'impact très visible sur les côtes rocheuses, mais peut avoir des effets très importants sur la dynamique sédimentaire des côtes plates : dans ces régions, qui sont en équilibre dynamique, la montée des eaux renforce les capacités érosives de la mer, et déplace donc globalement l'équilibre vers une reprise de l'érosion qui fait reculer les côtes. La montée du niveau moyen de la mer a ainsi des effets beaucoup plus importants que la simple translation de la ligne de côte jusqu'aux courbes de niveau correspondantes. 

V.2. Les précipitations
Selon le dernier rapport du GIEC, une augmentation des précipitations aux latitudes élevées est très probable tandis que dans les régions subtropicales on s'attend à une diminution, poursuivant une tendance déjà constatée, de sorte qu'à l'horizon 2025, un tiers de la population mondiale pourrait se trouver en état de stress hydrique.


V.3. La circulation thermohaline
La circulation thermohaline désigne les mouvements d'eau froide et salée vers les fonds océaniques qui prennent place aux hautes latitudes de l'hémisphère nord. Ce phénomène serait, avec d'autres, responsable du renouvellement des eaux profondes océaniques et de la relative douceur du climat européen. 
En cas de réchauffement climatique, le moteur qui anime les courants marins serait menacé. En effet, les courants acquièrent leur énergie cinétique lors de la plongée des eaux froides et salées, et donc denses, dans les profondeurs de l'océan Arctique. Or, l'augmentation de la température devrait accroître l'évaporation dans les régions tropicales et les précipitations dans les régions de plus haute latitude. L'océan Atlantique, en se réchauffant, recevrait alors plus de pluies, et en parallèle la calotte glaciaire pourrait partiellement fondre (voir Événement de Heinrich). Dans de telles circonstances, une des conséquences directes serait un apport massif d'eau douce aux abords des pôles, entraînant une diminution de la salinité marine et donc de la densité des eaux de surface. Cela peut empêcher leur plongée dans les abysses océaniques. Ainsi, les courants tels que le Gulf Stream pourraient ralentir ou s'arrêter, et ne plus assurer les échanges thermiques actuels entre l'équateur et zones tempérées. Pour le xxie siècle, le GIEC considérait dans son rapport 2007 comme très probable un ralentissement de la circulation thermohaline dans l'Atlantique, mais comme très improbable un changement brusque de cette circulation. 
Selon certaines thèses[réf. nécessaire], un phénomène d'arrêt du Gulf Stream, dû au réchauffement climatique, pourrait engendrer un effet paradoxal : par son inégale distribution de la chaleur, une ère glaciaire en Europe et dans les régions à hautes latitudes. En effet, l'Europe se situe à la même latitude que le Québec, et la seule différence de climat semble résider dans le fait que l'Europe profite de l'apport thermique du Gulf Stream[réf. nécessaire]. L'équateur, à l'inverse, accumulerait alors de la chaleur stimulant de ce fait la formation continuelle d'ouragans amenant des précipitations de grande ampleur. 
Cette hypothèse d'un refroidissement de l'Europe qui suivrait le réchauffement global n'est cependant pas validée. En effet, il n'est nullement établi que le Gulf Stream soit la seule cause des hivers doux en Europe. Ainsi, Richard Seager a publié en 2002 une étude scientifique sur l'influence du Gulf Stream sur le climat. Ses conclusions sont sans appel : l'effet du Gulf Stream est, selon lui, un mythe et a un effet mineur sur le climat en Europe. La différence entre les températures hivernales entre l'Amérique du Nord et l'Europe est due au sens des vents dominants (vent continental glacial du nord sur la côte Est de l'Amérique du Nord et vent océanique de l'ouest en Europe) et à la configuration des Montagnes Rocheuses. Même en cas d'arrêt du Gulf Stream, le climat de l'Europe occidentale serait comparable à celui de la côte Ouest des États-Unis plutôt qu'à celui de la côte Est. 


V.4. Glaces et couverture neigeuse
Les scientifiques du GIEC prévoient, pour le xxie siècle une diminution de la couverture neigeuse, et un retrait des banquises. Les glaciers et calottes glaciaires de l'hémisphère nord devraient aussi continuer à reculer, les glaciers situés à moins de 3 400 m d'altitude pouvant être amenés à disparaître. 
LesEn revanche, l'évolution de la calotte glaciaire antarctique au cours du xxie siècle est plus difficile à prévoir. 
Une équipe de chercheurs a récemment mis en évidence un lien entre l'activité humaine et l'effondrement de plates-formes de glace dans l'Antarctique. Les réchauffements locaux seraient dus à un changement de direction des vents dominants, cette modification étant elle-même due à l'augmentation de la concentration de l'air en gaz à effet de serre et la dégradation de la couche d'ozone en Antarctique à cause des CFC d'origine humaine. 
Toutefois, selon une lettre envoyée au journal Nature, ces réchauffements ne s'observent que localement. En effet, l'Antarctique connait globalement un climat de plus en plus froid et sa couverture glacée est en expansion, les élévations de la température dans ces secteurs très froids se révélant favorables à une augmentation des précipitations neigeuses donc à terme, à une augmentation des volumes de glace. 
Cependant, la quantité de glace de l'Antarctique déversée dans les mers a augmenté de 75 % durant les dix années précèdant 2008. Ce phénomène risque de s'amplifier en raison de la disparition de la banquise qui cesse alors d'opposer un obstacle au déversement des glaciers dans l'océan.


V.5. Conséquences brusques ou irréversibles, et prospectives
Le quatrième rapport d'évaluation du GIEC énonce que « le réchauffement anthropique de la planète pourrait entraîner certains effets qui sont brusques ou irréversibles, selon le rythme et l'ampleur des changements climatiques ».

La perte partielle des glaciers polaires pourrait impliquer plusieurs mètres d'élévation du niveau des mers, des changements majeurs dans les côtes et des inondations dans les zones basses, avec des effets plus grands dans les deltas et les îles de faible altitude. Ces phénomènes s'étendraient sur plusieurs millénaires mais il n'est pas exclu que le niveau de la mer s'élève plus rapidement que prévu, en quelques siècles93. 
Environ 20 à 30 % des espèces évaluées à ce jour sont susceptibles d'être exposées à un risque accru d'extinction si l'augmentation du réchauffement mondial moyen dépasse 1,5 à 2,5 °C (par rapport à 1980 - 1999). Avec une augmentation de la température mondiale moyenne supérieure d'environ 3,5 °C, les projections des modèles indiquent des extinctions (de 40 à 70 % des espèces évaluées) dans le monde entier93. En juin 2008, les États-Unis ont inscrit l'ours blanc d'Alaska sur la liste des espèces menacées94. 
Certains, comme le climatologue James Hansen, estiment que « la Terre pourrait avoir dépassé le seuil dangereux de CO2, et la sensibilité de la planète au dioxyde de carbone est bien plus importante que celle retenue dans les modèles95 ». 

Des visions prospectives optimistes et moins optimistes cohabitent en 2009 : certains insistent sur le fait que les solutions techniques existent, et qu'il ne reste qu'à les appliquer (les maisons pourraient être isolées, et produire plus d'électricité qu'elles n'en consomment, les transports maîtrisés, les villes pourraient être plus autonomes et dépolluer l'air). Au contraire, d'autres - tout en invitant à appliquer au plus vite ces solutions voire une décroissance soutenable et conviviale - réalertent, constatent que de 1990 à 2009, la tendance a été la réalisation des fourchettes hautes d'émission de gaz à effet de serre, conduisant aux scénarios-catastrophe du GIEC, et estiment qu' il est temps de cesser de parler de « changement » pour décrire une catastrophe


V.6. Des phénomènes à très long terme
La majorité des climatologues pensent que les phénomènes induits par l'émission des gaz à effet de serre vont se poursuivre et s'amplifier. Le troisième rapport du GIEC insiste en particulier sur les points suivants :

certains gaz à effet de serre, ont une espérance de vie longue, et influent donc sur l'effet de serre longtemps après leur émission (durée supérieure à 1 000 ans pour le CO2 selon le quatrième rapport) ; 
de par l'inertie du système climatique, le réchauffement planétaire se poursuivra après la stabilisation de la concentration des gaz à effet de serre. Ce réchauffement devrait cependant être plus lent ; 
l'inertie, plus grande encore, de la masse océanique fait que l'élévation du niveau des mers se poursuivra même après la stabilisation de la température moyenne du globe. La fonte de calottes glaciaires, comme celle du Groenland, sont des phénomènes se déroulant sur des centaines voire des milliers d'années76. 

Les récentes observations dans la zone arctique menées sous l'égide du programme européen Damoclès (Developping Arctic Modelling and Observing Capabillities for Long-term Environmental Studies) ont créé une véritable surprise dans le monde scientifique. En effet, celles-ci montrent une différence importante avec les prévisions issues des différents modèles et sur lesquelles sont basées les conclusions du GIEC : ceci se traduit par une nette accélération des effets dus à l'augmentation des gaz à effet de serre en Arctique (fonte totale de la banquise en été d'ici 2020) 99,100.


V.7. Rétroactions
Les scientifiques nomment ainsi des emballements du système climatique lorsqu'un seuil est dépassé. On parle aussi de bombe à carbone. De telles rétroactions ont déjà été observées lors de précédents réchauffements climatiques, à la fin d'une ère glaciaire ; le climat peut ainsi, en quelques années, se réchauffer de plusieurs degrés. Un exemple concerne les hydrates de méthane. Le méthane (CH4, qui n'est autre que le gaz naturel, à quelques « impuretés » près), est un gaz à effet de serre 23 fois plus réchauffant que le CO2. Il se forme lorsque la décomposition de la matière organique s'effectue avec un manque d'oxygène, et sous l'action de bactéries, un processus nommé méthanisation. Les sols humides (marais) sont très propices à cette création de méthane, qui est alors libéré dans l'atmosphère (cela peut donner lieu à des inflammations spontanées et l'on peut observer des feux follets). Si le sol est gelé, le méthane reste piégé dans la glace sous la forme d'hydrates de méthane. Le sol de Sibérie est ainsi un immense réservoir de méthane (sans doute trop diffus pour être exploité industriellement) : le département des études géologiques des États-Unis a évalué que ce réservoir pouvait être de la même ampleur que tout le gaz, le pétrole et le charbon réunis. Cependant, le magazine Science & Vie d'avril 2006 donnait plutôt comme valeur 1 400 Gt, comparativement à 5 000 Gt pour l'ensemble des combustibles fossiles. Si le sol se réchauffe, la glace fond et libère le méthane déjà présent initialement, ce qui a pour conséquence un effet de serre plus marqué, et par suite un emballement du réchauffement climatique, qui fait fondre la glace encore plus vite... D'où le nom de rétroaction. 
Une autre rétroaction serait le ralentissement et la modification des courants océaniques. L'océan capte aujourd'hui le tiers du CO2 émis par les activités humaines. Mais si les courants océaniques ralentissent, les couches d'eau superficielles peuvent se saturer en CO2 et ne pourraient plus en capter comme aujourd'hui. La quantité de CO2 que peut absorber un litre d'eau diminue à mesure que l'eau se réchauffe. Ainsi, de grandes quantités de CO2 peuvent être relarguées si les courants océaniques sont modifiés. En outre, l'accumulation de CO2 dans les océans conduit à l'acidification de ces derniers, ce qui affecte l'écosystème marin et peut induire à long terme un relargage de CO2. 
Les moteurs de la circulation océanique sont de deux types : l'eau en se rapprochant des pôles se refroidit et devient donc plus dense. De plus, l'eau de mer qui gèle rejette son sel dans l'eau liquide (la glace est constituée d'eau douce), devenant au voisinage des calottes glaciaires encore plus dense. Cette eau plonge donc et alimente la pompe : l'eau plus chaude de la surface est aspirée. L'eau du fond (froide) remonte dans les zones des tropiques et / ou équatoriales et se réchauffe, ceci en un cycle de plus de 1 000 ans. 
Si les calottes de glace fondent, la pompe se bloque : en effet, l'eau qui plonge provient de la calotte et non plus de l'eau refroidie en provenance des tropiques. Un effet similaire est observé si les précipitations augmentent aux hautes latitudes (ce qui est prévu par les modèles) : l'eau qui plongera sera l'eau douce de pluie. À terme, une forte perturbation du Gulf Stream est envisageable. 

V.8. Conséquences du réchauffement climatique sur l'homme et la biosphère
Au-delà des conséquences directes, physiques et climatiques, du réchauffement planétaire, celui-ci influera sur les écosystèmes, en particulier en modifiant la biodiversité. D'après le GIEC, la capacité de nombreux écosystèmes à s'adapter naturellement sera probablement dépassée par la combinaison sans précédent des :

bouleversements climatiques : inondations, incendies de forêts, sècheresses, insectes, acidification des océans ; 
changements mondiaux : changements d'affectation des sols (déboisement, barrages, etc.), pollution, surexploitation des ressources. 

Le déséquilibre naturel qui s'en suivra pourrait entraîner la disparition de plusieurs espèces animales et végétales. C'est une préoccupation dont les États, comme la France, commencent à tenir compte. Pour l'ensemble des populations humaines, ces effets « physiques » et « écologiques » auront de fortes répercussions. La très grande complexité des systèmes écologiques, économiques et sociaux affectés par le réchauffement climatique ne permet pas de faire des prévisions chiffrées comme pour la modélisation physique de la Terre. 
Au niveau biologique et écologique, un consensus scientifique a été atteint sur les points suivants : 

certaines espèces biologiques verront peut-être leur aire de répartition augmenter, mais le bilan du réchauffement climatique en termes de biodiversité sera négatif selon un certain nombre d'études103,104,105 et selon le consensus du 4e rapport du GIEC qui envisage la disparition de 40 à 70 % des espèces évaluées93 ; 
certains systèmes naturels seront plus affectés que d'autres par le réchauffement planétaire. Les systèmes les plus sensibles seraient : les glaciers, les récifs coralliens, les mangroves, les forêts boréales et tropicales, les écosystèmes polaires et alpins, les prairies humides. Le blanchissement des récifs coralliens a été observé pour la première fois dès 1979 dans les Antilles106. Ce phénomène s'est développé régulièrement dans l'espace et le temps à des échelles toujours plus grandes, par exemple à l'échelle de l'océan Indien en 1998[réf. nécessaire]. Si le réchauffement continue au rythme actuel, on craint une extinction de masse des récifs coralliens à l'échelle planétaire à partir de 2015 / 2020 ; 
les dommages causés aux systèmes naturels, que ce soit par leur ampleur géographique ou leur intensité, seront proportionnels à l'intensité et à la rapidité du réchauffement planétaire. 


V.8.1. Conséquences négatives pour l'humanité
Le GIEC prévoit des conséquences négatives majeures pour l'humanité au xxie siècle :

une baisse des rendements agricoles potentiels dans la plupart des zones tropicales et subtropicales ; 
une diminution des ressources en eau dans la plupart des régions sèches tropicales et subtropicales ; 
une diminution du débit des sources d'eau issues de la fonte des glaces et des neiges, suite à la disparition de ces glaces et de ces neiges. 
une augmentation des phénomènes météorologiques extrêmes comme les pluies torrentielles, les tempêtes et les sécheresses, ainsi qu'une augmentation de l'impact de ces phénomènes sur l'agriculture ; 
une augmentation des feux de forêt durant des étés plus chauds ; 
l'extension des zones infestées par des maladies comme le choléra ou le paludisme. Ce risque est fortement minimisé par le professeur spécialiste Paul Reiter107 mais le gouvernement du Royaume-Uni fait remarquer que ce professeur a choisi d'ignorer tous les rapports récents qui le contredisent108 ; 
des risques d'inondation accrus, à la fois à cause de l'élévation du niveau de la mer et de modifications du climat ; 
une plus forte consommation d'énergie à des fins de climatisation ; 
une baisse des rendements agricoles potentiels aux latitudes moyennes et élevées (dans l'hypothèse d'un réchauffement fort). 


V.8.2. Conséquences positives pour l'humanité
Elles sont aussi associées au réchauffement prévu au xxie siècle :

une plus faible mortalité hivernale aux moyennes et hautes latitudes (par opposition à une mortalité estivale plus élevée, comme par exemple la canicule de 2003) ; 
une augmentation éventuelle des ressources en eau dans certaines régions sèches tropicales et subtropicales mais une diminution des ressources dans les régions tempérées (notamment dans les régions de climat méditerranéen) ; 
une hausse des rendements agricoles potentiels dans certaines régions aux latitudes moyennes (dans l'hypothèse d'un réchauffement faible) ; 
l'ouverture de nouvelles voies maritimes dans l'arctique canadien suite à la fonte des glaces dans le passage du Nord-Ouest109. 




V.9. Les conséquences en France
En ce qui concerne la France, l'élévation de température risque d'augmenter le nombre de canicules en 2100. Alors que le nombre de jours de canicule est actuellement de 3 à 10 par an, il pourrait s'élever à une moyenne de 20 à 40 en 2100, rendant banale la canicule exceptionnelle de 2003,112. 
Les précipitations seraient plus importantes en hiver, mais moindres en été. Les régions connaissant des durées de plus de 25 jours consécutifs sans pluie, actuellement limitées au sud-est de la France, s'étendraient à la moitié ouest du territoire. 
La végétation connaitrait une remontée vers le nord. L'épicéa risquerait de disparaitre du Massif Central et des Pyrénées. Le chêne, très répandu dans l'Est de la France, verrait son domaine réduit au Jura et aux Vosges, mais le pin maritime, actuellement implanté sur la façade Ouest, s'étendrait sur la moitié ouest de la France et le chêne vert s'étendrait dans le tiers sud, marquant une étendue du climat méditerranéen. 
Les cultures du midi méditerranéen, telles que celle de l'olivier, pourraient s'implanter dans la vallée du Rhône. On peut désormais trouver des oliviers en tant qu'arbres d'ornement sur toute la façade sud-ouest de l'océan Atlantique, et ce jusqu'en Vendée. Par contre, faute d'eau suffisante, la culture du maïs serait limitée à la partie nord et nord-est du territoire. Les céréales verraient leur rendement augmenter si l'élévation de température ne dépasse pas 2 °C. Par contre, si elle était supérieure, les plantes cultivées auraient du mal à s'adapter et on pourrait craindre des difficultés agricoles. 
Les chutes de neige seront moins abondantes, entraînant un moindre approvisionnement en eau des fleuves, mais également des difficultés d'ordre économique pour l'économie de montagne. Par exemple, les stations de ski situées à moins de 1 500 m d'altitude seraient amenées à fermer leurs pistes et à se reconvertir. 



VI. Conséquences humaines du réchauffement climatique
Face au réchauffement climatique, l'Académie des Sciences américaine note, dans un rapport de 2002 : « il est important de ne pas adopter d'attitude fataliste en face des menaces posées par le changement de climat. (...) Les sociétés ont dû faire face à des changements du climat graduels ou abrupts durant des millénaires et ont su s'adapter grâce à des réactions diverses, telles que s'abriter, développer l'irrigation ou migrer vers des régions plus hospitalières. Néanmoins, parce que le changement du climat est destiné à continuer dans les prochaines décennies, dénier la possibilité d'événements climatiques abrupts ou minimiser leur impact dans le passé pourrait s'avérer coûteux. ». 
Nombre de chercheurs prédisent des conséquences désastreuses en cas d'un réchauffement de 1,5 à 7 °C, mais la plupart estiment qu'en limitant le réchauffement global à 1 °C, les conséquences seraient de grande ampleur mais resteraient acceptables. 

VI.1. Submersion
La montée du niveau de la mer, due essentiellement à la dilatation thermique des océans, est évaluée entre 18 et 59 cm d'ici 2 100 par le 4e rapport du GIEC. Elle inquiète les populations de certaines îles de l'océan Pacifique ou de l'océan Indien qui pourraient se voir complètement submergées. À ce phénomène de montée des eaux s'ajoute un phénomène encore plus important de subduction (enfoncement des terres dans l'Océan) (voir notamment l'article sur l'archipel des Tuvalu et les écoréfugiés). 
Mais cette montée des eaux apparemment minime menace également les 20 % de la population mondiale vivant sur les littor 


VI.2. Agriculture et pêcheries 
L'accroissement de l'évaporation devrait augmenter localement la pluviosité, sauf dans les pays méditerranéens qui verraient la sècheresse s'accentuer, dans un contexte où la violence et / ou la fréquence et gravité des aléas climatiques pourraient croître. 
En zone tempérée (hors des zones arides qui pourraient le devenir encore plus) et circumpolaire, dans un premier temps, la conjonction du réchauffement et de l'augmentation du taux de CO2 dans l'air et les pluies pourrait accroître la productivité des écosystèmes. L'agriculture du Nord des États-Unis, du Canada, de la Russie et des pays nordiques pourraient peut-être en profiter, mais des signes de dépérissement forestier semblent déjà visible dans ces zones. 
Les satellites montrent que la productivité de l'hémisphère Nord a augmenté depuis 1982, du fait de ce réchauffement et de l'enrichissement de l'atmosphère en CO2, mais aussi en partie à cause de l'eutrophisation des écosystèmes, les engrais d'origine humaine (phosphates et nitrates notamment) étant entrainés là où ces substances étaient beaucoup plus rares autrefois. L'augmentation de la biomasse n'est par ailleurs pas nécessairement bénéfique et comporte le risque de s'accompagner d'une régression de la biodiversité. Enfin, au-delà d'un certain seuil, les modèles du GIEC calés sur des tests en laboratoire et en extérieur, prédisent qu'un taux de CO2 ne bénéficierait plus aux plantes, les effets négatifs pouvant alors l'emporter. 
Dans le sud de l'Amérique du Nord, de la Chine, du Japon et de l'Europe, de longues sécheresses, avec des épisodes répétés de canicules pourraient induire des phénomènes d'aridification puis de désertification et salinisation empêchant l'agriculture, détruisant les récoltes ou les rendant très coûteuses. 
De graves incendies pourraient massivement détruire les cultures (en 2007, le feu a détruit en Grèce de vastes zones agricoles dont des oliveraies). Même sans incendies, l'augmentation de l'évapotranspiration en été, liée à une productivité dopée par le CO2, pourrait augmenter la sensibilité d'un milieu aux sécheresses et aggraver de ce fait le risque d'incendies de forêts et de stress et maladies des arbres et des plantes cultivées. 
Une augmentation de la biomasse totale ne compenserait probablement pas un recul d'espèces cultivées, pêchées et chassées. Le bilan global ne peut à ce jour être estimé, mais il pourrait être désavantageux, même dans les zones où les effets positifs se feraient le plus sentir. Pour le GIEC, mis en balance avec les effets négatifs, ces quelques aspects positifs ne permettent pas de considérer le réchauffement climatique comme globalement bénéfique. 
On ignore aussi à partir de quand les écosystèmes (marins notamment) réagiront négativement à l'acidification des eaux qu'entraîne la dissolution de quantités croissantes d'acide carbonique. 
Le Comité économique et social européen dans son avis du 3 février 2009 note que des études comparatives concluent à un bilan de l'agriculture bio en moyenne meilleur (au regard de la consommation de matières premières et d'énergie et au regard du carbone stocké ou des émissions de gaz à effet de serre) que celui de l'agriculture dite conventionnelle, même si l'on tient compte des rendements moindres de l'agriculture bio, ce qui a justifié que le gouvernement allemand, l'intègre parmi les moyens de lutter contre le changement climatique). LE CESRE rappelle aussi qu'une agriculture réorientée et adaptée pourrait selon divers spécialistes et ONG aussi contribuer à tamponner ou freiner les effets du réchauffement (Cool farming)). Le comité ne cite pas les agrocarburants comme une solution, citant le climatologue Paul Crutzen selon qui les émissions de protoxyde d'azote induites par la culture et production de biodiesel, suffisent, dans certaines conditions à faire que le méthylester de colza puisse avoir des effets climatiques pires que ceux du diesel fait avec du pétrole fossile. Le comité, pose aussi la question des fumures traditionnelles et se demande « si l'utilisation intégrale des plantes, telle qu'elle est prévue dans le cadre des biocarburants de la deuxième génération, ne risque pas de porter atteinte aux objectifs fixés en matière de développement de la couche d'humus », c'est-à-dire de contribuer à encore épuiser la matière organique des sols. Le comité repose la question de l'écobilan des biocarburants en citant une étude comparative, de l'Empa qui a conclu qu'une Volkswagen Golf nécessitait 5 265 m2 de colza pour parcourir 10 000 km avec du biodiesel, alors que 37 m2 de panneaux solaires (1/140ème de la parcelle de colza précédente) suffirait à produire assez d'électricité pour parcourir la même distance. 


VI.3. Forêt et sylviculture
Selon une étude récente, en 30 ans, un réchauffement moyen de 0,5 °C a déjà doublé le taux de mortalité des arbres des grandes forêts de l'ouest américain, en favorisant les sècheresses et pullulations de ravageurs (dont scolytes qui ont par exemple détruit environ 1,4 million d'hectares de pins dans le nord-ouest du Colorado). Le manque de neige induit un déficit hydrique et un allongement des sècheresses estivales, avec multiplication des incendies, alertent les auteurs qui craignent des impacts en cascade sur la faune et les écosystèmes. L'augmentation de la mortalité touche des arbres (feuillus et conifères) de toutes les tailles et différentes essences et à toutes les altitudes. Dans le nord-ouest américain et le sud de la Colombie britannique (Canada), le taux de mortalité dans les vieilles forêts de conifères a même doublé en 17 ans (c'est une fois et demie plus rapide que la progression du taux de mortalité des arbres des futaies californiennes où ce taux a été multiplié par deux en 25 ans). L'accélération de la mortalité a été moindre dans les forêts de l'ouest ne bordant pas le Pacifique (dans le Colorado et l'Arizona), mais « un doublement de ce taux de mortalité finira par réduire de moitié l'âge moyen des arbres des futaies, entraînant une diminution de leur taille moyenne », estime T Veblen qui craint aussi une moindre fixation du CO2 de l'atmosphère et qui appelle à « envisager de nouvelles politiques permettant de réduire la vulnérabilité des forêts et des populations », notamment en limitant l'urbanisation résidentielles dans les zones vulnérables. 
En France, selon les prévision de l'INRA, plusieurs essences ne survivront pas dans la moitié sud de la France et plusieurs ravageurs des arbres pourraient continuer remonter vers le nord. 


VI.4. L'accès à l'océan Arctique
Une diminution des glaces polaires arctiques ouvrirait de nouvelles routes commerciales pour les navires, et rendrait accessibles des ressources sous-marines de pétrole ou de matières premières, mais avec des conséquences néfastes sur nombre d'espèces, comme le plancton ou les poissons à haute valeur commerciale. 
L'accès à ces matières premières en des zones aujourd'hui non accessibles risque d'être source de conflit entre pays côtiers de l'océan Arctique. Ainsi, les États-Unis et le Canada ont-ils protesté lorsque, le 2 août 2007, la Russie planta son drapeau au fond de l'océan sous le pôle Nord. 


VI.5. Économie 
Un rapport de 700 pages de sir Nicholas Stern, économiste anglais, estime que le réchauffement climatique entrainerait un coût économique de 5 500 milliards d'euros en tenant compte de l'ensemble des générations (présente et futures) ayant à en subir les conséquences. 
En 2007, pour la première fois, le World monuments fund (WMF, Fonds mondial pour les monuments) a introduit les modifications climatiques dans la liste des menaces pour 100 sites, monuments et chefs-d'oeuvre de l'architecture menacés, les autres menaces principales étant les guerres et conflits politiques, et le développement industriel et urbain anarchique. 


VI.6. Santé 
Des conséquences des phénomènes climatiques sont redoutées, non seulement sur l'économie, mais également sur la santé publique : le quatrième rapport du GIEC met en avant certains effets sur la santé humaine, tels que « la mortalité associée à la chaleur en Europe, les vecteurs de maladies infectieuses dans diverses régions et les allergies aux pollens aux latitudes moyennes et élevées de l'hémisphère Nord ». 
Les changements climatiques pourront modifier la distribution géographique de certaines maladies infectieuses. Des températures élevées dans les régions chaudes pourraient réduire l'extension du parasite responsable de la bilharziose. Mais le paludisme fait sa réapparition au nord et au sud des tropiques. Aux États-Unis, cette maladie était en général limitée à la Californie, mais depuis 1990, des épidémies sont apparues dans d'autres États, tels le Texas, la Floride, mais aussi New York. Il est également réapparu dans des zones où il était peu fréquent, telles le sud de l'Europe et de la Russie ou le long de l'océan Indien. On constate également que les moustiques et les maladies qu'ils transmettent ont gagné en altitude. 
Sous les climats tempérés, le réchauffement climatique réduirait le nombre de décès induit par le froid ou les maladies respiratoires. Cependant, l'augmentation de la fréquence des canicules estivales augmenterait le nombre de décès en été. Il est difficile de savoir quel sera le bilan global, et si une diminution de l'espérance de vie en découlera. 


VI.7. Une déstabilisation géopolitique mondiale
Selon un rapport de 2003 commandé par le Pentagone et selon un rapport de 2007 de l'UNEP, le réchauffement climatique pourrait entraîner des phénomènes de déstabilisation mondiale, avec des risques de guerre civile.



VII. Réponses des États, collectivités, entreprises, citoyens face à la menace                    climatique
La réalité du risque et du phénomène fait maintenant presque consensus. Nicholas Stern, en 2006, reconnaissait lui-même avoir sous-estimé l'ampleur du problème

« La croissance des émissions de CO2 est beaucoup plus forte que prévue, les capacités d'absorption de la Planète se réduisent et la vitesse des changements climatiques est plus rapide qu'envisagée. »

Face au problème, trois approches se complètent : lutte contre les émissions de GES, puits de carbone, et adaptation.
L'effort international a d'abord visé à réduire le CO2 (gaz à longue durée de vie), alors qu'une action urgente sur les polluants à courte durée (dont le méthane, l'ozone troposphérique et le « carbone noir ») pourrait mieux réduire le réchauffement de l'Arctique. La réduction du CO2 est aussi importante, mais ses effets se feront sentir à plus long terme (après 2 100).
La prospective éclaire les gouvernements, entreprises et individus, qui grâce à la connaissance des tendances générales peuvent prendre des décisions politiques et stratégiques plus pertinentes pour limiter les impacts du changement climatique. 
Les rapports du GIEC sont la principale base d'information et discussions, dont dans le cadre du protocole de Kyoto et de ses suites (Bali, décembre 2007, etc.). L'augmentation prévue de 1,5 à 7 °C pour le siècle à venir, pourrait être moindre si des mesures environnementales sévères étaient prises ou qu'un réel compétiteur aux énergies fossiles émergeait. En dépit des succès dans le secteur des énergies renouvelables, du nucléaire et surtout d'un changement de mode de vie et de consommation, la recherche n'a pas encore offert d'alternative à court terme aux carburants fossiles. Énergie éolienne, énergie hydroélectrique, énergie géothermique, énergie solaire, méthanisation, énergie hydrolienne, pile à combustible, énergie nucléaire, stockage géologique du dioxyde de carbone sont néanmoins en rapide développement. Le gisement d'économies d'énergie - les négawatts - est encore considérable. 
La société civile propose aussi des réponses, notamment via les campagnes et actions de lobbying des ONG et associations locales. 
En France, les ONG de protection de l'environnement et les associations concernées se sont regroupées au sein du Réseau Action Climat (RAC). 
Le réchauffement climatique devrait se traduire par un temps plus instable (vagues de chaleur ou de froid, inondations ou sécheresse, tempêtes et cyclones). De plus, d'après le GIEC, la capacité à s'adapter naturellement de nombreux écosystèmes sera probablement dépassée, causant massivement l' extinction des espèces, par la combinaison sans précédent de : 

changements climatiques provoquant : incendies de forêts, augmentation probable de l'intensité des cyclones, acidification des océans, déplacement des espèces, fonte des glaciers et calottes polaires, impacts économiques et géopolitiques majeurs.
la pression humaine amplifiée par la surpopulation : régression et dégradation des sols (déforestation, barrages, nouvelles cultures et organismes génétiquement modifiés), pollution, surexploitation des ressources.

Par conséquent, afin de contrer les effets et les menaces du réchauffement climatique les mesures à prendre devront concerner :

la réduction des gaz à effets de serre par les gouvernements, les industriels, et les citoyens (réduction ou substitution de l'emploi des sources de carbone) ; 
la réduction de la pression sur les écosystèmes dont la survie de l'homme dépend car il se trouve au sommet de la chaîne alimentaire (pression due aux impacts spectaculaires et aux actions plus discrètes mais multipliées par un très grand nombre d'individus). Des mesures peuvent être prises en vertu du principe de précaution mais sont rendues difficiles à cause de l'action des groupes de pression et des différentes controverses sur le réchauffement climatique. On peut en outre se poser la question de l'avenir du capitalisme, de sa course au profits continuelle, et de notre insatiable appétit de consommation142. 


VII.1. Le Protocole de Kyoto 
La Convention Cadre des Nations unies sur les changements climatiques a été signée en 1992 lors du sommet de la terre à Rio de Janeiro. Elle est entrée en vigueur le 21 mars 1994. Elle a été ratifiée à ce jour par 192 États. Les parties à la convention cadre sur les changements climatiques se sont fixés comme objectif de stabiliser la concentration des gaz à effet de serre dans l'atmosphère à « un niveau qui empêche toute perturbation anthropique dangereuse du climat ». Les pays développés ont comme objectif de ramener leurs émissions de gaz à effet de serre en 2010 au niveau de 1990, cet objectif n'est pas légalement contraignant. 
En 1997, les parties à la Convention cadre sur les changements climatiques des Nations unies (UNFCCC) ont adopté le protocole de Kyoto, dont la nouveauté consiste à établir des engagements de réduction contraignants pour les pays dits de l'annexe B (pays industrialisés et en transition) et à mettre en place des mécanismes dit « de flexibilité » (marché de permis, mise en oeuvre conjointe et mécanisme de développement propre) pour remplir cet engagement. Le protocole de Kyoto est entré en vigueur le 16 février 2005 suite à sa ratification par la Fédération de Russie. 
En juillet 2006, le protocole de Kyoto a été ratifié par 156 États. Les États-Unis et l'Australie (voir infra) ne sont pas signataires. Les États-Unis sont pourtant le deuxième émetteur (20 % des émissions de gaz à effet de serre). Les pays de l'annexe B se sont engagés à réduire leurs émissions de six gaz à effet de serre (CO2, CH4, N2O, SF6, HFC, PFC) de 5,2 % en 2008-2012 par rapport au niveau de 1990. Cet objectif représente en réalité une diminution d'environ 20 % par rapport au niveau d'émissions anticipé pour 2010 si aucune mesure de contrôle n'avait été adoptée. Les objectifs de réduction par pays vont d'une réduction de 8 % pour l'Union européenne à une possibilité d'augmentation de 10 % pour l'Islande. 
Après la victoire des travaillistes aux élections législatives australiennes du 24 novembre 2007, le nouveau premier ministre Kevin Rudd a annoncé avoir ratifié le protocole de Kyoto. 
Des pays en voie de développement fortement contributeurs aux émissions comme l'Inde, 5e émetteur mondial, et la Chine, 1re émettrice, n'ont pas d'objectifs de réduction car ils étaient considérés comme insuffisamment industrialisés et parce que leurs niveaux d'émissions ramenés au nombre d'habitants sont extrêmement faibles. Le mécanisme dit « de développement propre » (MDP), instauré par le protocole de Kyoto, permet aux investisseurs, en contrepartie d'un investissement propre dans un pays en développement, de gagner des « crédits carbone ». Ce mécanisme permet aux pays développés d'avoir accès aux réductions à bas coûts des pays en développement et donc de diminuer le coût de leur engagement. Il permet aux pays en développement de bénéficier d'investissements propres. Il encourage les transferts de technologie. Le MDP apparait cependant insuffisant pour infléchir profondément les trajectoires d'émissions de ces pays. L'absence d'engagement de réduction des pays en développement est une des raisons avancées par les États-Unis pour justifier leur refus de ratifier le protocole. C'est pourquoi un des enjeux majeurs pour la période après Kyoto est de définir des modalités d'association de ces pays à l'effort commun de réduction. 


VII.2. L'UE, pionnière dans la lutte contre les émissions de CO
L'Union européenne reste le 3e pollueur mondial après la Chine et les États-Unis, mais dispose d'atouts pour lutter contre le réchauffement. 
L'UE a lancé en 2005 le marché de permis européen (1er marché de permis contraignant au niveau mondial). La Commission européenne va en 2007-2008 activer son observatoire de l'énergie, restée embryonnaire, et publier (prévu en 2007) un « Livre vert » sur l'adaptation de l'UE au changement climatique, support de débat avant une prise de décision en 2008. La Directive sur le système européen d'échange de droits d'émission sera modifiée en 2008, pour inclure notamment les émissions de l'aviation. La proposition sur les limites d'émission des voitures (120 g de CO2 par km soit 12 kg de CO2 / 100 km147; rappelons que chaque automobile parcourt en moyenne 15 000 km/an) devrait être publiée au second semestre de 2007. La DG Recherche doit proposer en novembre un plan européen, et des propositions de législation sur les piles à combustibles et les avions « propres ». Des appels d'offre sur l'énergie et le climat devraient être publiés avant mi 2007. Le 29 juin 2007, la commission publie et met en consultation un Livre vert sur la question et sur les possibilités d'action de l'UECOM(2007) 354 final). Il prône à la fois l'adaptation et l'atténuation, l'amélioration des connaissances (y compris sur les besoins et coûts d'adaptation - Cf. 7e programme-cadre de recherche de l'UE (2007-2013), l'élaboration de stratégies et d'échanges de bonnes pratiques entre pays, de nouveaux produits assurantiels (« dérivés climatiques », « obligations catastrophe », l'adaptation des marchés européens des assurances (cf. directive « Solvabilité II ») et des fonds « catastrophes naturelles » ainsi que des politiques agriculture et pêche, avec le développement d'une solidarité interne à l'UE et avec les pays extérieurs touchés. 50 millions E sont réservés par la Commission pour 2007-2010 pour favoriser le dialogue et l'aide à des mesures d'atténuation et d'adaptation ciblées, dans les pays pauvres. 
La France a également (juillet 2007) publié une Stratégie nationale d'adaptation au changement climatique et envisagerait une gouvernance adaptée, notamment dans le cadre du Grenelle de l'Environnement. 
L'UE dispose de ressources en éolien terrestre et offshore (déjà 66 % de la puissance éolienne installée dans le monde en 2006, essentiellement au Danemark qui produit ainsi près de 40 % de sa puissance électrique) devant les États-Unis (16 %), l'Inde (8 %) et le Japon (2 %), en technologies solaires et d'un tiers du parc nucléaire mondial. Cela la rend moins dépendante des énergies fossiles que la Chine et les États-Unis. La France, pays le plus nucléarisé, reste cependant loin du record de 1961 où 51 % de son énergie électrique venait du renouvelable (hydroélectrique). 
L'UE encourage aussi tous les acteurs à préparer leur adaptation au changement climatique. 


VII.3. Les États-Unis, pour ou contre Kyoto
Deuxième pays pollueur derrière la Chine, les États-Unis via l'administration de George W. Bush refusèrent de présenter de nouveau en juillet 2005 le traité pour ratification parce qu'ils considèrent que cela freinerait l'économie nationale et que le combat contre le réchauffement climatique doit se faire non pas avec une simple réduction des gaz à effet de serre, mais par une meilleure gestion de leur émission. 
De nombreux États des États-Unis ont néanmoins pris des mesures de restriction sur les gaz à effet de serre. 

VII.3.1. Lutte contre le réchauffement climatique aux États-Unis
Depuis 2001, les États du Texas, de la Californie, du New Hampshire, ont instauré un dispositif de contrôle des émissions de gaz pour différents secteurs industriels et énergétiques. Le dispositif adopté par la Californie, qui s'appliquera à partir de 2009, prévoit réduire les émissions de gaz polluants de 22 % en moyenne d'ici 2012 et de 30 % d'ici 2016. 
En outre, le principe des marchés des permis d'émission consiste à accorder aux industriels « pollueurs » gratuitement, à prix fixe ou aux enchères, des quotas d'émissions de CO2, que ceux-ci peuvent ensuite s'échanger. Chaque émetteur de CO2 doit alors vérifier qu'il détient autant de permis d'émission que ce qu'il va émettre. Dans le cas contraire, il se trouve contraint ou bien de diminuer ses émissions, ou bien d'acheter des permis. Inversement, si ses efforts de maitrise des émissions lui permettent de posséder un excédent de permis, il peut les vendre. 
De tels procédés ont été réalisés pour réduire les pluies acides aux États-Unis et ont connu des succès (programme « Acid rain »). Ce système des marchés de permis d'émission fait partie du dispositif du Protocole de Kyoto qui à la date de juillet 2006[13] n'est toujours pas ratifié par les États-Unis. 
En 2004, le sénateur républicain John McCain et le démocrate Joseph Lieberman déposent un projet de loi visant à limiter les rejets dans l'atmosphère ; soutenu par les grandes entreprises Alcoa, DuPont de Nemours et American Electric Power, il n'est pourtant pas adopté. 
Les États-Unis financent avec la Chine, le Japon, la Russie et l'UE, le projet ITER (International Thermonuclear Experimental Reactor), projet de recherche sur la fusion nucléaire contrôlée, mené à Cadarache (Sud de la France). Toutefois la production nette d'énergie par fusion nucléaire chaude reste à l'état d'espoir lointain : les prévisions les plus optimistes des partisans du projet parlent de plusieurs dizaines d'années. Certains voient plus d'espoir dans la production d'énergie par réactions nucléaires en matière condensée. 
Le 8 juillet 2008, George Bush signe un texte engageant les États-Unis à réduire de moitié des émissions des GES d'ici à 2050, à Toyako (Japon), dans le cadre d'une réunion du G8. 


VII.3.2. Exemples à l'échelle des États et de villes américains
Les décisions pour réduire les émissions de CO2 sont prises par les états fédérés : en 2005, 18 de ces états obligeaient les producteurs d'électricité à utiliser en partie des sources d'énergie renouvelables. 
En 2005, les maires de 136 villes américaines, ont pris l'engagement d'appliquer les normes du protocole de Kyoto et à réduire d'ici 2012 leurs émissions de gaz à effet de serre de 7 % par rapport à 1990. 
L'état du Nevada a pour objectif d'atteindre le seuil de 20 % de sa consommation en énergie renouvelable, d'ici 2015, notamment grâce aux centrales solaires installées dans le désert. 
En outre, à l'initiative du maire de Seattle, 166 grandes villes américaines, dont New York et Boston, se sont engagées solennellement à respecter le protocole de Kyoto en mars 2005. 

VII.3.2.a. Californie 
Alors que la population californienne représente 12 % de la population américaine, elle ne consomme que 7 % de l'électricité produite dans le pays ; ainsi, la Californie se trouve à la première place pour la rentabilité énergétique par personne. L'État s'est engagé à limiter les émissions de gaz à effet de serre : les objectifs annoncés sont une diminution de 11 % avant 2010 et de 87 % avant 2050. Le 30 août 2006, le gouvernement et le Parlement de Californie signent un accord pour diminuer la production de gaz à effet de serre, mettant l'État en conformité avec le protocole de Kyoto. La décision AB32 (Global Warming Solutions Act) a été prise de réduire d'un quart les émissions de gaz à effet de serre d'ici 2020. Des sanctions financières seront prises contre les industries qui ne respectent pas cet engagement. Un marché de permis d'émissions sera créé et contrôlé par l'Air Resources Board. 
La Californie s'est aussi engagée à respecter des règles plus strictes sur la consommation et les pots d'échappement de véhicules neufs ; cette politique est imitée par deux autres États de l'Ouest : Washington et Oregon. Le 20 septembre 2006, Bill Lockyer le ministre de la Justice de Californie, lance des poursuites judiciaires contre trois constructeurs automobiles américains et trois japonais, et leur demande des dommages et intérêts pour la pollution qu'ils engendrent. Selon lui, les véhicules automobiles représentent 30 % des émissions de dioxyde de carbone de l'État. 
En 2005, le gouverneur républicain Arnold Schwarzenegger proposait que le budget de l'État de Californie finance à hauteur de 6,5 millions de dollars la construction de stations pour les véhicules roulant à l'hydrogène. 
Le code d'éducation de la Californie (chapitre IV, sections 8700 à 8784) insiste pour que les élèves soient sensibilisés aux problèmes de l'environnement. 


VII.3.2.b. Énergies renouvelables en Californie
Grâce à son bon ensoleillement, la Californie développe l'énergie solaire : l'État abrite des collecteurs cylindro-paraboliques dont la puissance atteint 80 MW, la plus grande centrale à tour comme Solar one puis Solar 2 ne dépasse pas 10 MW. 
Un projet de loi oblige les promoteurs immobiliers à installer un système d'énergie solaire sur 15 % des nouvelles maisons construites en Californie à partir de 2006. Le projet de loi prévoit que, d'ici 2010, 55 % des maisons seront équipées en panneaux solaires. Le gouverneur Arnold Schwarzenegger avait fait campagne pour inciter à installer des systèmes solaires dans la moitié des maisons de l'État à partir de 2005. 
La centrale thermo-solaire Nevada Solar One est en construction depuis le 11 février 2006 à Boulder City. À terme, elle développera une puissance de 64 MW et sera la troisième du monde. Selon ses concepteurs, la centrale devrait permettre d'éliminer un volume de pollution équivalent à la suppression d'un million de voitures en circulation sur le territoire des États-Unis. 
La Californie a adopté une loi qui contraint les grands groupes automobiles à vendre des véhicules respectant des normes strictes de rejets de CO2. 
La Californie est l'État où l'énergie éolienne est la plus développée avec une capacité de production de plus de 2040 MW installés en 2004, loin devant le Texas (1293 MW). La principale région de production se trouve au nord de l'État, à l'est de San Francisco. 
À 150 km au nord de San Francisco, 19 centrales géothermiques (350 puits) sont contrôlées par la société Calpine dans les comtés de Lake et de Sonoma. Elles produisent environ 850 mégawatts, c'est-à-dire presqu'autant qu'une petite centrale nucléaire. 



VII.4. Nouveaux pays industrialisés contre États-Unis
Un point de débat est à quel degré les nouveaux pays industrialisés tel que l'Inde et la Chine devraient restreindre leurs émissions de CO2. Les émissions de CO2 de la Chine ont dépassé celles des États-Unis en 2007,166 alors qu'elle ne produit que 5,4 fois moins de richesses que l'UE ou les États-Unis, et elle n'aurait dû, en théorie, atteindre ce niveau qu'aux alentours de 2020. En 2007, la Chine est le premier producteur et consommateur de charbon, sa première source d'énergie, qui est extrêmement polluante. De plus, l'augmentation du niveau de vie accroît la demande de produits « énergivores » tels que les automobiles ou les climatisations. 
La Chine a répondu qu'elle avait moins d'obligations à réduire ses émissions de CO2 par habitant puisqu'elles représentent un sixième de celle des États-Unis. L'Inde, également l'un des plus gros pollueur de la planète a présenté les mêmes affirmations, ses émissions de CO2 par habitants étant près de vingt fois inférieures à celle des États-Unis. Cependant les États-Unis ont répliqués que s'ils devaient supporter le coût des réductions de CO2, la Chine devrait faire de même 


VII.5. Mesures individuelles de lutte contre le réchauffement climatique
L'humanité rejette actuellement 6 Gt (gigatonne = milliard de tonnes) d'équivalent carbone par an dans l'atmosphère, soit environ une tonne par habitant. On estime que les océans en absorbent 3 Gt et qu'il faudrait donc abaisser les émissions de gaz à effet de serre de moitié pour arrêter d'enrichir l'atmosphère, ce qui représente une émission moyenne de 500 kg d'équivalent carbone par habitant. Chaque Français en émet environ deux tonnes, soit quatre fois plus qu'il ne faudrait. En dehors de mesures collectives, des personnalités ont esquissé les gestes quotidiens à mettre en oeuvre, dès aujourd'hui, pour limiter le réchauffement climatique comme Jean-Marc Jancovici ou Al Gore. 
Quelques mesures relèvent des économies d'énergie, en particulier des énergies fossiles : 
Éviter de prendre l'avion. Un km en avion long courrier émet 60 g d'équivalent carbone par personne. Un voyage intercontinental représente près des 500 kg d'équivalent carbone. A fortiori, pour les voyages court-courrier (100 g d'équivalent carbone par km et par personne), préférer le train ; 
Utiliser le moins possible les véhicules automobiles (préférer la bicyclette ou les transports en commun chaque fois que possible). Une voiture émet entre 100 et 250 g d'équivalent CO2 par km parcouru, soit entre 30 et 70 g d'équivalent carbone. 20 000 km par an représentent entre 600 et 1 400 kg d'équivalent carbone. Si une automobile est nécessaire, choisir le modèle le moins polluant et le plus efficace possible (par exemple, certains constructeurs ont annoncé des véhicules consommant moins de 1,5 L/100 km176,177) ; 
Atteindre une isolation optimale des bâtiments, au mieux par le recours à l'architecture bioclimatique qui réduit au maximum les besoins de chauffage (15 kWh/m2/an, les anciennes maisons étant à 450 kWh/m2/an) et supprime le besoin de climatisation active, tout en améliorant le confort de vie. 




<TXT=w-Scientologie10>

I. Histoire 

I.1. La fondation par Ron Hubbard
Lafayette Ronald Hubbard (13 mars 1911, 24 janvier 1986), mieux connu sous le nom de Ron Hubbard, était un auteur de science-fiction et fantasy américain . 
Le premier article de Ron Hubbard sur la dianétique parut en mai 1950 dans le magazine Astounding Science-Fiction dont il était un auteur habituel. L'article avait été annoncé depuis plusieurs mois par le rédacteur en chef John W. Campbell qui le présentait comme un travail scientifique important 3. 
En parallèle paraissait, le 8 mai 1950, le livre Dianétique : la science moderne de la santé mentale. Ron Hubbard y déclare avoir identifié la source des maladies psychosomatiques, après plusieurs années de recherches personnelles. Pour lui, la dianétique correspond à une approche scientifique et rationnelle de la psychologie. 
La méthode de dianétique connut un succès rapide. Dès juillet, le livre était un best-seller et des « clubs de dianétique » se créèrent un peu partout aux États-Unis pour expérimenter la méthode d'audition décrite par Hubbard. À ce moment, l'association psychiatrique américaine exigea que la dianétique soit soumise à une enquête scientifique. 
En 1952, Hubbard élargit la dianétique en une philosophie laïque qu'il appela « scientologie » et la déclara comme une religion en décembre 1953, date à laquelle la première église de scientologie fut fondée à Camden au New Jersey. 


I.2. Expansion et premières controverses
À la fin des années 1950, l'Église de scientologie s'implante progressivement dans d'autres pays : en Nouvelle-Zélande, en Afrique du Sud puis en France, à Paris, en 1959. 
En 1958, le fisc américain remit en cause le statut religieux de la scientologie. 
Hubbard s'établit à cette époque en Grande-Bretagne et, en 1959, acheta le manoir géorgien de Saint Hill, situé près de la ville d' East Grinstead au Sussex, qui devint alors le siège mondial de la scientologie. 
La scientologie devint le sujet de controverses dans le monde anglophone vers le milieu des années 1960. Dans l'État de Victoria en Australie, après la constitution d'un rapport sur les activités du mouvement, une loi sur les pratiques psychologiques mène à l'interdiction de la scientologie dans cet État en 1965. Deux autres États australiens feront de même, mais ces lois furent déclarées inconstitutionnelles en 19695. 
À la même époque, la Grande-Bretagne tenta d'interdire l'accès du pays aux scientologues étrangers et donc l'accès au centre de formation du siège international après un rapport de la Chambre des communes britannique critiquant les méthodes psychothérapeutiques de la scientologie et la considérant comme nuisible à la société et à la santé des individus. 
La Nouvelle-Zélande, l'Afrique du Sud et la province de l'Ontario au Canada menèrent également des enquêtes publiques sur les activités de la scientologie. 


I.3. Réorganisations du mouvement 
À partir de 1966, Hubbard commença à se désengager de la direction du mouvement. Il démissionna en 1967 du poste de directeur exécutif, et fonda la « Sea Organisation » ou « Sea Org », qui devint le groupe de gestion internationale de la Scientologie. L'office du gardien fut fondé en 1966. 
La Sea Org fut jusqu'en 1975 une organisation maritime dirigée par Hubbard ; à cette date elle s'installa à Clearwater en Floride, qui devint le nouveau siège mondial de l'Église de scientologie. 
En 1977 éclata aux États-Unis l'« affaire Snow White », révélant une opération montée par l'Église pour purger les dossiers défavorables sur la scientologie et son fondateur L. Ron Hubbard. Ce projet incluait une série d'infiltrations et de vols de 136 organismes gouvernementaux, ambassades et consulats, ainsi que d'organismes privés critiques à l'égard de la Scientologie, réalisée par des membres de l'Église dans plus de trente pays. 
À cette occasion, le FBI découvrit des dossiers que l'Église constituait sur ses ennemis potentiels,7, afin de les mettre hors d'état de lui nuire. 
Onze cadres haut placés de l'Église furent inculpés. Ils plaidèrent coupables ou furent reconnus comme tels par la Cour fédérale pour les délits d'obstruction à la justice, de cambriolage de bureaux du gouvernement, et de vol de documents et biens de l'État, et furent condamnés à des peines de quatre à cinq ans de prison, et à des amendes de 10 000 dollars 8. 
En 1978, trois dirigeants du siège français, ainsi qu'Hubbard, furent convaincus d'escroquerie par le tribunal de Paris, lors d'un procès qualifié de « procès du siècle » par l'Église. 
Suite à l'affaire Snowhite, l'Office du gardien fut supprimé, et les scientologues impliqués dans l'affaire furent démis de leurs responsabilités dans l'organisation. Le mouvement entreprit alors une restructuration : en 1981 est créée l'Église scientologue internationale, qui est l'organe de direction, et en 1982 le Centre de technologie religieuse, sa structure religieuse, à qui Hubbard céda ses droits sur les marques scientologues. 


I.4. Statut religieux et risque sectaire
Dans les années 1980, après la tragédie de Guyana et les plaintes des associations de familles concernant l'Église de l'Unification, les États-Unis puis les États européens s'inquiétèrent des dangers des « nouveaux mouvements religieux » . 
De nouveaux rapports étudiant les risques sectaires de divers mouvements furent alors produits, comme le rapport Vivien en France, qui pointait des pressions morales endettant les adeptes de la scientologie. 
En 1993, la situation changea aux États-Unis où l'Église de Scientologie fut reconnue comme religion par les services fiscaux 12. 
Cependant, en France, le rapport parlementaire de 1995 qui dressait une liste indicative des sectes très controversée y désignait la scientologie comme une secte. Un rapport de 1999 de la MILS la classait comme secte « absolue » et recommandait sa dissolution. 
Un rapport de la Chambre des représentants belge (proposition de loi de deux parlementaires) n° 313/7-95/96 en date du 28 avril 1997 la classe comme faisant partie des mouvements sectaires nuisibles. 
En République fédérale d'Allemagne également, une commission d'enquête parlementaire fédérale sur les « sectes et psychogroupes » la considère comme un « mouvement politique extrémiste » dont la forme de pensée totalitaire est explicitement rapprochée de la pensée nazie. 
Les États-Unis réagirent en 1999 en publiant un « rapport sur la liberté religieuse dans le monde » très critique envers ces politiques nationales. 
Un rapport du parlement européen notait en 1997 que le reproche le plus répété envers la Scientologie est de tenter de noyauter les organes de l'État, mais qu'il n'avait pu être réellement prouvé. Il concluait que dans la plupart des pays d'Europe, les modifications de l'arsenal juridique n'étaient pas nécessaires. C'est le point de vue qui fut adopté par la plupart des pays, qui choisirent de ne pas créer de législation spécifique, mais de ne traiter le phénomène sectaire qu'au travers de l'atteinte à l'ordre public. 



II. Croyances et pratiques

II.1. Doctrine
Dans ses écritures, la Scientologie se donne comme but : « Une civilisation sans folie, sans criminel et sans guerre, dans laquelle les gens capables puissent prospérer et les gens honnêtes puissent avoir des droits, et dans laquelle l'homme soit libre d'atteindre des sommets plus élevés. »17. La Scientologie se considère comme une « philosophie religieuse appliquée ». En d'autres termes elle se présente comme une religion, tout en offrant des « solutions pour les problèmes » de ses adeptes. La Scientologie, qui se définit elle-même comme une religion traditionnelle, se fonde sur la croyance selon laquelle l'homme a été créé pour travailler à son propre salut spirituel, et que ce n'est que dans cette optique qu'il peut comprendre pleinement sa relation avec Dieu. La Scientologie affirme aussi que l'homme est fondamentalement bon mais qu'à cause de son mental réactif, source d'irrationalité, il peut être conduit à agir de manière mauvaise. Ainsi, son « salut spirituel » dépendrait de sa relation avec lui-même, avec ses semblables et du fait d'arriver à une « fraternité avec l'univers ». La Scientologie affirme donner à l'individu le moyen de résoudre par lui-même ses problèmes, mettre de l'« ordre dans sa propre vie » et également lui permettre d'aider efficacement les autres. Le résultat obtenu se manifesterait par des progrès concrets visant à débarrasser la société de ce que la Scientologie estime être ses fléaux (les drogues, l'illettrisme, le crime, la violence et l'intolérance). Les Églises de Scientologie constitueraient alors, selon elles-mêmes, des points centraux, dont émaneraient programmes et activités. 
La scientologie considère que la motivation fondamentale de la vie est la survie, elle-même étant située sur une échelle graduée allant de la mort à l'immortalité potentielle. Cette motivation est appelée la dynamique. Cette dynamique ou impulsion fondamentale se sépare en 8 dynamiques (symbolisées par la croix à 8 branches de la scientologie). L'homme aurait une impulsion à survivre sur chacune d'entre elles. 

La première dynamique serait l'impulsion à survivre en tant qu'individu. 
La seconde dynamique l'impulsion à survivre à travers le sexe, la procréation et la famille. 
La troisième dynamique l'impulsion à survivre en tant que groupe ou en tant que groupes. 
La quatrième l'impulsion à survivre en tant qu'espèce ; en l'occurrence l'humanité. 
La cinquième est l'impulsion à faire survivre la totalité des formes de vie : plantes, animaux, insectes, etc. 
La sixième est l'impulsion à survivre en tant qu'univers matériel (les énergies, la matière, les rochers, les planètes font partie de cette impulsion). 
La septième dynamique est l'impulsion à survivre en tant qu'esprits (la scientologie considère que la personne est un esprit « habitant un corps » et n'est ni son corps ni son mental). 
La huitième dynamique étant la dynamique de l'être suprême. 

Toutes ces dynamiques sont des divisions arbitraires de la dynamique fondamentale qui les englobe toutes.22.


II.2. Finalité 
Selon les critères de la Scientologie, la vie des scientologues est censée s'améliorer graduellement. De même, les scientologues affirment « progresser spirituellement » petit à petit. Ron Hubbard affirmait avoir développé une voie précise d'étude religieuse, consistant en une série d'étapes progressives et payantes, effectuées dans une séquence précise qui aiderait l'individu à atteindre un état d'« existence très élevée ». Cette « ascension » en scientologie permettrait à l'individu de comprendre de mieux en mieux ce que la Scientologie estime être « la nature spirituelle de l'homme » et sa relation avec ce que ce mouvement pense être « l'Être suprême » (à définir). Les adeptes sont censés « améliorer leur vie » grâce à la Scientologie. Ils seraient plus heureux et auraient des relations et une vie de famille plus « positives », et ils réussiraient mieux dans leur travail. Et à leur tour, grâce au prosélytisme scientologiste, ils apporteraient leur contribution à la société en « améliorant les conditions de vie ».


II.3. L'étude des textes
L'étude des ouvrages de Ron Hubbard fait partie de la vie des scientologues, qu'ils participent à des services religieux dans une église de scientologie ou qu'ils poursuivent leur étude des « Écritures » de la scientologie à domicile. (la collection complète des livres de R. Hubbard coûte 3175E 23) Ils les étudient dans toutes les Églises de scientologie, sous la surveillance de superviseurs de cours, et sont encouragés à continuer leur étude de la religion à domicile, en suivant des cours par correspondance par exemple.


II.4. La dianétique

II.4.1. L'audition
L'audition constitue, aux yeux des scientologues, une « technologie spirituelle » dont l'application permet d'« améliorer sa propre condition et son existence, ainsi que celles des gens de son entourage ». Selon les scientologues, l'audition constitue la pratique essentielle de la scientologie. Elle peut être dispensée à des groupes de personnes lors d'un office dominical ou lors d'autres rassemblements religieux, ou à une seule personne lors de séances dirigées par un ministre de scientologie, l'« auditeur ». Quand l'audition est donnée individuellement, l'auditeur, par des questions répétées, aiderait la personne auditée à examiner un moment particulier de son existence. Celle-ci deviendrait plus heureuse, plus confiante, plus consciente, plus maîtresse de sa vie. L'auditeur utilise un « électropsychomètre » ou électromètre qui est supposé pouvoir mesurer l'état ou les changements d'état spirituel de la personne. L'électromètre permettrait également à l'auditeur d'aider la personne à localiser des domaines de détresse ou d'angoisse. Lorsque ces moments sont localisés et examinés par la personne auditée, ceux-ci cesseraient d'avoir une influence indue sur leur vie présente et la personne y gagnerait soulagement et autodéterminisme. L'audition est aussi dispensée à des groupes lors d'offices du Dimanche et lors d'autres rassemblements, selon le même principe : l'auditeur dirige les activités de toute l'assistance. L'église de scientologie affirme augmenter considérablement le niveau de communication, de conscience et d'aptitude de tous les membres de l'assistance. L'audition de groupe permettrait aux participants de bénéficier gratuitement et régulièrement des avantages de l'audition. En fait, l'audition de groupe est gratuite lors des offices du dimanche, et peut être payante en dehors de ce cadre. 
Arnaud Palisson dans sa thèse de l'université de Cergy-Pontoise en droit pénal a tenté de démontrer que le caractère extrême de certains exercices place la scientologie à la limite de la légalité (exercice illégal de la médecine, droit de l'enfance, code du travail, etc.). Pour le psychiatre Louis Jolyon West, cette technique est surtout une forme d'hypnose apte à faire entrer le sujet en transe 25. Certains détracteurs soutiennent que l'Église de Scientologie, contrairement aux psychothérapies, à la médecine ou à la confession catholique n'a pas de règles déontologiques lui interdisant d'exploiter les secrets que ses adeptes ont confiés lors d'auditions ou en remplissant des questionnaires. L'organisation dément ces accusations et affirme considérer les communications entre un ministre de l'église et un paroissien comme sacro-saintes, ceci étant stipulé par le « Code de l'auditeur » qui régit toute séance d'audition. 
À l'inverse, les universitaires spécialistes des religions ayant étudié la scientologie, comme Bryan Wilson et Régis Dericquebourg, décrivent l'audition comme une forme de conseil pastoral permettant une plus grande compréhension de sa spiritualité et de sa relation avec l'être suprême. Ils y voient une sorte de « méthodisme » spirituel.28 


II.4.2. Les niveaux avancés

II.4.2.a. État «Clair»
Dans la dianétique et la scientologie, l'état Clair correspond à une condition dans laquelle une personne n'est plus soumise à des influences non souhaitées provenant de souvenirs passés, ni à des émotions et épisodes traumatiques passés. Une personne Claire est donc « débarrassée de ces influences néfastes ». 
Le fondateur, L. Ron Hubbard, définit le stade de « Clair » comme celui d'une personne qui n'aurait plus son propre « mental réactif »29 et qui ne souffrirait donc plus des effets que ce mental réactif peut causer. Ce stade de Clair serait atteint aux moyens de l'« audition » en dianétique et scientologie. 
Un Clair serait rationnel dans ce sens qu'il formerait les meilleures solutions possibles à partir des données qu'il détient et de par son propre point de vue. Le Clair n'aurait donc plus d'« engrammes ».30 Ceux-ci, lorsqu'ils sont « restimulés », feraient dévier la justesse des raisonnements en y faisant entrer des données fausses et cachées. Le Los Angeles Times rapporte que L. Ron. Hubbard a écrit qu'il y a 75 millions d'années notre univers fut organisé en une Confédération galactique de 76 planètes, dirigée par le tout-puissant seigneur galactique Xenu. Xenu aurait fait expédier sur Terre par fusées - ressemblant à des DC-8 - les 13,5 trillions d'habitants, les aurait mis dans des volcans, aurait fait exploser d'énormes bombes H et aurait ensuite soudé ensemble ces âmes désincarnées, qu'il appelle des « thétans de corps ». Les humains ne seraient donc pas seuls dans leur corps mais seraient composés d'eux-mêmes et de milliers de ces âmes parasites. Par ailleurs, il faut environ 200 000 euros pour apprendre à se débarrasser de ces « thétans de corps» . D'après un opposant s'appuyant sur un témoignage, l'histoire sur l'origine de l'humanité aurait été conçue à une époque à laquelle Hubbard aurait consommé beaucoup de drogue. 
L'histoire de Xenu ferait partie des doctrines secrètes qui seraient enseignées seulement aux membres avancés de l'Eglise. Cette dernière n'évoque pas ouvertement ce mythe et va parfois jusqu'à nier son existence dans la pensée scientologue. Naturellement, les personnes qui croient à cette cosmogonie sans être des adeptes de la scientologie sont très rares. Il convient ici de rappeler qu'avant de fonder la Scientologie, Hubbard était un écrivain de science-fiction. L'auteur américain de science-fiction et patron de presse Lloyd Eshbach, raconte que Hubbard lui aurait dit : « J'aimerais créer une religion. C'est là où se trouve l'argent ». L'Église de Scientologie réfute cette affirmation et soutient sur son site qu'« une fausse attribution fut reconnue par des tribunaux de justice en Allemagne ». 



II.5. Les offices
Tous les dimanches, l'aumônier conduit des services du culte ouverts à « tous les individus qui partagent l'espoir que l'homme vivra mieux et sera plus heureux dans le futur ». L'aumônier célèbre aussi des mariages, des baptêmes et des funérailles pour ses adeptes et les membres de leur famille. 
Dans les pays appliquant une séparation des églises et de l'état, dans les états laïques (comme la France, ou la Belgique), ou dans les pays ayant une religion officielle, ces cérémonies n'ont pas de valeur légale. 


II.6. Les fêtes
De nombreuses fêtes scientologiques (en) ponctuent l'année civile, parmi lesquelles :

25 janvier : Criminon Day commémore la fondation en 1970 du programme scientologue Criminon, qui vise à aider à la réhabilitation des prisonniers par la diffusion de copies libres d'ouvrages scientologistes tels que The Way to Happiness (Le chemin du bonheur). 
19 février : Narconon Day 
22 février : Celebrity Day (Fête de la célébrité) célèbre l'anniversaire de l'ouverture du Celebrity Centre International à Los Angeles en 1970, dédié à la réhabilitation de la culture au travers de l'art. 
5 mars : CCHR Day célèbre la Citizens Commission on Human Rights (Commission citoyenne sur les Droits de l'Homme). 
13 mars : Anniversaire de L. Ron Hubbard, fondateur de la Dianétique et de la Scientologie, est né le 13 mars 1911. 
24 mars : Student Day (Fête des étudiants) célèbre l'ouverture du cours spécial de Saint Hill en 1961. 
20 avril : L. Ron Hubbard Exhibition Day (Fête de l'exposition L. Ron Hubbard) célèbre l'ouverture en 1991 du L. Ron Hubbard Life Exhibition à Hollywood, en California. 
9 mai : Anniversary of Dianetics (Anniversaire de la dianétique) 
25 mai : Integrity Day (Fête de l'Intégrité) est une journée de réflexion sur l'étude réalisée en 1965 par Ron Hubbard concernant l'éthique scientologue. 
6 juin : Maiden Voyage Anniversary (Anniversaire du voyage inaugural du Freewinds) 
18 juin : Academy Day (Fête de l'Académie) 
12 août : Sea Org Day 
4 septembre : Clear Day célèbre l'inauguration de Hubbard's Clearing Course, en 1965. 
Deuxième dimanche de septembre : Auditor's Day (Fête des Auditeurs) 
7 octobre : Anniversaire de l'Association internationale des Scientologistes (AIS ou IAS en anglais) 
27 novembre : Publications Day (Fête des Publications) commémore la journée Publications Worldwide (Publications mondiales) organisée à Saint Hill Manor en 1967. 
7 décembre : Flag Land Base célèbre l'ouverture du Flag Land Base à Clearwater (Floride) en 1975. 
30 décembre : Freedom Day (Fête de la Liberté) célèbre la reconnaissance officielle de l'Église de Scientologie aux États-Unis en 1974. 
31 décembre : Réveillon du Nouvel An. 
Il existe aussi des fêtes scientologiques locales qui célèbrent l'implantation de l'Église de Scientologie en différents lieux : 
28 janvier : fondation de l'Église de Scientologie en Nouvelle-Zélande, en 2009. 
31 mars : fondation de l'Église de Scientologie à Vienne ( Autriche) en 1971. 
23 septembre : ouverture de la première École de Dianétique et de Scientologie à Tel Aviv ( Israël) en 1980. 




III. Dans le monde aujourd'hui 
Née dans le monde anglophone, la scientologie a aujourd'hui une envergure mondiale. En 2008, elle essaye de s'étendre dans toute l'Afrique (l'Afrique du Sud notamment, où l'Église de scientologie est reconnue comme association d'utilité publique depuis le 3 décembre 2007). 
Le statut juridique de la Scientologie diffère selon les pays où elle est installée : elle se présente selon les États comme une organisation commerciale, une religion, un centre culturel de dianétique ou bien une simple technique de développement personnel. 
Frédéric Lenoir soulignait en 1998 la différence de statut entre certains pays d'Europe dont la France, où l'organisation était listée comme « secte absolue », et celle en Amérique du Nord, où l'Église de Scientologie a été reconnue comme religion par les services fiscaux des États-Unis en 1993,40, et où s'affichent parmi ses membres des personnalités telles que les acteurs John Travolta et Tom Cruise, les actrices Juliette Lewis et Catherine Bell, sans oublier Katie Holmes ainsi que les musiciens Isaac Hayes, Chick Corea et Beck... Ces Very Important People bénéficient d'un traitement à part au sein de l'organisation qui a créé pour eux des Celebrity centers. 
En septembre 2004, Nicolas Sarkozy, alors ministre du Budget, avait reçu à Bercy de façon très médiatisée l'acteur Tom Cruise, scientologue avéré. Durant des interviews pour l'émission "90 minutes" diffusée le 31 mai 2005 sur la chaine française canal+, Claude Guéant déclare que la scientologie n'est pas un danger pour les personnes et Nicolas Sarkozy ne peut se décider si la scientologie est une secte ou non. En février 2008 « Emmanuelle Mignon, alors directrice de cabinet du président de la République, [...] s'était interrogée sur la pertinence de qualifier ce mouvement de "secte" »46 

III.1. Obtention de statut religieux
L'Église de scientologie chercherait à être reconnue comme une religion et effectue des demandes en ce sens dans de nombreux pays, avec des résultats divers. En Suède, au Portugal et au Venezuela, elle a été reconnue officiellement comme une religion. 
Dans plusieurs pays, l'organisation a fait appel à la justice pour obtenir cette reconnaissance. En Espagne, en 2005 le ministère de la Justice avait refusé l'inscription de l'Église de Scientologie au registre des associations religieuses. En novembre 2007, après deux ans d'instruction, l'Audience Nationale, haute instance juridique espagnole, a obligé le ministère de la Justice à enregistrer l'Église sur le registre des associations religieuses, conférant à cette dernière les mêmes privilèges qu'aux catholiques.50,51 
Suite à cette décision de l'Audience nationale, la justice espagnole a inscrit la scientologie au registre légal des religions le 19 décembre 2007. 52 La ville de Moscou refusant d'enregistrer l'Église comme association religieuse, la Cour européenne des droits de l'homme a considéré, à l'unanimité, qu'il s'agissait d'une violation de l'article 11 (liberté de réunion et d'association) de la Convention européenne des droits de l'homme combiné avec l'article 9 (liberté de pensée, de conscience et de religion) 53. D'après l'arrêt, « Les autorités n'ont pas agi de bonne foi et ont manqué à leur devoir de neutralité et d'impartialité envers la communauté religieuse représentée par l'Église requérante ». 
L'organisation scientologue a également obtenu le droit de célébrer les mariages dans plusieurs pays ce qu'elle considère comme une reconnaissance officielle de ce qu'elle serait une religion à part entière notamment en Afrique du Sud en Australie, en Inde, en Italie, au Mexique, en Nouvelle-Zélande, à Taïwan, en Tanzanie et au Zimbabwe. 


III.2. Association sous surveillance
En Suisse, plusieurs tribunaux lui ont refusé l'appellation « religion » et l'ont désignée comme exclusivement commerciale. 
En Grande-Bretagne, les services fiscaux considèrent depuis 2000 l'Église de scientologie comme une organisation sans but lucratif et l'ont exemptée de TVA. Suite à cela, en 2006, un tribunal a tranché en faveur de l'Église face aux services fiscaux qui refusaient de rembourser la TVA versée de manière indue depuis 1977.59 
La situation de la scientologie présente des caractéristiques assez similaires en France et en Belgique, où elle est régulièrement qualifiée de secte depuis les travaux parlementaires respectivement en 1995 et 1997, sans que ce terme ne renvoie à une définition légale précise; et où elle est également poursuivie en justice dans des procès pour escroquerie durant depuis une dizaine d'années. En France les différentes organisations de scientologie ont le statut d'association à but non lucratif, certaines se déclarent association cultuelle mais n'ont pas demandé à bénéficier des avantages attachés à ce statut 60 définis par la loi de 1905; de même elles sont constituées en associations sans but lucratif en Belgique. 
En 1997, la Cour d'appel de Lyon a considéré que, pour juger des faits qualifiés d'escroqueries ou de complicités d'escroqueries, commis dans le cadre d'activités de scientologues, « il est vain [...] de s'interroger sur le point de savoir si l'Église de Scientologie constitue une secte ou une religion ». 
Suite à des perquisitions effectuées en 1999 dans le cadre d'un procès pour escroquerie, l'Église de Scientologie a fait appel en mars 2001 à l'intervention des Nations unies pour intervenir dans ce qu'elle considère comme une « campagne d'intimidation et de harcèlement » que les autorités belges lui feraient subir, en prenant pour cibles ses paroissiens belges et son bureau européen des Droits de l'Homme 62. Dans ce même procès, le parquet fédéral a annoncé en septembre 2007 le renvoi devant la justice de douze personnes physiques et de deux personnes morales : l'ASBL « Église de Scientologie de Belgique » et le « Bureau des droits de l'homme de l'Église de Scientologie » . 
En Allemagne, considérée comme une secte en 1997, l'Église de Scientologie inquiète le gouvernement fédéral qui l'accuse de chercher à « exercer une influence totalitaire sur les institutions et la société 64 ». Elle fut déchue par le gouvernement fédéral de son statut de communauté religieuse et placée sous la surveillance de l'État, celui-ci jugeant que certaines activités de l'organisation de la Scientologie pouvaient porter atteinte à la démocratie et aux droits de l'homme. Considérée comme une entreprise commerciale, toute mesure fiscale en sa faveur furent interdites et elle fut assujettie au régime des entreprises commerciales 65. 
En 1999, la Dianetic Stuttgart eV, sous-organisation de l'Église de Scientologie, est reconnue par la Cour administrative de Stuttgart comme une association à but idéaliste et non comme une entreprise commerciale. 
En janvier 2003, le Bureau fédéral des finances a accordé aux neuf églises de Scientologie une exonération sur l'argent donné pour soutenir l'Église de Scientologie internationale (l'église mère basée à Los Angeles 67). Ces exemptions fiscales partielles sont liées à sa reconnaissance d'utilité publique aux États-Unis. En effet, il existe un accord entre les deux pays qui permet d'éviter la double imposition 68. La Scientologie n'est pas, pour autant, reconnue comme religion en Allemagne où elle fait toujours l'objet de surveillance de la part des services de renseignement. 
Le 11 novembre 2004, le Tribunal administratif de Cologne a rejeté un recours de la Scientologie demandant la fin de cette surveillance. À ce sujet, le 24 mars 2004, 3 scientologues perdent un procès intenté contre l'Allemagne devant l'ONU 70. 
Le 13 janvier 2007, l'ouverture d'une filiale de l'Église de Scientologie de 6 étages sur 4000 m² à Berlin suscite de vives réactions de la part du gouvernement et de la CDU, le parti de la chancelière Angela Merkel, concernant la politique du Sénat berlinois vis-à-vis des sectes. Le Land de Berlin est le seul des seize Länder à avoir refusé, depuis 2003, de surveiller les agissements de l'Église de Scientologie sur son territoire. 
En 2008, la conférence des ministres de l'Intérieur de l'État et des Länder avait demandé à l'organisme chargé de la protection de la constitution une enquête sur la scientologie pour déterminer si elle représentait une menace à la constitution allemande. Dans son rapport de 46 pages remis aux ministres de l'intérieur, les protecteurs de la constitution ont parlé d'une « image de la situation pleine de lacunes » véhiculée par les anti-scientologues. Ils mettent en garde contre une « perte de réputation pour les organismes gouvernementaux concernés » lors d'une procédure contre la scientologie, et concluent que les statuts ni les autres prises de position de la scientologie « ne permettent de conclure que l'association poursuit des buts répréhensibles ». 



IV. Organisation

IV.1. Église de scientologie internationale
L'Église de scientologie internationale est depuis 1981 l'organe de direction de la scientologie. Les Églises implantées dans les différents pays du monde sont rattachées à elle.


IV.2. Centre de technologie religieuse
Le Centre de technologie religieuse (Religious Technology Center ou RTC) est présenté comme la structure ecclésiastique de la scientologie. Il a été créé en 1982 et est dirigé depuis 1987 par David Miscavige.


IV.3. Office des affaires spéciales
L'Office des affaires spéciales (Office of Special Affairs ou OSA) est le service de communication de la scientologie. 
Il succède à l'Office du gardien, impliqué en 1979 dans le scandale de l'opération Blanche Neige, une tentative d'infiltration du gouvernement américain pour laquelle plusieurs scientologues furent condamnés à des peines de prison 73. 
Selon les organisations de lutte contre les sectes 74, il serait le bureau officiel de la propagande scientologiste et constituerait même des fichiers. 


IV.4. La nébuleuse scientologue
Une des caractéristiques de l'église de scientologie est l'extrême diversification de ses activités : écoles de dessin, de musique ou de management, groupes de pression à but humanitaire.

Éthique et liberté, un journal76 
Le Bureau des droits de l'homme de l'église de scientologie, la Commission des citoyens pour les droits de l'homme et Des jeunes pour les droits de l'Homme 
Narconon, un programme de lutte contre la drogue 
Le Collège Hubbard d'Administration International Diplômes de commerces et d'administration reconnues par l'état américain77 
L'école de l'éveil 
La Commission d'enquête Permanente sur les violations des droits de l'Homme 
La Coordination des Associations et des Particuliers pour la Liberté de Conscience 
le Centre Français des scientologues contre la discrimination 
Espoir d'un futur 
l'Association de l'étude de la nouvelle foi 
Criminon 
le Groupement pour l'Amélioration des Méthodes Éducatives 
Wise, (World Institute of Scientology Enterprises) regroupement de sociétés scientologues. 

Certains de ses détracteurs considèrent que l'Église de Scientologie fait feu de tout bois et change de statut en fonction des protections légales que cela lui offre : elle est une organisation commerciale lorsqu'elle peut attaquer pour violation de copyright ceux qui publient ses ouvrages confidentiels 78, elle est une religion dans les pays où la liberté de culte permet tout, elle devient technique de développement personnel lorsqu'il s'agit d'approcher les entreprises ou dans les pays qui se défendent contre les sectes. Elle déclare aussi être une Association ou Centre culturel de dianétique (en Argentine, Colombie et Espagne), un Collège Hubbard d'indépendance personnelle (en Écosse), un Centre culturel de dianétique ou d'Amérique latine (au Mexique), et un Institut de philosophie appliquée ou de technologie de dianétique (au Mexique). 
Souvent, les noms choisis par l'organisation ressemblent aux noms d'autres organismes publics ou privés : pour les opposants à la Scientologie, ces noms sont destinés à faire passer les organismes scientologues pour des services officiels ou pour des organismes aux noms semblables et déjà connus du grand public, et ces activités sont soupçonnées de servir de tremplin pour le recrutement de nouveaux adeptes, qui s'engageraient dans une activité en ignorant les liens entre la Scientologie et ces organismes. 
Dans une vidéo officielle, l'Église de Scientologie joue de ces ambigüités en présentant des personnes qui la soutiennent. Les légendes incrustées aux images mentionnent (sans jamais donner un nom) des fonctions vagues telles que « membre du cabinet du maire de Marseille » ou « Conseil des communautés européennes » 



V. Controverses 

V.1. Le coût des « services »
Chaque centre ou Église de Scientologie, nommé en interne « org (-anisation) », met à la disposition de ses adhérents de nombreux services et ouvrages considérés comme des livres à caractère religieux selon les adeptes : cinq cent mille pages, trois mille conférences enregistrées, et une centaine de films, tous attribués au fondateur de la scientologie, Ron Hubbard. Les cours et ouvrages fournis constituent un ensemble de méthodes, censées permettre de devenir plus « apte », de méthodes pour faire survivre la Scientologie en tant que mouvement et de mythologie propre a la Scientologie (dont les textes sur Xenu qui coûteraient près de 50 000 euros[réf. nécessaire], et dont des versions sont diffusées sur Internet ou Usenet principalement par des détracteurs mais aussi par des défenseurs de la Scientologie). 
Récemment, l'Église a entièrement réédité ses ouvrages car la grande majorité avait été altérée et rendue incompréhensible par les éditeurs (erreurs de transcription, de traduction, de ponctuation, pages mélangées, chapitres mélangés, paragraphes ajoutés/supprimés/mélangés, etc.). Le travail sur la réédition des ouvrages a été entamé en 2000 et a été annoncée mondialement le 14 juillet 2007 lors d'un event (fête ou littéralement événement) vidéo où les nouveaux livres ont été vendus pour la première fois. Un test aurait été effectué sur des scientologues qui ont lu les premiers les livres réédités. Le temps de lecture des livres s'est réduit de plusieurs jours en moyenne pour les anciens livres incompréhensibles considérés comme ardus par les scientologues eux-mêmes à quelques heures pour les livres récemment réédités et retraduits. Le prix des livres varie entre 12 et 30 euros.81 
La critique la plus commune est que la scientologie ferait miroiter à ses adeptes la possibilité d'atteindre un état qu'ils n'atteindront jamais, se contentant d'une progression perpétuelle. Les adeptes, pour passer d'un grade à un autre, doivent suivre des stages et acheter des documents. À titre d'exemple pour un premier contact, une série de 15 CD (conférences d'une heure par CD) datant de 1956, enregistrée en pleine guerre froide, Le Congrès de Washington sur les radiations et la confrontation est vendue actuellement environ 200 euros. Nombre de témoignages montreraient la surprise, une fois passé un niveau, de s'entendre dire que ce n'était qu'une étape alors que ce devait enfin être l'état d'illumination attendu. Le coût financier de l'étape suivante serait bien plus élevé. 
D'après une enquête du mensuel Capital de mai 2009, l'Eglise de scientologie incite aussi ses adeptes à acheter toute une série de produits et services à prix prohibitifs comme l'électromètre -qui n'est en fait qu'un ohmmètre- (3800 euros), le stage de purification (1464 euros) ou l'heure d'audition (jusqu'à 400 euros). Selon Gilles Tanguy, le journaliste de Capital qui s'y est inscrit pour les besoin de l'enquête, chaque Eglise de scientologie fait ses comptes le jeudi. Et "la paie des vendeurs dépend directement" du chiffre d'affaires qu'ils ont facturé aux adeptes. D'après un site antiscientologue, le coût financier imposé à une personne pour atteindre le niveau « OT8 », le plus élevé dispensé actuellement, pourrait être estimé grossièrement (2005) à 400 000 euros. 


V.2. Le rejet d'une forme de médecine
La Scientologie préconise l'abstinence de drogues et médicaments psychiatriques, et l'aspirine est déconseillée avant d'être auditée. Cette attitude vis-à-vis de la médication correspond à une attitude plus large de rejet de certaines connaissances scientifiques conventionnelles, lesquelles sont remplacées par la doctrine scientologique. 
La Scientologie rejette catégoriquement la psychiatrie et la considère comme étant une « industrie mortuaire ». Dans une exposition tenue à Jefferson City en janvier 2007, la Scientologie a accusé les psychiatres d'abuser sexuellement de leurs patients et d'être responsables de l'existence des attentats-suicide, dont ceux du 11 septembre 2001. La Scientologie affirme dénoncer depuis 1952 de nombreuses pratiques concernant le domaine de la santé mentale : lobotomie et électrochocs en particulier, mais aussi toutes les pratiques consistant à traiter des problèmes d'origine mentale simple avec des psychotropes ou des opérations chirurgicales au niveau du cerveau entraînant de graves troubles de comportement chez ceux qui les subissent. Ses adeptes arguent aussi que ces dénonciations continues auraient provoqué des réactions de défense de la part de groupes dont les intérêts étaient concernés. 
Les idées de la Scientologie en la matière sont principalement diffusées par la Commission des citoyens pour les droits de l'homme (CCDH), qui serait à l'origine de propositions parlementaires relatives à la santé mentale ; ce qui explique que d'autres critiques de la psychiatrie tentent de se démarquer de la Scientologie. L'Église de Scientologie dispose également, à Bruxelles, d'un Bureau européen des affaires publiques et des droits de l'homme. Le Collectif des médecins et des citoyens contre les traitements dégradants de la psychiatrie est rattaché à la Scientologie. Il s'agit d'une association non déclarée en préfecture de médecins scientologues. Les opposants à la Scientologie considèrent cela comme une guerre contre la psychiatrie et la psychologie de la part de la Scientologie. D'après eux, la Scientologie se positionne comme une concurrente de ces sciences, par le développement de compétences et de techniques d'analyse des pratiques de communication et de soin des problèmes mentaux des psychiatres. Ils l'accusent de mal plagier des techniques psychiatriques et d'utiliser des techniques dangereuses. Selon les scientologues, les psychologues et surtout les psychiatres cherchent à régler des problèmes mentaux en s'attaquant à la matière (médicaments qui entraînent des changements chimiques, opérations, etc.). 


V.3. Ses détracteurs
Les détracteurs sont appelés dans le jargon de la Scientologie les « suppressifs ». Quand un adepte a un problème, les scientologues suspectent la présence d'un suppressif dans ses relations, cherchent à savoir qui par l'audition, et en général préconisent de ne plus rencontrer cette personne. De nombreuses histoires de séparations entre mari et femme, parents et enfants, liées à la scientologie en témoignent. D'un autre coté, des observateurs indépendants soulignent la faiblesse des arguments de certains de ces détracteurs ; ainsi, Marco Frenschkowsky (universitaire allemand), quoique opposé lui-même à la scientologie, a écrit qu'il y avait peu de marchés plus lucratifs en Allemagne que d'être un ex-scientologue et d'attaquer la scientologie.88 D'après lui ces « apostats », après quelques semaines de scientologie et sans avoir presque rien lu de cette philosophie publient de longs exposés (pour le compte du « marché antisecte ») qui se ressemblent tous. 
Il existe de nombreux opposants à la scientologie à travers le monde. Pour la francophonie, citons Roger Gonnet, ancien membre de la scientologie et créateur d'un site antiscientologue 89 où il lui reproche entre autres d'ériger la diffamation (propagande noire selon les termes de Ron Hubbard) en principe de défense, ainsi que l'existence d'une société secrète et d'un pouvoir centralisé dont la simple existence elle-même est à la limite de la légalité. Il signale des condamnations pour escroquerie comme dans le cas de contrats de travail d'une durée de 5 000 ans, qui peuvent s'apparenter à une forme d'esclavage ; les contrats ont parfois même une durée supérieure à un milliard d'années. Il est régulièrement attaqué par l'Église de Scientologie à divers titres (diffamation sur des forums, absence de déclarations à la CNIL). L'église de Scientologie a déjà été condamnée à indemniser Roger Gonnet après avoir été déboutée, et il a aussi été condamné à plusieurs reprises.92 
Des journalistes ou des chercheurs étudiant la Scientologie se sont plaints d'être l'objet de harcèlement et de graves menaces : Paulette Cooper, auteur du premier livre critique connu sur le sujet, aurait été harcelée dans le but de la pousser au suicide , de la faire interner ou incarcérer dans le cadre d'une opération appelée Freakout ; Paul Ariès, auteur en 1999 de La Scientologie, une secte contre la république, affirme avoir reçu des menaces de mort. D'après les détracteurs, une fois passé un certain nombre d'étapes on pourrait se retrouver dans l'administration centrale de la scientologie, autrefois située sur un bateau. Là, la scientologie entretiendrait une milice armée. Là aussi seraient centrés les organismes de police de la scientologie : espionnage, spécialistes de la diffamation. Ces organisations scientologiques auraient souvent recours aux cambriolages, aux vols de courrier. On trouverait dans les textes d'Hubbard les ordres et la justification de tels actes. 
En 2006, la série animée américaine South Park diffusée sur Comedy Central diffuse un épisode nommé Piégé dans le placard, qui parodie notamment les préceptes de l'église concernant Xenu, ainsi que l'utilisation de stars au profit de l'image de la secte (Tom Cruise et John Travolta). L'épisode rappelle également que L. Ron Hubbard était auteur de science-fiction. La fin se réfère au caractère procédurier de l'église, et tous ceux qui ont participé à l'épisode sont crédités John Smith et Jane Smith. 
Début 2008, suite aux pressions et aux menaces de procès de l'organisation pour faire retirer de Youtube une vidéo montrant Tom Cruise en train de s'exprimer devant une assemblée de scientologues, un groupe d'internautes, sous le pseudonyme collectif « Anonymous », annonce déclarer la guerre à la scientologie. Sous le nom de Projet Chanology, diverses attaques visant à perturber les sites Internet de l'organisation se sont succédé à partir du 14 janvier, en particulier des dénis de service, des google bombing et la publication sur Internet de milliers de documents et rapports internes récupérés via le réseau. Peu de temps après, ce petit groupe est devenu une force planétaire qui entend user de méthodes pacifiques et festives pour dénoncer les mensonges de la scientologie. 


VI. Procès en France
Une procédure a été ouverte en 1983 en France contre l'Église de Scientologie, et une autre en 1989 pour « escroquerie » et « exercice illégal de la médecine ». L'instruction de cette dernière affaire, confiée à la juge Marie-Paule Moracchini (désaisie le 18 octobre 2000), avait tant traîné que les faits ont été déclarés prescrits par la juge d'instruction parisienne Colette Bismuth-Sauron en juillet 2002, soit 13 ans plus tard. Cette décision a été qualifiée à l'époque de « nouvelle victoire » par l'Église de Scientologie.99 Au cours de cette instruction, un tome et demi de pièces du dossier avaient mystérieusement disparu en plein Palais de Justice de Paris.100 
À la suite d'une nouvelle enquête commencée en 1998, deux structures proches de la Scientologie, l'Association spirituelle de l'église de scientologie et la librairie Scientologie espace liberté ont été renvoyées en correctionnelle le 8 septembre 2008 par un juge parisien pour « escroquerie en bande organisée »101,102. Le Parquet avait requis un non-lieu, estimant que « l'information n'a pas permis d'établir que les remises de fonds effectués par les plaignants aient procédé de démarches frauduleuses » et « qu'il ne peut être considéré que l'incitation à faire du sauna, prendre des vitamines et courir pour se "purifier" constitue un délit d'exercice illégal de la médecine ».103Ceci a été présenté par l'avocat des parties civiles comme une « attitude complaisante » laissant entrevoir des motivations politiques. L'ordonnance de renvoi de l'Église de scientologie en tant que personne morale devant le tribunal comporte les accusations suivantes : « multiples manipulations bancaires », « surfacturations de produits vendus par les scientologues », « dissimulation de ses gains », et retient la circonstance de bande organisée concernant les dirigeants français de la scientologie. L'ordonnance cite également des documents internes de la scientologie qui, selon elle, « ne laissent aucun doute sur la finalité commerciale de l'action scientologue, dénotant une véritable obsession pour le rendement financier ». Le procès pour « escroquerie en bande organisée » se tient à Paris en mai 2009. Une condamnation pourrait rendre possible la dissolution du mouvement en France, en application de la loi About-Picard. Le jugement sera rendu le 27 octobre 2009. 


<TXT=w-Sélectionnaturelle11>

I. Historique 
Alors que plusieurs théories évolutives existaient déjà sous le nom de transformisme, Charles Darwin (1809-1882) propose ce mécanisme que l'on désigne sous le terme de darwinisme ou sélection darwinienne. Le terme "sélection naturelle" a été imaginé par Darwin par analogie avec la sélection artificielle pratiquée par les humains depuis des millénaires : les agriculteurs ou éleveurs choisissent à chaque génération les individus présentant les meilleurs caractéristiques pour les faire se reproduire. Le mécanisme de sélection darwinienne permet donc d'expliquer de façon naturaliste la complexité adaptative des êtres vivants, sans avoir recours au finalisme ni à une intervention surnaturelle, d'origine divine, par exemple. 


II. Principes de la sélection naturelle
La théorie de la sélection naturelle telle qu'elle a été initialement décrite par Charles Darwin, repose sur trois principes:

le principe de variation
le principe d'adaptation
le principe d'hérédité


II.1. Principe 1 : Les individus diffèrent les uns des autres
En général, dans une population d'individus d'une même espèce, il existe des différences plus ou moins importantes entre ces individus. En biologie, on appelle caractère, tout ce qui est visible et peut varier d'un individu à l'autre. On dit qu'il existe plusieurs traits pour un même caractère. Par exemple, chez l'être humain, la couleur de la peau, la couleur des yeux sont des caractères pour lesquels il existe de multiples variations ou traits. La variation d'un caractère chez un individu donné constitue son phénotype. C'est là, la première condition pour qu'il y ait sélection naturelle : au sein d'une population, certains caractères doivent présenter des variations, c'est le principe de variation.


II.2. Principe 2 : Les individus les plus adaptés au milieu survivent et se                        reproduisent davantage
Certains individus portent des variations qui leur permettent de se reproduire davantage que les autres, dans un environnement précis. On dit qu'ils disposent d'un avantage sélectif sur leurs congénères:

La première possibilité est, par exemple, qu'en échappant mieux aux prédateurs, en étant moins malades, en accédant plus facilement à la nourriture, ces individus atteignent plus facilement l'âge adulte, pour être apte à la reproduction. Ceux qui ont une meilleure capacité de survie pourront donc se reproduire davantage. 
Dans le cas particulier de la reproduction sexuée, les individus ayant survécu peuvent être porteurs d'un caractère particulièrement attirant pour les partenaires de sexe opposé. Ceux-là seront capables d'engendrer une plus grande descendance en copulant davantage. 

Dans les deux cas, l'augmentation de la capacité à survivre et à se reproduire se traduit par une augmentation du taux de reproduction et donc par une descendance plus nombreuse, pour les individus porteurs de ces caractéristiques. On dit alors que ce trait de caractère donné offre un avantage sélectif, par rapport à d'autres. C'est dans ce principe d'adaptation uniquement, qu'intervient le milieu de vie.                

II.3. Principe 3 : Les caractéristiques avantageuses doivent être héréditaires
La troisième condition pour qu'il y ait sélection naturelle est que les caractéristiques des individus doivent être héréditaires, c'est-à-dire qu'elles puissent être transmises à leur descendance. En effet certains caractères, comme le bronzage ou la culture, ne dépendent pas du génotype, c'est-à-dire l'ensemble des gènes de l'individu. Lors de la reproduction, ce sont donc les gènes qui, transmis aux descendants, entraîneront le passage de certains caractères d'une génération à l'autre. C'est le principe d'hérédité. 
Ces trois premiers principes entraînent donc que les variations héréditaires qui confèrent un avantage sélectif seront davantage transmises à la génération suivante que les variations moins avantageuses. En effet les individus qui portent les variations avantageuses se reproduisent plus. Au fil des générations, on verra donc la fréquence des gènes désavantageux diminuer jusqu'à éventuellement disparaître, tandis que les variations avantageuses se répandront dans la population, jusqu'à éventuellement être partagées par tous les membres de la population ou de l'espèce. Par exemple, dans la population humaine, la bipédie est un caractère commun à tous les êtres humains modernes. 


II.4. Une histoire imaginée par Richard Dawkins
Cette histoire amusante n'a d'autre but que de bien fixer un point important de la théorie darwinienne. 
Deux brontosaures voient un T-Rex avancer dans leur direction et se mettent à courir aussi vite qu'ils le peuvent. Puis l'un des deux dit à l'autre : 

« Pourquoi nous fatiguons-nous au juste ? Nous n'avons de toute façon pas la moindre chance d'arriver à courir plus vite qu'un T-Rex ! » 

Et l'autre lui répond cyniquement : 

« Je ne cherche pas à courir plus vite que le T-Rex. Je cherche juste à courir plus vite que toi ! » 

L'idée est de rappeler que le processus concerne moins une compétition entre espèces, qu'une compétition à l'intérieur de chaque espèce. C'est à partir de ce constat et de la découverte du conflit sexuel que Thierry Lodé développe l'hypothèse que le conflit au sens large (conflit sexuel, conflit de reproduction, coévolution) serait un puissant vecteur d'évolution, né de multiples interactions antagonistes. En fait, le rôle des interactions et des mécanismes coévolutifs est encore probablement sous-estimé. 



III. La sélection naturelle explique l'adaptation des espèces à leur milieu 
Lorsqu'on observe des espèces dans leur milieu de vie, elles semblent toutes être profondément adaptées à chacun de leur milieu : le long cou et les longues pattes de la girafe sont en effet bien adaptés pour attraper des feuilles hautes des acacias des savanes africaines. On pourrait tout aussi bien dire que ce sont les organismes non adaptés qui n'ont pas survécu dans ce milieu. 
En outre, certaines variations avantageuses dans un environnement donné peuvent devenir néfastes sous d'autres conditions. Par exemple, dans un milieu enneigé, une fourrure blanche permet de ne pas être vu par ses futures proies ou ses prédateurs, mais si le milieu devient forestier et plus sombre, il n'y aura plus de camouflage et les individus porteurs de fourrure blanche perdront leur avantage sélectif. La conséquence de ce phénomène est donc qu'au fil des générations, par la sélection naturelle, les caractères observés dans une population seront plus ou moins adaptés aux évolutions de son écosystème. 
Autres exemples, chez les humains la couleur de la peau est une adaptation due à la sélection naturelle, et non à un bronzage qui se serait « fixé » à tout jamais dans certaines populations. En zones ensoleillées les individus à la peau claire ont plus de risque de développer un cancer de la peau à cause des rayons UV, ils sont donc désavantagés car leur espérance de vie est moindre. En zones moins ensoleillées ces individus seraient avantagés car la lumière du soleil permet au corps de produire de la vitamine D, et de plus le corps économise de l'énergie et des nutriments en fabriquant moins de mélanine, le pigment de la peau. 
Les facteurs de l'environnement qui peuvent donc entraîner une sélection naturelle peuvent être:

Des facteurs physico-chimiques (le biotope) : climat, milieu occupé (terrestre, aquatique, cavernicole...)
D'autres êtres vivants (la biocénose) : présence de prédateurs, de parasites, de microbes, de compétiteurs, etc.


III.1. Adaptations convergentes
L'adaptation des espèces à leur niche écologique peut parfois conduire deux espèces qui occupent un milieu similaire, à acquérir des ressemblances qui ne sont alors pas dues à leur éventuelle parenté. On parle dans ce cas d'évolution convergente. Ce phénomène s'interprète comme le fait que les mêmes contraintes du milieu mènent aux mêmes « solutions adaptatives ». 
Les yeux des vertébrés et des céphalopodes constituent l'un des exemples les plus frappants de convergence alors même que l'ancêtre commun de ces deux taxons ne possédait pas d'yeux complexes. ces deux lignées ont évolué vers des systèmes optiques qui présentent une très forte similarité. La différence résidant surtout dans l'orientation des cellules sensorielles dans la rétine 
De tels cas de convergence évolutive sont souvent mis en avant pour argumenter en faveur d'une conception adaptationniste de l'évolution par sélection naturelle selon laquelle l'essentiel des caractéristiques observées dans les espèces vivantes ne sont pas dues au hasard mais sont le résultat de diverses pressions de sélection au cours de l'histoire évolutive des espèces. 



IV. Origine des variations héréditaires dans une population
Pour qu'il y ait sélection, encore faut-il que plusieurs variations d'un même caractère soient présentes dans la population afin que l'individu le plus adapté l'« emporte » sur les autres. En effet, dans l'exemple ci-dessus, si la totalité des individus sont identiques et porteurs de la variation phénotypique « fourrure blanche », en cas de réchauffement climatique aucun individu ne pourra survivre, et l'espèce s'éteindra. En cas de modification de l'environnement, pour qu'une espèce survive, il faut qu'elle s'adapte par la sélection naturelle. Il est donc indispensable, qu'avant le changement du milieu elle présente en son sein une diversité génétique importante.

IV.1. L'information génétique portée par l'ADN est relativement instable
La diversité génétique dans une population d'individus a pour origine des modifications de l'information génétique dans l'ADN des cellules. Il s'agit: 

de mutations ponctuelles dans la séquence d'ADN des gènes, ou de mutations plus importantes comme des réarrangements chromosomiques ; 
de recombinaisons génétiques qui se produisent lors des transferts d'ADN des bactéries et lors de la reproduction sexuée des eucaryotes.

On peut ajouter aux modifications du génome citées plus haut, une autre source de diversité génétique, soient les migrations par lesquelles le stock génétique dans une population donnée se voit renouvelé par l'arrivée d'autres membres de l'espèce porteuse d'un pool génétique différent. 
Les modifications génétiques sont aléatoires : ce n'est pas l'environnement qui « dicte » quel gène doit muter, mais bien le hasard. C'est pour cela qu'on observe dans les populations beaucoup de variations inadaptées au milieu de vie (par exemple, les maladies génétiques rares). Une erreur courante consiste à croire que les modifications génétiques sont une conséquence de la sélection naturelle. En revanche la sélection naturelle a bien le pouvoir de cumuler les innovations adaptées, ce qui aboutit à des adaptations complexes. 


IV.2. La sélection naturelle agit après les modifications de l'information                        génétique
C'est donc parmi la grande diversité génétique des individus, que vont ensuite être sélectionnés les phénotypes et les allèles les plus adaptés à l'environnement. Pour en faire la démonstration on pourrait simplement montrer que les phénotypes nouveaux étaient présents avant le changement du milieu. Ceci n'est pas toujours évident et quand bien même ce serait le cas, un mécanisme d'induction d'une mutation donnée par l'environnement n'est pas à exclure. C'est grâce à une astuce mathématique que Luria et Delbrück montrent que ce sont bien les mutations préexistantes dans une population de bactéries qui sont sélectionnées quand on ajoute un virus.


IV.3. L'hypothèse des caractères acquis
La génétique moléculaire n'existant pas à son époque, Darwin ne pouvait prendre en compte les mécanismes moléculaires à l'origine des nouveaux caractères. Sa théorie de la sélection naturelle incluait donc l'hypothèse de la transmission des caractères acquis. Dans son ouvrage de 1868, La variation des animaux et des plantes sous l'effet de la domestication, il alla même jusqu'à proposer une théorie pour cette transmission des caractères acquis.. 
Dans la théorie initiale de Darwin telle qu'il l'expose dans L'Origine des espèces, ces variations entre les individus trouvent leur origine dans le fait que des individus acquièrent des caractéristiques différentes au cours de leur vie. Ces caractères acquis seraient alors transmis à leur descendance et cela expliquerait les variations observées et l'évolution des caractéristiques de l'espèce. Toute autre source de variation reste pourtant acceptable, comme par exemple le hasard; en revanche, le problème qui se poserait alors serait de savoir comment empêcher la dilution, puis l'éventuelle disparition de ces caractères s'ils ne sont pas "entretenus". 
La théorie de l'hérédité des caractères acquis a été considérée comme invalidée par August Weismann à la fin du xixe siècle. En réponse aux néo-lamarckiens qui soutenaient le contraire, il montra que des mutilations n'étaient pas transmises. On en déduisit abusivement qu'aucun caractère acquis ne pouvait se transmettre, alors qu'une mutilation ne peut être assimilée à une acquisition par l'organisme de fonctions nouvelles comme le voulait Lamarck. On ne peut prouver avec certitude l'impossibilité d'hérédité de caractères acquis (une inexistence ne peut être prouvée qu'en mathématiques, par l'absurde). On peut à défaut en chercher s'il existe quelque exemple réel réfutant cette impossibilité. Ce qui serait le cas chez les oiseaux où un seul ovaire, le gauche, peut se développer. Il a bien fallu que des caractères acquis soient devenus héréditaires pour orienter l'Évolution. Plusieurs recherches ont été menées en ce sens au début du xxe siècle, notamment par Paul Kammerer. Cela qui fait aussi l'objet de recherches dans le domaine de l'immunologie. 
Les caractères innés sont bel et bien transmis au cours de la reproduction mais avec des variations qui suivent les lois de l'hérédité mendélienne, du nom de leur découvreur, Gregor Mendel (1822-1884) dont les travaux sur les lignées de pois ne furent redécouverts qu'au début du xxe siècle et étaient malheureusement ignorés de Darwin. Mendel apporte pour le coup la réponse au problème de la dilution : un caractère ne s'affaiblit pas; il est simplement transmis en tant que dominant, transmis en tant que récessif, ou éliminé; mais chez les individus qui le portent, il reste totalement présent, ce qui assure sa pérennité s'il est favorable à son porteur (ou, pour être plus précis, à la descendance de son porteur). 


V. La sélection naturelle en génétique des populations
La convergence entre la théorie darwinienne et la théorie de l'hérédité donnera alors naissance au cours des années 1930 à la génétique des populations, en particulier grâce aux travaux théoriques de Ronald Fisher. À la même période, grâce aux expériences de Thomas Morgan et Theodosius Dobzhansky sur les mouches drosophiles, les mécanismes moléculaires responsables des phénomènes d'hérédité génétique commenceront à être identifiés. L'une des découvertes majeures de la biologie sera alors de montrer que la diversité génétique qui garantit la variété des phénotypes est due à des modifications aléatoires du génotype (mutations, recombinaisons génétiques, ...) en particulier lors de sa transmission d'une génération à l'autre, au moment de la reproduction. 
Même si ce n'est pas le mécanisme qui avait été envisagé par Darwin dans sa théorie de la sélection naturelle, il n'en reste pas moins que ces processus permettent de rendre parfaitement compte de la sélection naturelle dans le cadre de ce qui est considéré comme la théorie centrale de la biologie moderne, la théorie synthétique de l'évolution ou synthèse néo-darwinienne qui fait le lien entre les mécanismes au niveau de la génétique moléculaire et les phénomènes d'évolution à l'échelle des populations. 
Ainsi la sélection naturelle peut se "mesurer" grâce à des calculs statistiques. 


VI. Cas de sélection naturelle scientifiquement démontrés 
Il apparaît aujourd'hui évident que tout organe ayant une fonction définie, par exemple la nageoire du poisson, est une adaptation à un milieu et le résultat d'une sélection naturelle. Cependant la démonstration scientifique doit, elle, passer par la mise en évidence d'une corrélation chiffrée entre les variations d'un caractère héréditaire et celles d'un paramètre précis de l'environnement. Parmi les exemples les plus célèbres, on peut citer :

Les pinsons des Galapagos : les épisodes de sécheresse furent suivis par une raréfaction des graines molles et donc par une augmentation de la taille du bec des pinsons leur permettant de briser la coquille des graines restantes, plus dures. 
La sélection naturelle chez les bactéries de résistances aux virus bactériophages a été mise en évidence par l'expérience de Luria et Delbrück. Ils ont en effet démontré pour la première fois que les mutations préexistent à la sélection et qu'elles sont bien aléatoires. 
Le mélanisme industriel de la phalène du bouleau en Angleterre: dans cette espèce de papillons, la proportion d'individus clairs aurait diminué du fait de la pollution qui noircissait les surfaces des troncs d'arbre sur lesquels ils se posaient. En effet, Les individus clairs (initialement présents en majorité), étant plus visibles que les autres sur les troncs noircis par les rejets de suies, étaient plus sujets à la prédation que les autres. Les phalènes sombres qui existaient avant la pollution seraient alors devenues majoritaires. Puis, le phénomène se serait inversé lorsque les industries polluantes ont progressivement disparu. En fait, cette belle histoire s'écroule lorsqu'on sait que les phalènes ne se posent pas sur les troncs mais sous les feuilles des bouleaux. La mélanine étant impliquée dans la réponse immunitaire chez les invertébrés, les modifications des fréquences des morphes sombres et clairs sont considérées comme des réponses à la toxicité des rejets industriels plutôt qu'envers la modification de la couleur des troncs. 


VI.1. Sélection naturelle dans l'espèce humaine
La sélection naturelle produit aussi ses effets dans l'espèce humaine :

La capacité chez l'adulte à digérer le lactose du lait a été sélectionnée il y a environ 9000 ans dans les populations humaines originaires d'Europe du nord ou d'Asie centrale où on élevait du bétail pour son lait, mais pas dans d'autres populations où cet aliment est absent4. Récemment, il a été découvert que cette capacité était présente également chez certaines ethnies d'Afrique de l'Est, les Tutsis et Fulanis du Soudan, de Tanzanie et du Kenya, mais à partir de trois autres mutations, sélectionnées indépendamment l'une de l'autre5. Ces dernières seraient apparues il y a 7000 à 3000 ans. Il s'agit là d'un exemple de convergence évolutive. 
La persistance d'une maladie génétique comme la drépanocytose est due au fait que l'allèle responsable de la maladie confère aussi un avantage sélectif en augmentant aussi la résistance au parasite responsable du paludisme. 



VI.2. Relation entre sélection naturelle et les activités humaines
L'homme peut aussi exercer involontairement une pression de sélection sur certains organismes dont l'évolution, en retour, peut être néfaste pour l'économie ou la santé humaine :

La résistance aux antibiotiques : Par exemple, depuis 1961 les bactéries staphylocoques dorés résistantes à la méticilline se sont multipliées et rendent inefficaces le traitement par cet antibiotique dans un grand nombre de cas. Ces résistances sont à l'origine de nombreuses maladies nosocomiales, en milieu hospitalier.
La résistance des criquets pèlerins aux insecticides : Comme dans l'exemple précédent, l'utilisation massive d'insecticide pour éradiquer les criquets s'est traduite par une plus grande résistance. En effet, les individus survivant au traitement insecticide peuvent engendrer plusieurs milliers de descendants en quelques générations, soit en quelques années, qui eux-mêmes héritent de cette capacité de résistance. L'élimination de cette nouvelle population exige alors un traitement encore plus agressif pour l'écosystème.



VI.3. Sélection naturelle, coévolution et compétition
Dans les exemples précédents, il s'agit d'espèces évoluant au gré des améliorations techniques humaines mais ce phénomène peut aussi s'observer dans les interactions biologiques entre deux espèces. Un exemple très étudié d'une telle coévolution est le parasitisme de ponte chez les coucous. Dans ces espèces, le parent pond ses oeufs dans le nid d'autres oiseaux. Dès sa naissance, le jeune coucou expulse les oeufs présents afin d'être le seul à bénéficier des soins prodigués par les parents de l'espèce hôte ainsi trompés. Parmi les espèces parasitées, certaines ont évolué des stratégies antiparasitiques, en l'occurrence une aptitude à distinguer les oeufs de coucou de leurs propres oeufs. Cela a créé une pression de sélection pour certaines espèces de coucou qui ont évolué une forme de mimétisme des oeufs de telle sorte que ceux-ci ressemblent étonnamment aux oeufs de leurs hôtes. À leur tour, les espèces parasitées (comme la pie-grièche écorcheur) ont développé des capacités accrues de discrimination de leurs propres oeufs des oeufs mimétiques du coucou, capacité qui est absente chez des espèces non ou moins parasitées. 
De tels phénomènes de coévolution sont fréquemment observés dans les cas de parasitisme mais parfois aussi dans certains cas de compétition interspécifique ou dans le cadre compétition intraspécifique de la sélection sexuelle. La coévolution inter-sexes s'observent souvent en réponse à la sélection sexuelle post-copulatoire : par exemple, dans certaines espèces d'oiseaux et d'invertébrés, les pénis des mâles ont une forme hélicoïdale qui fonctionne comme un goupillon éliminant la semence des autres mâles et leur permettant de déposer leurs propres gamètes au plus profond du vagin des femelles avec lesquelles ils s'accouplent afin de s'assurer la paternité de la progéniture ; dans ces mêmes espèces, on observe une augmentation proportionnelle de la taille du vagin avec une morphologie tout en sinuosités qui contrecarre les stratégies reproductives des mâles. 



VII. La sélection artificielle 
L'adjectif naturelle s'oppose chez Darwin au concept de sélection artificielle connue et pratiquée depuis quelques milliers d'années par les éleveurs. En effet les animaux d'élevage domestiques ou les espèces de plantes cultivées (vaches, chiens, roses...) constituent autant de variations « monstrueuses » absentes dans la nature. Elles sont le fruit de la lente sélection d'individus intéressants (pour les rendements, ou du point de vue esthétique) par les éleveurs et les agriculteurs (voir élevage sélectif des animaux). C'est cette observation qui permit à Darwin d'émettre l'hypothèse d'une sélection opérée par la nature sur les espèces sauvages. 
Par exemple : 

La rose cultivée est une mutation d'une rose sauvage. Cette mutation a été sélectionnée par les horticulteurs, elle est répandue dans les jardins: c'est la sélection artificielle. 
Dans la nature des roses cultivées ne peuvent se reproduire, c'est la sélection naturelle.

La sélection artificielle, malgré son intense pression (élimination de tout géniteur qui ne répond pas aux critères du choix), ne parvient pas, après des pratiques millénaires, à faire naître de nouvelles espèces. Les races ne s'isolent pas et peuvent s'hybrider sans perte ou baisse de fécondité. La domestication et la culture révèlent les limites, assez étroites entre lesquelles l'espèce varie sans courir de péril, mais elles n'impriment pas un mouvement évolutif aux espèces qu'elles concernent.


VIII. La sélection utilitaire et la sélection sexuelle
La sélection darwinienne s'appuie sur de deux mécanismes conjoints. Le premier est la sélection utilitaire (ou sélection de survie ou sélection écologique) le second est la sélection sexuelle. Dans le grand public, ce dernier aspect de sélection sexuelle est souvent ignoré et on identifie la sélection naturelle avec la sélection de survie. Or c'est une erreur car ces deux mécanismes sont bien à l'oeuvre dans le monde vivant. 
La sélection utilitaire correspond à un processus de tri entre individus en vertu de leur capacité à survivre et/ou à être féconds 10. Ce terme désigne plus spécifiquement le mécanisme qui fait évoluer les espèces sous la pression "externe" de l'environnement ou "interne" de la compétition intraspécifique. En effet, elle repose sur l'idée que pour pouvoir se reproduire, il faut d'abord survivre. Ce type de sélection favorise donc les individus capables d'échapper ou de se protéger des prédateurs mais aussi de résister aux parasites : c'est la compétition interspécifique. Il existe aussi une compétition intraspécifique : les individus d'une même espèce étant en compétition entre eux pour trouver des ressources dans l'environnement, qu'il s'agisse de proies ou d'autres ressources non-nutritives comme des abris (terrier, nid, ...). Enfin, il y a des facteurs dits abiotiques qui sélectionnent les individus les mieux capables de résister à l'environnement biotopique, aux conditions climatiques, etc. 
La sélection sexuelle est un phénomène qui a lieu à une étape différente de la vie de l'individu. Elle désigne le fait qu'il y a aussi une compétition au sein de chaque espèce pour accéder aux partenaires sexuels dans le cadre de la reproduction sexuée. Cet aspect de la théorie fut pleinement développé par Darwin dans son ouvrage intitulé La Descendance de l'Homme. Dans le cadre de la sélection sexuelle, il va donc se produire une compétition intra-sexe, entre les individus d'un même sexe, mais aussi inter-sexe, entre les sexes (les individus d'un sexe devant choisir avec quel individu de l'autre sexe ils vont s'accoupler). La sélection sexuelle permet donc d'expliquer des caractères ou des comportements qui pénalisent la survie quand ils sont analysés en dehors du contexte reproductif, comme la queue du paon, les bois des mégacéros. 


IX. Pression de sélection
Rétrospectivement, les modifications successives au cours des générations des populations peuvent sembler orientées, comme si ces modifications étaient "tirées" ou "poussées" dans une certaine direction. Par exemple, en suivant les observations de Darwin sur les pinsons des Galapagos, on peut observer que certaines espèces semblent suivre une tendance vers un élargissement du bec qui devient de plus en plus massif alors que chez d'autres espèces de pinsons, la tendance est plutôt vers un affinement du bec. 
Ce phénomène qui se manifeste comme une tendance apparente dans l'évolution d'une ou plusieurs espèces a reçu le nom de pression de sélection. Ces pressions de sélection sont 'orientées' par les pressions dites intérieures à l'espèce (sélection sexuelle, compétition intraspécifique) et les pressions dites extérieures à l'espèce (limitation des ressources, modifications de l'environnement, prédateurs, parasites...), bref, tout ce qui influence la survie et la reproduction des individus. 
Les pressions de sélection s'exercent différemment d'une espèce à l'autre ou d'un milieu écologique à un autre, voire d'une sous-population d'individus à une autre. Ainsi il peut se produire au sein d'une même espèce une divergence si deux sous-populations sont soumises à des pressions de sélection légèrement différentes. Ces deux populations évolueront vers des formes différentes qu'on appelle morphes et si le phénomène se poursuit dans le temps on peut aboutir à la formation de deux espèces distinctes, c'est la spéciation sympatrique. Les deux espèces occupent alors des niches écologiques suffisamment distinctes pour qu'elles n'entrent plus directement en compétition l'une avec l'autre et suivent alors des "trajectoires" évolutives différentes en réponse aux pressions de sélection spécifiques auxquelles elles sont soumises. 


X. La sélection naturelle permet l'apparition d'adaptations biologiques complexes
Les systèmes vivants apparaissent comme très complexes et sont des adaptations tellement poussées à un milieu que les humains y trouvent une source d'innovations techniques et industrielles (par exemple, les attaches scratch ou velcro, les industries pharmaceutique, et chimique). voir:bionique. 
Cette complexité n'a pu voir le jour sans le pouvoir qu'a la sélection naturelle d'accumuler les « bonnes » innovations génétiques : 
Chaque innovation évolutive apparaît de manière aléatoire. La sélection naturelle favorise ensuite chacun de ces petits "sauts" évolutifs (a, puis ab, puis abc... puis abcde). Elle permet ainsi l'apparition d'adaptations de plus en plus poussées (abcde). En effet, si le caractère (a) n'avait pas été sélectionné, le caractère (abcde) ne serait jamais apparu. Car (c) dépend de (a). Un caractère complexe, comme une enzyme, résulte d'une accumulation d'innovations sélectionnées successivement, et non de simples apparitions indépendantes, au hasard des innovations génétiques (même si certains "sauts" évolutifs peuvent être plus ou moins importants ou graduels, voir équilibre ponctué). Donc la sélection naturelle ne fait pas que favoriser les adaptations les plus complexes ; elle permet aussi leur apparition. 
Cela n'est valable que si la sélection naturelle s'opère de manière continue, ce qui est envisageable par une compétition intra-spécifique, que Malthus et Darwin estiment inévitable dans une population. En effet les êtres vivants ont une tendance naturelle et universelle à se reproduire en plus grand nombre qu'à la génération précédente. 
Les simples innovations dues au hasard sur quelques générations ne suffisent pas à rendre compte de la complexité des êtres vivants et de leur adaptation à leur milieu. Il faut la sélection naturelle pour accumuler les petites innovations et pour en arriver à un organe aussi complexe que l'oeil de mammifère, par exemple. 
Cela peut être une réponse aux critiques de certains néo-créationnistes, qui affirment que les systèmes vivants (enzymes...) sont trop complexes et harmonieux pour que leur apparition ne soit due qu'à des mutations aléatoires, et que donc selon eux il n'aurait pas eu d'évolution. 


XI. Limites de la sélection naturelle pour expliquer l'évolution de la lignée                    humaine
Comme toutes les autres espèces animales, l'espèce humaine est le produit de l'évolution et de la sélection naturelle. Par exemple, on peut penser que la bipédie a offert dans le passé un tel avantage sélectif aux individus capables de se déplacer debout que les gènes associés à ce mode de locomotion se sont répandus dans toute la population humaine. 
Il reste néanmoins difficile de reconstruire le détail de l'histoire évolutive de notre espèce et en particulier d'identifier quels facteurs exacts et quels processus précis ont pu intervenir dans l'évolution humaine (e.g., sélection sexuelle, sélection de groupe, sélection culturelle, dérive génétique ...) Dans certains cas, il est néanmoins possible d'identifier les pressions de sélection et les adaptations résultant de la sélection naturelle dans les populations humaines. L'un des exemples les plus documentés est la capacité de résistance au paludisme. Les individus porteurs d'un gène entraînant une anomalie de leurs cellules sanguines résistent mieux au parasite qui cause cette maladie. Par contre, leurs enfants courent le risque de souffrir de drépanocytose s'ils héritent de ce gène des deux parents. L'avantage sélectif fournit par la résistance au paludisme permet donc d'expliquer pourquoi dans les populations humaines où ce parasite est endémique (Afrique) se maintient l'allèle d'un gène pouvant entraîner une maladie relativement grave. 
L'une des tentatives les plus décriées d'appliquer la théorie darwinienne à l'espèce humaine fut l'utilisation qu'il fut faite de la sociobiologie développée par E.O. Wilson pour expliquer le comportement des espèces ultra-sociales (fourmis, termites, abeilles, ...). En effet, même s'il est vrai que l'une des caractéristiques de l'être humain (et aussi d'autres primates) est son mode de vie très social, la théorie de Wilson nécessite aussi un mode de reproduction et d'organisation sociale très particulier dite eusocialité, très différents de ce qu'on observe chez les humains. Comme s'en défend Wilson lui-même, il est donc absurde de vouloir appliquer directement les conclusions des travaux menés sur ces espèces à l'espèce humaine. Toutefois certains outils théoriques développés initialement dans le cadre de la sociobiologie peuvent se révéler parfaitement pertinents pour l'étude de l'homme. 
Le défi majeur de la paléoanthropologie reste néanmoins de parvenir à un cadre théorique pour expliquer l'évolution humaine au moyen de mécanismes plus riches que la seule sélection naturelle. Or l'importance de phénomènes comme la culture, la sélection sexuelle, la dérive génétique reste difficile à évaluer : 

« Ainsi, même dans le cadre des théories modernes de l'évolution, qu'on appelle néodarwinisme ou théorie synthétique de l'évolution, les évolutionnistes s'efforcent de réserver une place à part à l'homme, étant entendu que si son corps a évolué, il reste que ce qui fait l'humain échappe aux lois de l'évolution. »

Un exemple récent d'une telle difficulté concerne le rôle de la sélection de groupe. Alors que ce mécanisme qui "favorise la survie du groupe au détriment de la survie de l'individu" a été très critiqué dans l'évolution animale. Il semble que l'une des particularités de l'espèce humaine est que justement, des phénomènes de sélection multi-niveaux ont pu jouer un rôle important au cours de son évolution et en particulier dans l'évolution de sa psychologie.


XII. Sélection naturelle, comportement et culture 
Tout comportement a une composante génétique et héréditaire. Il a été démontré que l'environnement pouvait agir sur l'évolution d'un comportement héréditaire et inné, chez certains animaux. 
Quant à la culture qui ne se transmet pas par l'ADN, mais par l'apprentissage, elle peut également être sujette à une sélection. Par exemple, si j'ai un comportement qui m'apporte de la satisfaction, comme m'habiller à la mode, je le reproduirai et je l'enseignerai ou on m'imitera. Et inversement, ce comportement ne sera pas transmis, si cela ne donne pas satisfaction. La théorie de la mémétique émise par Richard Dawkins, désigne ces entités, qu'on appelle mèmes, comme éléments de base de la sélection que subit la culture, au même titre que le sont les gènes pour l'évolution du vivant. 
En retour la culture peut entraîner de nouvelles conditions de vie, et donc modifier la sélection naturelle. Par exemple l'utilisation de lait de vache dans l'alimentation a favorisé les génotypes tolérants à la digestion du lactose (sucre présent dans le lait). 
Ainsi, l'évolution de la culture serait le produit d'une interaction entre une sélection naturelle et une sélection culturelle [réf. nécessaire]. 
Ainsi on peut envisager que ce modèle d'évolution faisant jouer des rétroactions puisse suivre un cercle vicieux ou vertueux, ce qui entraîne une évolution perpétuelle et continue. À condition que les rétroactions soient positives. Ce modèle est corroboré par les observations: la culture humaine suit bien une évolution continue [réf. nécessaire], et les structures anatomiques qui ont permis cette évolution (volume crânien, structure du cortex) ont aussi évolué de manière continue depuis 2 millions d'années, au moins. 
La culture humaine, qui fait toute la singularité de notre espèce, pourrait donc être le résultat d'un tel modèle d'évolution, avec une certaine indépendance vis-à-vis de l'environnement, si on ne tient pas compte des modifications engendrées par les humains eux-mêmes. 
E.O. Wilson parle de co-évolution des gènes et de la culture. Mais cette approche est critiquée[réf. nécessaire]. 
Cela est expliqué par certains scientifiques dans la théorie de la construction de niche: le comportement ou une autre activité peut influencer sur l'environnement immédiat (la niche écologique) et, en retour, modifier la pression de sélection naturelle. 


XIII. Les autres mécanismes de l'évolution
Lorsque les fréquences de certaines variations héréditaires changent uniquement à cause du hasard, on parle de dérive, voire (si le groupe est très réduit) d'effet fondateur. Ces caractères doivent être relativement neutres pour la sélection naturelle (il n'y a ni avantage, ni désavantage sélectif). Si une même évolution se produit de manière répétée dans un même milieu au cours des générations, ce n'est pas la dérive, mais on peut parler de sélection.


XIV. Dimension universelle de la sélection naturelle
Dans le cadre de cette théorie, tout système dans lequel s'observeraient ces trois premiers principes donnerait lieu à un phénomène d'évolution par sélection naturelle. Dans le monde vivant, la transmission héréditaire de l'information génétique, qui obéit à ces trois principes, résulte donc dans une évolution des espèces par sélection naturelle. Cependant, d'un point de vue théorique, l'évolution par sélection naturelle ne dépend pas de la nature précise des mécanismes qui permettent l'apparition de variations, la transmission héréditaire et la traduction de l'information héréditaire en caractères phénotypiques. Le fait que Darwin lui-même ignorait jusqu'à l'existence des gènes illustre bien le distinguo qu'il convient de faire entre le cadre théorique de la sélection darwinienne et ses manifestations observables dans les écosystèmes terrestres. 
Par conséquent, les phénomènes évolutionnaires observés dans le monde vivant pourraient tout à fait se manifester dans d'autres systèmes qui mettraient en oeuvre les trois principes fondamentaux de la sélection darwinienne. C'est par exemple, l'hypothèse faite en exobiologie selon laquelle des formes de vie extraterrestres pourraient être apparues sur la base de mécanismes fondamentaux différents de ceux que l'on connait de la biologie terrestre. Au sein de telles formes de vie évoluant par sélection naturelle, on devrait donc observer des phénomènes similaires à ceux que l'on connait sur Terre : adaptation, coévolution, reproduction sexuée... 
Les plus adaptationnistes des biologistes de l'évolution (comme Simon Conway Morris) ont ainsi proposé que les contraintes environnementales sont suffisamment fortes et similaires pour que les formes de vie extraterrestres devraient présenter d'importantes convergences évolutives avec la vie terrestre ; parmi lesquelles la présence d'yeux, l'eusocialité, ou des capacités cognitives complexes. 
Plus près de nous, l'application des principes de la sélection naturelle à la sphère culturelle humaine a donné lieu à la mémétique qui cherche à expliquer les variations, la transmission, et la stabilisation des phénomènes culturels par analogie avec les espèces vivantes. Dans ce cadre théorique, les mèmes sont les unités de sélection, ou réplicateurs, des phénomènes culturels. La sélection intervient sur les mèmes en fonction de leur capacité à "survivre" c'est-à-dire à persister dans l'esprit des individus et de leur capacité à "se reproduire", c'est-à-dire à passer d'un individu à un autre par l'imitation, la communication, l'enseignement, etc. À chaque reproduction, un mème peut donc "muter" : à force d'être racontée, une même histoire sera par exemple déformée, c'est le principe du téléphone arabe. Ainsi, malgré les différences notables entre la nature des réplicateurs biologiques et culturels, certaines analogies peuvent être envisagées : coévolution (y compris entre gène et mème) ou formation de complexes de mèmes. La pertinence de l'application de la théorie darwinienne aux phénomènes culturels reste néanmoins très débattue. 
Enfin on peut aussi citer l'exemple des algorithmes évolutionnaires utilisés dans le domaine de l'optimisation en ingénierie. Ceux-ci permettent de rechercher une solution à un problème donné en mettant en compétition une population de solutions potentielles dont seules les meilleures sont conservées pour être recombinées et donner naissance à une nouvelle génération de solutions. 
Étant donné que dans cette méthode la sélection est le résultat d'une intervention humaine, ce cas relève plutôt de la sélection artificielle. 


<TXT=w-Télécommunications12>

I. Généralités

I.1. Étymologie
Le mot télécommunications vient du préfixe grec tele- (XXXX-), signifiant loin, et du latin communicare, signifiant partager. Le mot télécommunication a été utilisé pour la première fois en 1904 par Édouard Estaunié, ingénieur aux Postes et Télégraphes, directeur de 1901 à 1910 de l'école professionnelle des Postes et Télégraphes (ancêtre de l'École nationale supérieure des télécommunications), dans son Traité pratique de télécommunication électrique.


I.2. Définition
Les télécommunications (abrév. fam. télécoms), sont considérées comme des technologies et techniques appliquées et non comme une science. 
On entend par télécommunications toute transmission, émission et réception à distance, de signes, de signaux, d'écrits, d'images, de sons ou de renseignements de toutes natures, par fil électrique, radioélectricité, liaison optique, ou autres systèmes électromagnétiques. 



II. Histoire

II.1. Origine des télécommunications
Les moyens simples naturels anciens comme la parole ou les signaux à vue, permettent de communiquer à courte distance. Le besoin de communiquer à plus grande distance dans les sociétés humaines organisées a amené très vite à développer des télécommunications primitives: tambours, signaux de fumée, langage sifflé, etc.. 
Certains de ces types de communications, comme les pavillons, sémaphores ou héliographes sont encore utilisés dans la marine, même si cet usage est devenu marginal. 


II.2. Télégraphe et téléphone
Bien que la communication par signaux optiques entre des points hauts soit très ancienne, on doit à l'ingénieur Claude Chappe la création à partir de 1794 du premier réseau simple et efficace de transmission optique de messages. Ce réseau qu'il a nommé « télégraphe » fut développé sur les grands axes français et resta en service jusqu'en 1848. 
Le premier service commercial de télégraphe électrique fut construit par Charles Wheatstone et William Fothergill Cooke, et ouvrit en 1839. C'était une amélioration du télégraphe électromagnétique déjà inventé. Samuel Morse développa indépendamment une version de télégraphe électrique, qu'il montra le 2 septembre 1837. Le code Morse était une avance importante sur le télégraphe de Wheatstone. 
Le premier câble télégraphique transatlantique fut achevé le 27 juillet 18665. Sa longueur était de 4200 km pour un poids total de 7000 tonnes. 
Le téléphone classique fut inventé indépendamment par Alexander Bell et Elisha Gray en 1876. Cependant, c'est Antonio Meucci qui inventa le premier dispositif permettant la transmission de la voix à l'aide d'une ligne parcourue par un signal. 


II.3. Télécommunications et sciences
Le domaine des télécommunications est un lieu de convergence et d'interaction entre les différentes technologies et disciplines scientifiques.
Les mathématiques et plus particulièrement les mathématiques appliquées sont à la base du développement des théories du traitement du signal (modernisation des télécommunications), de la cryptologie (sécurisation des échanges), de la théorie de l'information et du numérique. 
La physique a permis grâce au développement des mathématiques d'édifier la théorie de l'électromagnétisme. Sont apparus alors les premiers postes à galène, puis les tubes à vides, les semi-conducteurs et l'opto-électronique, qui sont à la base de l'électronique. 
La chimie, par le biais de l'affinement des processus chimiques, a permis de réduire le poids et d'allonger l'autonomie des batteries, autorisant l'emploi d'appareils portatifs de télécommunications. De même, l'invention du laser a ouvert la voie aux communications par fibres optiques modernes. 
L'informatique fondamentale et appliquée quant à elle a révolutionné le monde de la communication à distance par le développement des langages de programmation et des programmes informatiques (génie logiciel) associés à la micro-éléctronique. 


III. Technique des télécommunications

III.1. Principes
Une liaison de télécommunications comporte trois éléments principaux :

un émetteur qui prend l'information et la convertit en signal électrique, optique ou radioélectrique ; 
un média de transmission, pouvant être une ligne de transmission, une fibre optique ou l'espace radioélectrique, qui relie émetteur et récepteur ; 
un récepteur qui reçoit le signal et le convertit en information utilisable. 

Par exemple, en radiodiffusion, l'émetteur de radiodiffusion émet grâce à son antenne la voix ou la musique, qui passe dans l'espace sous forme d'onde électromagnétique, jusqu'à un récepteur AM ou FM qui la restitue. 
Les liaisons de télécommunications peuvent être monodirectionnelles, comme en radiodiffusion ou télévision, ou bidirectionnelles, utilisant alors un émetteur-récepteur. Quand plusieurs liaisons sont interconnectées entre plusieurs utilisateurs, on obtient un réseau, comme par exemple le réseau téléphonique ou internet. 


III.2. Médias de transmission
La transmission s'effectue par différents médias selon les systèmes. Historiquement le fil téléphonique fut le premier support de télécommunication et permit le développement du télégraphe et du téléphone. Il est toujours le média principal pour le raccordement aux réseaux téléphonique et aux réseaux informatiques (téléphone, fax, minitel, internet, ...), sous forme de paire(s) torsadée(s). 
Le câble coaxial était le média du haut débit avant l'apparition des fibres optiques, il est toujours utilisé dans les réseaux industriels en raison de sa robustesse face aux perturbations. C'est aussi le support de prédilection pour les raccordement en radiofréquence à l'intérieur d'un équipement, parfois remplacé par le guide d'onde pour les transmissions de micro-ondes de forte puissance. 
La fibre optique, qui raccorde progressivement les abonnés en ville, est aussi le média des câbles sous-marins modernes. C'est un fil en verre ou en plastique très fin qui a la propriété de conduire la lumière. 
La « radio », qui peut être définie comme toute communication par l'intermédiaire de l'espace hertzien, a révolutionné les télécommunications au début du xxe siècle. C'est le média de la radiodiffusion de programmes, des services de communications en radiotéléphonie, des réseaux de téléphonie mobile, du Wi-Fi, des loisirs radio comme le radioamateurisme, des liaisons par satellite de télécommunications ou par faisceau hertzien, aussi bien que des simples télécommandes domestiques. La radioélectricité étudie la transmission hertzienne, la propagation des ondes, les interfaces avec l'émetteur et le récepteur par l'intermédiaire des antennes. 
Les liaisons optiques dans l'espace, donc non guidées par fibres, sont utilisées en communications par satellites, ainsi que dans des applications aussi simples que les télécommandes audio-vidéo. 
Enfin, certains milieux ne peuvent être traversés que par des ondes acoustiques, c'est le cas des communications dans les mines, ou entre plongeurs, qui s'effectue par ondes ultra-sonores. 


III.3. Émission et réception
Quel que soit le média de transmission, un émetteur convertit l'information en signal électrique, optique ou radioélectrique adapté au média, en le modulant et en l'amplifiant. Inversement, un récepteur convertit le signal transmis en information utilisable. 
La technique de ces fonctions d'interface est donc très dépendante du média, de la fréquence d'utilisation, et surtout de la puissance nécessaire pour compenser les pertes de propagation. Ainsi, la transmission sur une ligne Ethernet par exemple n'utilise que quelques circuits intégrés et du câble de faible section, alors qu'une liaison vers une sonde planétaire demande des émetteurs de forte puissance et des antennes de plusieurs dizaines de mètres. 
Dans un canal de transmission hertzien, le signal porté par l'onde radioélectrique est atténué par la perte d'espace, les absorptions atmosphériques et les précipitations, et dégradé par les diffractions et réflexions. L'Équation des télécommunications inclut tous ces facteurs et détermine la puissance et les antennes nécessaires. 
L'antenne radioélectrique convertit les signaux électriques en onde radioélectrique à l'émission, et inversement en réception. De nombreux types d'antennes ont été développées, selon la fréquence d'utilisation, le gain nécessaire et l'application, depuis les antennes miniatures intégrées aux téléphones mobiles, jusqu'aux paraboles géantes de radioastronomie. 
Dans les applications bidirectionnelles, comme la radiotéléphonie, les deux fonctions peuvent être combinées dans un émetteur-récepteur. Un récepteur suivi d'un émetteur constituent un répéteur, par exemple sur un satellite de télécommunication, ou dans un câble sous-marin. 


III.4. Partage du média de transmission
Le partage du média entre utilisateurs se fait par les techniques d'affectation, de multiplexage et d'accès multiple. 
L'affectation de fréquences par bande et par service sur le média hertzien est la première technique apparue pour empêcher les brouillages mutuels. 
À l'intérieur d'une bande de fréquences, le multiplexage fréquentiel est la division d'un média de transmission en plusieurs canaux, chacun étant affecté à une liaison. Cette affectation peut être fixe, par exemple en radiodiffusion FM, une station émet à 96,1 MHz, une autre à 94,5 MHz. L'affectation des fréquences peut être dynamique comme en FDMA (Accès multiple par division en fréquence), utilisée par exemple lors de transmissions par satellite. Chaque utilisateur du canal y reçoit dans ce cas une autorisation temporaire pour une des fréquences disponibles. 
En communications numériques, le multiplexage peut également être temporel ou par codage :

Les techniques d'étalement de spectre comme le (CDMA) sont utilisées notamment en téléphonie mobile. Chaque liaison y est modulée par un code unique d'étalement, pour lequel les autres utilisateurs apparaissent comme du bruit après démodulation. 
Le codage par paquets (TDMA) est la clé du système ATM de communications internationales et de tout le réseau internet. Chaque utilisateur y transmet des « paquets numériques » munis d'adresses, qui se succèdent dans le canal. 

Le fonctionnement de ces techniques d'accès multiple nécessitent des protocoles pour les demandes d'affectation, les adressages, dont le plus connu est le TCP/IP d'Internet.


III.5. Traitement du signal
Le traitement du signal permet d'adapter l'information (sous forme de signal analogique ou numérique) au média de transmission et de la restituer après réception. 
À l'émission, les techniques de compression permettent de réduire le débit nécessaire, idéalement sans perte de qualité perceptible, par exemple sur la musique (MP3) ou sur la vidéo (MPEG), les codages transforment le signal d'information binaire en une forme adaptée à la modulation. 
À la réception, les opérations inverses sont effectuées : démodulation, décodage, correction et décompression. La correction d'erreur permet, grâce à un ajout d'information redondante par un code correcteur, de diviser de plusieurs ordres de grandeur le taux d'erreur. 
Ces techniques varient selon que les signaux à transmettre sont analogiques, comme la musique, la voix, l'image, ou numériques, comme les fichiers ou les textes. Un signal analogique varie continûment alors qu'un signal numérique est une succession d'états discrets, binaires dans le cas le plus simple, se succédant en séquence. 
Dans de nombreuses applications (TNT, téléphonie mobile, etc), le signal analogique est converti en numérique, ce qui permet des traitements plus efficaces, en particulier le filtrage du bruit 8. Seuls la modulation, l'amplification et le couplage au média restent alors analogiques. 


III.6. Systèmes et réseaux
Un ensemble de liaisons et de fonctions permettant d'assurer un service, constitue un système de télécommunications. 
Ainsi le système de satellites Inmarsat, destiné aux communications mobiles, comporte plusieurs satellites, plusieurs type de liaisons d'utilisateurs selon les débits et usages, des milliers de terminaux adaptés, et des liaisons de télémesure et de télécommande permettant le contrôle des satellites depuis les stations terrestres, celles-ci étant également connectées par des liaisons terrestres dédiées. 
Un système de télécommunications peut avoir une architecture :

de type "point à point", comme par exemple un cable hertzien ou optique, ou une liaison radiotéléphonique. Des répéteurs peuvent y être inclus pour amplifier et corriger les signaux ; 
de « diffusion », comme en télévision où un émetteur est reçu par des milliers de récepteurs ; 
de « collecte », comme en surveillance océanographique, où des centaines de capteurs sont reçus par un système central ; 
en structure de réseau, où un ensemble d'émetteurs et de récepteurs communiquent entre eux par des liaisons « étoilées » (topologie en étoile) ou « point à point ». C'est la plus commune. 

Un réseau de radiotéléphonie de secours est un réseau simple entre un central et des mobiles, géré par des procédures radio et des opérateurs. 
Un réseau commuté comme le réseau téléphonique comporte des liaisons individuelles d'abonné comme une ligne analogique ou une ligne RNIS, des centraux téléphoniques pour établir un circuit entre deux abonnés et des liaisons haut débit pour relier les centraux téléphoniques. 
Un réseau par paquet, comme Internet, comporte des routeurs qui aiguillent les paquets d'information d'une machine vers une autre désignée par son adresse IP.



IV. Applications des télécommunications 

IV.1. Voix et son
Le transport de la voix par la téléphonie, fut la première avancée des télécommunications, juste après les premiers télégraphes. Le téléphone est l'appareil qui sert à tenir une conversation bidirectionnelle avec une personne lointaine. Il est utilisé à titre privé, pour garder le contact avec ses proches ou à titre professionnel, pour échanger des informations orales sans avoir à se rencontrer physiquement. 
La téléphonie qui repose sur le réseau téléphonique permet également des services plus avancés tels que la messagerie vocale, la conférence téléphonique ou les services vocaux. La ligne téléphonique sert aussi de solution d'accès à Internet, d'abord avec un modem en bas débit, puis en haut débit grâce à l'ADSL. 
La radiotéléphonie, c'est à dire la communication à distance sans fil, a d'abord été appliquée aux communications maritimes pour en accroître la sécurité, puis militaires dès la première guerre mondiale, avant de devenir un media populaire avec la TSF. La radiotéléphonie est encore le moyen principal de communication du contrôle aérien, des liaisons maritimes par la radio maritime et des liaisons de sécurité (police, secours). C'est aussi l'activité principale du radioamateurisme. 
La radiodiffusion est la distribution de programmes à partir d'un émetteur vers des auditeurs équipés d'un récepteur. D'abord en modulation d'amplitude en basse fréquence (GO) et moyenne fréquence (PO), puis en modulation de fréquence en VHF, elle évolue vers la radio numérique, diffusée par satellite ou en VHF terrestre. 
La téléphonie mobile est la possibilité de téléphoner sans connexion filaire soit par une solution terrestre basée sur des zones de couverture de relais, soit par satellite. Le développement de ce moyen de communication est un phénomène de société remarquable de la fin du xxe siècle. Le geste de téléphoner dans la rue devient banal, au point d'inquiéter sur ses risques sanitaires et de créer un langage particulier, le langage SMS. En attendant de voir partout les programmes de télévision sur mobile en cours de développement, l'accès à Internet est déjà facile sur les dernières générations de téléphones. 


IV.2. Image et vidéo
La transmission d'images fixes par ligne téléphonique remonte au bélinographe, et est toujours utilisée sous le nom abrégé de fax, comme échange de pages photocopiées, documents commerciaux ou technique. Le radiofacsimilé qui permet de transmettre des images par radio est utilisé surtout pour la diffusion de cartes météo, soit directement depuis les satellites d'observation, soit retransmises vers les navires ou les terrains d'aviation. 
Après le téléphone et la radio, la télévision est présente dans tous les foyers. Les forêts d'antennes yagi et de paraboles ont envahi les villes, les chaînes satellites, d'abord analogiques puis numériques ont multiplié les programmes nationaux et internationaux. 
Les récepteurs modernes à plasma ou LCD fournissent des images de haute qualité et la télévision numérique terrestre augmente encore le choix des usagers. 
La transmission d'images simultanées à une liaison de téléphonie est possible grâce à la visioconférence utilisant des canaux à haut débit dédiés, par la transmission à balayage lent analogique ou SSTV, immortalisée par les premiers pas sur la lune, et par les techniques numériques nouvelles, webcam sur internet ou téléphone mobile de dernière génération. 


IV.3. Texte et données
Le télégraphe est l'ancêtre des transmissions de données et la première application des télécommunications : transmettre des caractères, donc un message, par signaux optiques, puis sur une ligne puis par ondes radio (radiotélégraphie). Le télétype puis le radiotélétype l'ont automatisé. 
Un réseau informatique est un ensemble d'équipements reliés entre eux pour échanger des informations. Quoique l'internet ne soit pas le seul système de réseau informatique, il en est presque devenu synonyme. La structure d'internet est complexe et peut se séparer en plusieurs parties : 

des fonctions de communication (les lignes d'abonnés, les modems, les routeurs qui connectent au web) ; 
des fonctions de transport entre utilisateurs (les protocoles, les serveurs,...) ; 
des applications qui fournissent le service final (messagerie, image, voix, moteur de recherche, etc).

La télémesure, terrestre comme en hydrologie ou en métérologie, ou spatiale comme les images météosat ou celles des sondes planétaires lointaines, permet la surveillance des installations industrielles, augmente notre connaissance de l'environnement, du climat ou de l'univers. 
La télécommande, la plus simple comme en domotique ou en HiFi et vidéo, ou la plus complexe comme celle des robots martiens, est la commande à distance sans fil, optique ou radio, généralement couplée à la télémesure. 


IV.4. Autres applications
Le signal radioélectrique peut contenir d'autres informations, comme des paramètres permettant les calculs de position, le temps universel, la détection de cibles ou la cartographie du terrain. 
Quoique le radar ne soit pas à proprement parler un système de communication, mais de télédétection, ses techniques combinent micro-onde, traitement du signal, radioélectricité, et peuvent être rattachées au monde des télécommunications. 
Initialement développé pour la détection des raids aériens, le radar fut très vite installé sur les navires, puis les avions. 
D'abord militaire puis civil, le contrôle aérien et maritime utilisent intensivement le radar pour la sécurité. Enfin le radar météorologique permet de cartographier les pluies et nuages, y compris depuis les satellites d'observation. 
La radionavigation a permis, dès les débuts de la radio, d'aider à la navigation maritime puis aérienne, grâce à la radiogoniométrie et aux radiophares, puis aux systèmes hyperboliques comme le LORAN. Les systèmes de navigation par satellite comme le GPS sont devenus un équipement courant des véhicules, en attendant le développement du futur Galileo. 
Les systèmes d'identification automatique comme l'AIS et de détection d'obstacle améliorent la sécurité de la navigation. 
La diffusion du temps universel et de signaux horaires est intégrée aux signaux de radionavigation GPS actuels, mais a longtemps été un service spécifique d'aide à la navigation astronomique, ou de synchronisation scientifique, par émissions HF comme le WWV, ou BF comme l'émetteur d'Allouis ou le DCF. 
Pour leurs télécommunications, les militaires utilisent des méthodes de discrétion comme l'évasion de fréquence, et de cryptage, sur des réseaux de radiotéléphonie HF et VHF, ou des satellites dédiés, comme Syracuse. les gouvernements utilisent également les techniques radioélectriques dans un but de renseignement électromagnétique, comme le système Echelon d'écoute satellitaire 9, ou des systèmes de brouillage et de contre-mesures radioélectriques. 



V. Télécommunications et société
Les télécommunications représentent un secteur d'activité économique significatif.

V.1. Télécommunications et développement
Les télécommunications sont un élément crucial de la société moderne. En 2006, l'industrie des télécommunications représentait un revenu de 1 200 milliards de dollars, soit 3 % du revenu mondial. 
À l'échelle microéconomique, les entreprises utilisent les télécommunications pour construire leur activité, comme les ventes en ligne, ou améliorer leur efficacité, comme les magasins traditionnels. Dans le monde entier, des services à domicile peuvent être obtenus sur simple appel téléphonique, des livraisons de pizzas au dépannage. Dans les communautés les plus pauvres, le téléphone mobile sert aussi bien au Bangladesh qu'en Côte d'Ivoire pour négocier les ventes agricoles au meilleur prix du marché. 
En raison des avantages économiques d'une infrastructure correcte de télécommunications, à laquelle une grande partie du monde n'a pas accès, l'écart de développement par manque de télécommunications, ou fracture numérique, peut se creuser. 


V.2. Culture et télécommunications
Les télécommunications modernes permettent de transmettre de l'image, du son et du texte dans le monde entier. Ces moyens techniques sont neutres par rapport à leur contenu. Cependant, les télécommunications sont à l'origine de débats en termes d'uniformisation de la culture, d'identité nationale ou, au contraire, de nouvelles possibilités d'expression, de communication permettant de s'affranchir des frontières et des espaces traditionnels. 
Le développement des moyens de transmission hertzien, terrestre puis satellitaire, a favorisé le déploiement à grande échelle des médias de masse (radio, télévision...) dans les sociétés 14, modifiant ainsi les modes de pensée et les schémas culturels traditionnels. Par exemple, pendant la guerre froide, la radio reçue internationalement en ondes courtes depuis les émetteurs américains vers la RDA, russes vers l'Europe ou chinois installé en Albanie, a servi de média de propagande entre deux idéologies. La télévision par satellite dont les paraboles garnissent les immeubles des banlieues européennes, permet aux communautés minoritaires de garder leur lien culturel. 
Enfin, la convergence des réseaux numériques et des infrastructure de télécommunications mondiales permet de se connecter au Web par le biais du réseau Internet presque en tout point de la surface terrestre. Ce nouveau mode de communication transforme progressivement les manières d'échanger, de communiquer et de travailler 15 non seulement dans une société mais aussi entre sociétés de cultures différentes. 
Cependant, on trouve aussi sur le Web par exemple des albums CD et des films avant leur mise en vente, ce qui provoque des réactions restrictives, voire policières, des grands distributeurs. Les informations vraies ou fausses peuvent circuler en quelques jours, les groupes extrémistes ou criminels peuvent s'organiser sans limitation. 


V.3. Entreprises des télécommunications
Les industriels des télécommunications conçoivent et produisent des équipements et des logiciels destinés aux télécommunications. Ils participent aussi à la normalisation en proposant de nouvelles solutions aux organismes de standardisation. 
Les constructeurs peuvent être des entreprises multinationales issues de plusieurs fusions-acquisitions comme Aastra, Alcatel-Lucent, Nokia-Siemens, ou des start-up comme Fortinet. Ils sont majoritairement d'Amérique du Nord : Cisco, 3Com, Nortel, d'Europe : Alcatel-Lucent, Ericsson, Nokia ou de Chine (ROC ou RPC) : Huawei, ZTE,D-Link. 
Certains constructeurs se focalisent sur une technologie comme Extreme Networks sur l'Ethernet. D'autres, comme Cisco, essayent de couvrir toutes les technologies, tous les marchés (particulier, entreprise, opérateur de télécommunications), tous les services (support, installation, architecture, etc). 
Un opérateur de télécommunications est une entreprise qui commercialise des services en utilisant les infrastructures de télécommunications. Ce peut-être une entreprise indépendante, ou une filiale d'un constructeur, qui loue une capacité sur un réseau pour vendre des abonnements et des connexions individuelles, ou encore une entreprise publique proprétaire du réseau, comme les opérateurs historiques européens. 


V.4. Organismes de normalisation et de standardisation 
L'interopérabilité entre équipements ou systèmes différents nécessite des standards et des protocoles de télécommunications précis qui évoluent en versions successives selon les avances techniques. Un fabricant dont une ou plusieurs innovations sont à la base d'une norme ou d'un standard, est assuré de prendre une avance significative sur son marché, les constructeurs d'équipements tissent donc des liens très étroits avec les organismes de normalisation et de standardisation. 
Parmi les principaux organismes de normalisation-standardisation mondiaux, citons :

l'ETSI : European Telecommunication Standards Institute ou Institut européen des normes de télécommunication ; 
l'ITU : International Telecommunication Union ou Union internationale des télécommunications ; 
l'IETF : Internet Engineering Task Force ;
l'ATM Forum ; 
l'ANSI : American National Standard Institute ;
l'IEEE: Institute of Electrical and Electronics Engineers.


V.5. Administration des télécommunications
Pour optimiser l'utilisation du spectre de fréquence et limiter les interférences entre systèmes, les états s'accordent au niveau international :

au niveau de l'Europe par le CEPT (Conférence européenne des administrations des postes et télécommunications) ; 
au niveau international par l'UIT - Union Internationale des Télécommunications (agence de l'ONU) ; 
par des révisions du « Règlement des Radiocommunications » ou RR sont prévues dans le cadre des CMR (Conférence Mondiale des Radiocommunications) ou « WRC » (World Radiocommunication Conference). 

Chaque pays gère ces règlementations internationales à l'intérieur de ses frontières, sous le contrôle d'administrations nationales :

la FCC (federal commission of communications) aux Etats-Unis ; 
l'ARCEP (autorité de régulation des communications électroniques et des postes) et l'ANFR (agence nationale des fréquences), en France ; 

Le secteur des télécommunications était historiquement lié à la puissance publique de chaque état et exploité par cet état. Depuis les années 1980-1990, un mouvement mondial de dé-règlementation (ou dé-régulation) du secteur des télécommunications est intervenu, amenant par exemple au dégroupage du réseau téléphonique.
